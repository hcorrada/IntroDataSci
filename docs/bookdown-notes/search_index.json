[
["index.html", "Lecture Notes: Introduction to Data Science 1 Preamble", " Lecture Notes: Introduction to Data Science CMSC320, University of Maryland, College Park Héctor Corrada Bravo 2018-02-20 1 Preamble These are lecture notes for CMSC320, Introduction to Data Science at the University of Maryland, College Park. Refer to the Course Web Page for further information. "],
["introduction-and-overview.html", "2 Introduction and Overview 2.1 What is Data Science? 2.2 Why Data Science? 2.3 Data Science in Society 2.4 Course Organization 2.5 General Workflow", " 2 Introduction and Overview 2.1 What is Data Science? Data science encapsulates the interdisciplinary activities required to create data-centric artifacts and applications that address specific scientific, socio-political, business, or other questions. Let’s look at the constiuent parts of this statement: 2.1.1 Data Measureable units of information gathered or captured from activity of people, places and things. 2.1.2 Specific Questions Seeking to understand a phenomenon, natural, social or other, can we formulate specific questions for which an answer posed in terms of patterns observed, tested and or modeled in data is appropriate. 2.1.3 Interdisciplinary Activities Formulating a question, assessing the appropriateness of the data and findings used to find an answer require understanding of the specific subject area. Deciding on the appropriateness of models and inferences made from models based on the data at hand requires understanding of statistical and computational methods. 2.1.4 Data-Centric Artifacts and Applications Answers to questions derived from data are usually shared and published in meaningful, succinct but sufficient, reproducible artifacts (papers, books, movies, comics). Going a step further, interactive applications that let others explore data, models and inferences are great. 2.2 Why Data Science? The granularity, size and accessibility data, comprising both physical, social, commercial and political spheres has exploded in the last decade or more. “I keep saying that the sexy job in the next 10 years will be statisticians” Hal Varian, Chief Economist at Google (http://www.nytimes.com/2009/08/06/technology/06stats.html?_r=0) “The ability to take data—to be able to understand it, to process it, to extract value from it, to visualize it, to communicate it—that’s going to be a hugely important skill in the next decades, not only at the professional level but even at the educational level for elementary school kids, for high school kids, for college kids.” “Because now we really do have essentially free and ubiquitous data. So the complimentary scarce factor is the ability to understand that data and extract value from it.” Hal Varian (http://www.mckinsey.com/insights/innovation/hal_varian_on_how_the_web_challenges_managers) 2.3 Data Science in Society Because of the large amount of data produced across many spheres of human social and creative activity, many societal questions may be addressed by establishing patterns in data. In the humanities, this can range from unproblematic quesitions of how to dissect a large creative corpora, say music, literature, based on raw characteristics of those works, text, sound and image. To more problematic questions, of analysis of intent, understanding, appreciation and valuation of these creative corpora. In the social sciences, issues of fairness and transparency in the current era of big data are especially problematic. Is data collected representative of population for which inferences are drawn? Are methods employed learning latent unfair factors from ostensibly fair data? These are issues that the research community is now starting to address. In all settings, issues of ethical collection of data, application of models, and deployment of data-centric artifacts are essential to grapple with. Issues of privacy are equally important. 2.4 Course Organization This course will cover basics of how to represent, model and communicate about data and data analyses using the R data analysis environment for Data Science. The course is roughly divided into five areas: Area 0: tools and skills ○ Toolset (Rstudio/tidyverse/rmarkdown) ○ Best practices ○ Debugging data science Area I: types and operations ○ Data tables and data types ○ Operations on tables ○ Basic plotting ○ Tidy data / the ER model ○ Relational Operations ○ SQL ○ Advanced: other data models, db consistency and concurrency Area II: wrangling ○ Data acquisition (load and scrape) ○ EDA Vis / grammar of graphics ○ Data cleaning (text, dates) ○ EDA: Summary statistics ○ Data analysis with optimization (derivatives) ○ Data transformations ○ Missing data Area III: modeling ○ Univariate probability and statistics ○ Hypothesis testing ○ Multivariate probablity and statistics (joint and conditional probability, Bayes thm) ○ Data Analysis with geometry (vectors, inner products, gradients and matrices) ○ Linear regression ○ Logistic regression ○ Gradient descent (batch and stochastic) ○ Trees and random forests ○ K-NN ○ Naïve Bayes ○ Clustering ○ PCA Area IV: essential applications ○ Text mining ○ Graphs ○ Forecasting Area V: communication ○ Designing data visualizations for communication not exploration ○ Interactive visualization ○ Writing about data, analysis, and inferences 2.5 General Workflow The data science activities we will cover are roughly organized into a general workflow that will help us navigate this material. 2.5.1 Defining the Goal What is the question/problem? Who wants to answer/solve it? What do they know/do now? How well can we expect to answer/solve it? How well do they want us to answer/solve it? 2.5.2 Data Collection and Management What data is available? Is it good enough? Is it enough? What are sensible measurements to derive from this data? Units, transformations, rates, ratios, etc. 2.5.3 Modeling What kind of problem is it? E.g., classification, clustering, regression, etc. What kind of model should I use? Do I have enough data for it? Does it really answer the question? 2.5.4 Model Evaluation Did it work? How well? Can I interpret the model? What have I learned? 2.5.5 Presentation Again, what are the measurements that tell the real story? How can I describe and visualize them effectively? 2.5.6 Deployment Where will it be hosted? Who will use it? Who will maintain it? "],
["an-illustrative-analysis.html", "3 An Illustrative Analysis 3.1 Gathering data 3.2 Manipulating the data 3.3 Visualizing the data 3.4 Modeling data 3.5 Visualizing model result 3.6 Abstracting the analysis 3.7 Making analyses accessible 3.8 Summary", " 3 An Illustrative Analysis http://fivethirtyeight.com has a clever series of articles on the types of movies different actors make in their careers: https://fivethirtyeight.com/tag/hollywood-taxonomy/ I’d like to do a similar analysis. Let’s do this in order: Let’s do this analysis for Diego Luna Let’s use a clustering algorithm to determine the different types of movies they make Then, let’s write an application that performs this analysis for any actor and test it with Gael García Bernal Let’s make the application interactive so that a user can change the actor and the number of movie clusters the method learns. For now, we will go step by step through this analysis without showing how we perform this analysis using R. As the course progresses, we will learn how to carry out these steps. 3.1 Gathering data 3.1.1 Movie ratings For this analysis we need to get the movies Diego Luna was in, along with their Rotten Tomatoes ratings. For that we scrape this webpage: https://www.rottentomatoes.com/celebrity/diego_luna. Once we scrape the data from the Rotten Tomatoes website and clean it up, this is part of what we have so far: RATING TITLE CREDIT BOX OFFICE YEAR 5 Flatliners Ray $16.9M 2017 85 Rogue One: A Star Wars Story Captain Cassian Andor $532.2M 2016 88 Blood Father Jonah — 2016 82 The Book of Life Manolo — 2014 100 I Stay with You (Me quedo contigo) Actor — 2014 67 Elysium Julio $90.8M 2013 41 Casa de mi padre Raul $5.9M 2012 This data includes, for each of the movies Diego Luna has acted in, the rotten tomatoes rating, the movie title, Diego Luna’s role in the movie, the U.S. domestic gross and the year of release. 3.1.2 Movie budgets and revenue For the movie budgets and revenue data we scrape this webpage: http://www.the-numbers.com/movie/budgets/all (Note 01.2018: after the initial version of this analysis, this website added pagination to this URL. We will be using the CSV file scraped originally in Summer 2017 for this analysis and leave the issue of dealing with pagination as an exercise.) ## Parsed with column specification: ## cols( ## release_date = col_date(format = &quot;&quot;), ## movie = col_character(), ## production_budget = col_double(), ## domestic_gross = col_double(), ## worldwide_gross = col_double() ## ) This is part of what we have for that table after loading and cleaning up: release_date movie production_budget domestic_gross worldwide_gross 2009-12-18 Avatar 425 760.50762 2783.9190 2015-12-18 Star Wars Ep. VII: The Force Awakens 306 936.66223 2058.6622 2007-05-24 Pirates of the Caribbean: At World’s End 300 309.42043 963.4204 2015-11-06 Spectre 300 200.07417 879.6209 2012-07-20 The Dark Knight Rises 275 448.13910 1084.4391 2013-07-02 The Lone Ranger 275 89.30212 260.0021 2012-03-09 John Carter 275 73.05868 282.7781 2010-11-24 Tangled 260 200.82194 586.5819 2007-05-04 Spider-Man 3 258 336.53030 890.8753 2015-05-01 Avengers: Age of Ultron 250 459.00587 1404.7059 This data is for 5358 movies, including its release date, title, production budget and total gross. The latter two are in millions of U.S. dollars. One thing we might want to check is if the budget and gross entries in this table are inflation adjusted or not. To do this, we can make a plot of domestic gross, which we are using for the subsequent analyses. Although we don’t know for sure, since the source of our data does not state this specifically, it looks like the domestic gross measurement is not inflation adjusted since gross increases over time. 3.2 Manipulating the data Next, we combine the datasets we obtained to get closer to the data we need to make the plot we want. We combine the two datasets using the movie title, so that the end result has the information in both tables for each movie. RATING TITLE CREDIT BOX OFFICE YEAR release_date production_budget domestic_gross worldwide_gross 5 Flatliners Ray $16.9M 2017 1990-08-10 26.0 61.30815 61.30815 85 Rogue One: A Star Wars Story Captain Cassian Andor $532.2M 2016 2016-12-16 200.0 532.17732 1050.98849 82 The Book of Life Manolo — 2014 2014-10-17 50.0 50.15154 97.65154 67 Elysium Julio $90.8M 2013 2013-08-09 120.0 93.05012 286.19209 51 Contraband Gonzalo $66.5M 2012 2012-01-13 25.0 66.52800 98.40685 94 Milk Jack Lira $31.8M 2008 2008-11-26 20.0 31.84130 57.29337 69 Criminal Rodrigo $0.8M 2004 2016-04-15 31.5 14.70870 38.77126 61 The Terminal Enrique Cruz $77.1M 2004 2004-06-18 75.0 77.07396 218.67396 80 Open Range Button $58.3M 2003 2003-08-15 26.0 58.33125 68.61399 76 Frida Alejandro Gomez $25.7M 2002 2002-10-25 12.0 25.88500 56.13124 3.3 Visualizing the data Now that we have the data we need, we can make a plot: Figure 3.1: Ratings and U.S. Domestic Gross of Diego Luna’s movies. We see that there is one clear outlier in Diego Luna’s movies, which probably is the one Star Wars movie he acted in. The remaining movies could potentially be grouped into two types of movies, those with higher rating and those with lower ratings. 3.4 Modeling data We can use a clustering algorithm to partition Diego Luna’s movies. We can use the data we obtained so far and see if the k-means clustering algorithm partitions these movies into three sensible groups using the movie’s rating and domestic gross. Let’s see how the movies are grouped: TITLE RATING domestic_gross cluster The Book of Life 82 50.15154 1 Milk 94 31.84130 1 Criminal 69 14.70870 1 Open Range 80 58.33125 1 Frida 76 25.88500 1 Flatliners 5 61.30815 2 Elysium 67 93.05012 2 Contraband 51 66.52800 2 The Terminal 61 77.07396 2 Rogue One: A Star Wars Story 85 532.17732 3 3.5 Visualizing model result Let’s remake the same plot as before, but use color to indicate each movie’s cluster assignment given by the k-means algorithm. The algorithm did make the Star Wars movie it’s own group since it’s so different that the other movies. The grouping of the remaining movies is not as clean. To make the plot and clustering more interpretable, let’s annotate the graph with some movie titles. In the k-means algorithm, each group of movies is represented by an average rating and an average domestic gross. What we can do is find the movie in each group that is closest to the average and use that movie title to annotate each group in the plot. Roughly, movies are clustered into Star Wars and low vs. high rated movies. The latter seem to have some difference in domestic gross. For example, movies like “The Terminal” have lower rating but make slightly more money than movies like “Frida”. We could use statistical modeling to see if that’s the case, but will skip that for now. Do note also, that the clustering algorithm we used seems to be assigning one of the movies incorrectly, which warrants further investigation. 3.6 Abstracting the analysis While not a tremendous success, we decide we want to carry on with this analysis. We would like to do this for other actors’ movies. One of the big advantages of using R is that we can write a piece of code that takes an actor’s name as input, and reproduces the steps of this analysis for that actor. We call these functions, we’ll see them and use them a lot in this course. For our analysis, this function must do the following: Scrape movie ratings from Rotten Tomatoes Clean up the scraped data Join with the budget data we downloaded previously Perform the clustering algorithm Make the final plot With this in mind, we can write functions for each of these steps, and then make one final function that puts all of these together. For instance, let’s write the scraping function. It will take an actor’s name and output the scraped data. Let’s test it with Gael García Bernal: RATING TITLE CREDIT BOX OFFICE YEAR 92% The Kindergarten Teacher Actor — 2018 No Score Yet Viva - A vida é uma festa Hector — 2017 97% Coco Hector $192M 2017 Good start. We can then write functions for each of the steps we did with Diego Luna before. Then put all of these steps into one function that calls our new functions to put all of our analysis together: We can test this with Gael García Bernal analyze_actor(&quot;Gael Garcia Bernal&quot;) 3.7 Making analyses accessible Now that we have written a function to analyze an actor’s movies, we can make these analyses easier to produce by creating an interactive application that wraps our new function. The shiny R package makes creating this type of application easy. 3.8 Summary In this analysis we saw examples of the common steps and operations in a data analysis: Data ingestion: we scraped and cleaned data from publicly accessible sites Data manipulation: we integrated data from multiple sources to prepare our analysis Data visualization: we made plots to explore patterns in our data Data modeling: we made a model to capture the grouping patterns in data automatically, using visualization to explore the results of this modeling Publishing: we abstracted our analysis into an application that allows us and others to perform this analysis over more datasets and explore the result of modeling using a variety of parameters "],
["setting-up-the-r-data-science-toolbox.html", "4 Setting up the R Data Science Toolbox 4.1 Some history 4.2 Setting up R 4.3 Setting up Rstudio 4.4 A first look at Rstudio 4.5 R packages 4.6 Additional R resources 4.7 Literate Programming 4.8 Finishing your setup", " 4 Setting up the R Data Science Toolbox Here we setup R, RStudio and anything else we will use in the course. 4.1 Some history R is an offspring of S, a language created in AT&amp;T Labs by John Chambers (now at Stanford) and others in 1976 with the goal of creating an environment for statistical computing and data analysis. The standard for the language in current use was settled in 1998. That same year, “S” won the ACM Software System award, awarded to software systems “that have a lasting influence, reflected in contributions to concepts, in commercial acceptance, or both”. In 1991, Robert Gentleman and Ross Ihaka created R to provide an open source implementation of the S language and environment. They also redesigned the language to enforce lexical scoping rules. It has been maintained by the R core group since 1997, and in 2015 an R consortium, including Microsoft, Google, and others, was created. Along with Python it is one of the most popular environments for data analysis (e.g., figure below from KDNuggets 2017 software survey We use R for this class because we find that besides it being a state-of-the-art data analysis environment, it provides a clean end-to-end platform for teaching material across the data management-modeling-communication spectrum that we study in class. However, be aware that as you move on in the Data Science field you most likely will need to add Python to your toolbelt. 4.2 Setting up R R is a free, open source, environment for data analysis. It is available as a free binary download for Mac, Linux and Windows. For the more adventorous, it can also be compiled from source. To install R in your computer go to https://cran.r-project.org/index.html and download and install the appropriate binary file. This will install the base R system: the R programming language, a few packages for common data analyses and a development environment. 4.3 Setting up Rstudio We will actually use Rstudio to interact with R. Rstudio is a very powerful application to make data analysis with R easier to do. To install go to https://www.rstudio.com/products/rstudio/download/ and download the appropriate version of Rstudio. 4.4 A first look at Rstudio Let’s take a first look at Rstudio. The first thing you will notice is that Rstudio is divided into panes. Let’s take a look first at the Console. 4.4.1 Interactive Console The most immediate way to interact with R is through the interactive console. Here we can write R instructions to perform our data analyses. We want to start using data so the first instructions we will look at deal with loading data. When you installed R, a few illustrative datasets were installed as well. Let’s take a look at the list of datasets you now have access to. Write the following command in the console This will list names and descriptions of datasets available in your R installation. Let’s try to find out more information about these datasets. In R, the first attempt to get help with something is to use the ? operation. So, to get help about the swiss dataset we can enter the following in the console This will make the documentation for the swiss dataset open in another pane. On your own: Find more information about a different dataset using the ? operator. 4.4.2 Data Viewer According to the documentation we just saw for swiss, this is a data.frame with 47 observations and 6 variables. The data.frame is the basic structure we will use to represent data throughout the course. We will see this again repeatedly, and use a couple of other names (e.g., tibble) to refer to this. Intuitively, you can think of the data.frame like a spreadsheet, with rows representing observations, and columns representing variables that describe those observations. Let’s see what the swiss data looks like using the Rstudio data viewer. The Data Viewer lets you reorder data by the values in a column. It also lets you filter rows of the data by values as well. On your own: Use the Data Viewer to explore another of the datasets you saw listed before. 4.4.3 Names, values and functions Let’s make a very short pause to talk about something you may have noticed. In the console, we’ve now written a few instructions, e.g. View(swiss). Let’s take a closer look at how these instructions are put together. expressions: first of all, we call these instructions expressions, which are just text that R can evaluate into a value. View(swiss) is an expression. values: so, what’s a value? They are numbers, strings, data frames, etc. This is the data we will be working with. The number 2 is a value. So is the string &quot;Hector&quot;. So, what value is produced when R evaluates the expression View(swiss)? Nothing, which we also treat as a value. That wasn’t very interesting, but it does have a side effect: it shows the swiss dataset in the Data viewer. How about a simpler expression: swiss, what value is produced when R evaluates the expression swiss? The data.frame containing that data. Try it out in the console. names: so if swiss isn’t a value, what is it? It is a name. We use these to refer to values. So, when we write the expression swiss, we tell R we want the value referenced by the name swiss, that is, the data itself! functions: Besides numbers, strings, data frames, etc. another important type of value is the function. Functions are a series of instructions that take some input value and produce a different value. The name View refers to the function that takes a data frame as input, and displays it in the Data viewer. Functions are called using the parentheses we saw before: View(swiss), the parentheses say that you are passing input swiss to the function View. We’ll see later how we can write our own functions. 4.4.4 Plotting Next, I want to show the Plots pane in Rstudio. Let’s make a plot using the swiss dataset: It’s not pretty, but it was very easy to produce. There’s a couple of things going on here… plot is a function, it takes two inputs, the data to put in the x and y axes, evaluates to nothing, but creates a plot of the data swiss$Education is how we refer to the Education column in the swiss data frame. On your own: Make a plot using other variables in the swiss dataset. 4.4.5 Editor So far, we’ve made some good progress: we know how to write expressions on the R console so that they are evaluated, we are starting to get a basic understanding of how these expressions are constructed, we can use the Data viewer to explore data frames, and made one plot that was displayed in the Plots pane. To finish this quick tour, I want to look at two more Rstudio panes: the file editor, and the File viewer. As you have noticed, everytime we want to evaluate an expression on the console, we have to write it in. For example, if we want to change the plot we made above to include a different variable, we have to write the whole thing again. Also, what if I forgot what expression I used to make a specific plot? Even better, what if I wanted somebody else to make the plot I just made? By far, one of the biggest advantages of using R over Excel or other similar programs, is that we can write expressions in scripts that are easy to share with others, making analyses easier to reproduce. Let’s write a script that we can use to make the same plot we just made. In the Rstudio menu select File&gt;New File&gt;R Script This will open a tab in the File editor in which we can write expressions: We can then evaluate the expressions in the file one at a time, or all at the same time. We can then save these expressions in a script. In the Rstudio menu select File&gt;Save and save as a text file. The convention is to use the .R or .r file extension, e.g., swiss_plot.r. On your own: Add expressions for additional plots to the script and save again. Run the new expressions. 4.4.6 Files viewer Rstudio includes a Files viewer that you can use to find and load files. You can find the Files near the Plots viewer 4.5 R packages Another of R’s advantages for data analysis is that it has attracted a large number of extremely useful additions provided by users worldwide. These are housed in CRAN. In this course we will make a lot of use of a set of packages bundled together into the tidyverse by Hadley Wickham and others. These packages make preparing, modeling and visualizing certain kinds data (which covers the vast majority of use cases) quite fun and pleasent. There is a webpage for the general tidyverse project: http://tidyverse.org, which includes pages for each of the packages included there. Let’s install the tidyverse into your R environment. There are two ways of installing packages. In the console, you can use the expression: In Rstudio, you can use the Packages tab: On your own: Install the following additional packages which we will use later on: rvest, stringr, nycflights13 and broom. 4.6 Additional R resources Resources for learning and reading about R are listed in our here. Of note are the swirl project and DataCamp’s [introduction to R] course. 4.7 Literate Programming One last note before we get started. R has great support for literate programming, where source code that contains both code, the result of evaluating that code, and text explaining that code co-exist in a single document. This is extremely valuable in data analysis, as many choices made by data analysts are worth explaning in text, and interpretation of the results of analyses can co-exist with the computations used in that analysis. This document you are reading contains both text and code. In class, we will use Rmarkdown for this purpose. 4.8 Finishing your setup Complete your exit ticket as instructed. "],
["part-data-representation-modeling-ingestion-and-cleaning.html", "(Part) Data representation modeling, ingestion and cleaning", " (Part) Data representation modeling, ingestion and cleaning "],
["measurements-and-data-types.html", "5 Measurements and Data Types 5.1 A data analysis to get us going 5.2 Getting data 5.3 Entities and attributes 5.4 Categorical attributes 5.5 Discrete numeric attributes 5.6 Continuous numeric data 5.7 Other examples 5.8 Other important datatypes 5.9 Units", " 5 Measurements and Data Types Now that we have our tools ready, let’s start doing some analysis. First, let’s go over some principles of R as a data analysis environment. R is a computational environment for data analysis. It is designed around a functional language, as opposed to procedural languages like Java or C, that has desirable properties for the type of operations and workflows that are frequently performed in the course of analyzing datasets. In this exercise we will start learning some of those desirable properties while performing an analysis of a real dataset. 5.1 A data analysis to get us going I’m going to do an analysis of Baltimore crime to guide our discussion of R. We’ll use data downloaded from Baltimore City’s awesome open data site (this was downloaded a couple of years ago so if you download now, you will get different results). The repository for this particular data is here. https://data.baltimorecity.gov/Crime/BPD-Arrests/3i3v-ibrt 5.2 Getting data We’ve prepared the data previously into a comma-separated value file (.csv file): each line contains attribute values (separated by commas) describing arrests in the City of Baltimore. The read_csv command is part of the readr R package and allows you to read a dataset stored in a csv file. This function is extremely versatile, and you can read more about it by using the standard help system in R: ?read_csv. The result of running calling this function is the data itself, so, by running the function in the console, the result of the function is printed. To make use of this dataset we want to assign the result of calling read_csv (i.e., the dataset) to a variable: library(tidyverse) arrest_tab &lt;- read_csv(&quot;data/BPD_Arrests.csv&quot;) arrest_tab ## # A tibble: 104,528 x 15 ## arrest age race sex arrestDate arrestTime arrestLocation ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;time&gt; &lt;chr&gt; ## 1 11126858 23 B M 01/01/2011 00&#39;00&quot; &lt;NA&gt; ## 2 11127013 37 B M 01/01/2011 01&#39;00&quot; 2000 Wilkens Ave ## 3 11126887 46 B M 01/01/2011 01&#39;00&quot; 2800 Mayfield Ave ## 4 11126873 50 B M 01/01/2011 04&#39;00&quot; 2100 Ashburton St ## 5 11126968 33 B M 01/01/2011 05&#39;00&quot; 4000 Wilsby Ave ## 6 11127041 41 B M 01/01/2011 05&#39;00&quot; 2900 Spellman Rd ## 7 11126932 29 B M 01/01/2011 05&#39;00&quot; 800 N Monroe St ## 8 11126940 20 W M 01/01/2011 05&#39;00&quot; 5200 Moravia Rd ## 9 11127051 24 B M 01/01/2011 07&#39;00&quot; 2400 Gainsdbourgh Ct ## 10 11127018 53 B M 01/01/2011 15&#39;00&quot; 3300 Woodland Ave ## # ... with 104,518 more rows, and 8 more variables: incidentOffense &lt;chr&gt;, ## # incidentLocation &lt;chr&gt;, charge &lt;chr&gt;, chargeDescription &lt;chr&gt;, ## # district &lt;chr&gt;, post &lt;int&gt;, neighborhood &lt;chr&gt;, `Location 1` &lt;chr&gt; Now we can ask what type of value is stored in the arrest_tab variable: class(arrest_tab) ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; The data.frame is a workhorse data structure in R. It encapsulates the idea of entities (in rows) and attribute values (in columns). We call these rectangular datasets. The other types tbl_df and tbl are added by tidyverse for improved functionality. We can ask other features of this dataset: # This is a comment in R, by the way # How many rows (entities) does this dataset contain? nrow(arrest_tab) ## [1] 104528 # How many columns (attributes)? ncol(arrest_tab) ## [1] 15 # What are the names of those columns? colnames(arrest_tab) ## [1] &quot;arrest&quot; &quot;age&quot; &quot;race&quot; ## [4] &quot;sex&quot; &quot;arrestDate&quot; &quot;arrestTime&quot; ## [7] &quot;arrestLocation&quot; &quot;incidentOffense&quot; &quot;incidentLocation&quot; ## [10] &quot;charge&quot; &quot;chargeDescription&quot; &quot;district&quot; ## [13] &quot;post&quot; &quot;neighborhood&quot; &quot;Location 1&quot; Now, in Rstudio you can view the data frame using View(arrest_tab). 5.2.1 Names, values and functions Let’s review the concepts of names values and functions again. In the console, we’ve now written a few instructions, e.g. View(arrest_tab). Let’s take a closer look at how these instructions are put together. expressions: first of all, we call these instructions expressions, which are just text that R can evaluate into a value. View(arrest_tab) is an expression. values: so, what’s a value? They are numbers, strings, data frames, etc. This is the data we will be working with. The number 2 is a value. So is the string &quot;Hector&quot;. So, what value is produced when R evaluates the expression View(arrest_tab)? Nothing, which we also treat as a value. That wasn’t very interesting, but it does have a side effect: it shows the arrest_tab dataset in the Data viewer. How about a simpler expression: arrest_tab, what value is produced when R evaluates the expression arrest_tab? The data.frame containing that data. Try it out in the console. names: so if arrest_tab isn’t a value, what is it? It is a name. We use these to refer to values. So, when we write the expression arrest_tab, we tell R we want the value referenced by the name arrest_tab, that is, the data itself! functions: Besides numbers, strings, data frames, etc. another important type of value is the function. Functions are a series of instructions that take some input value and produce a different value. The name View refers to the function that takes a data frame as input, and displays it in the Data viewer. Functions are called using the parentheses we saw before: View(arrest_tab), the parentheses say that you are passing input arrest_tab to the function View. We’ll see later how we can write our own functions. 5.3 Entities and attributes As a reminder, we are using the term entities to refer to the objects to which data in a dataset refers to. For instance, in our example dataset, each arrest is an entity. In a rectangular dataset (a data frame) this corresponds to rows in a table. We then say that a dataset contains attributes for each entity. For instance, attributes of each arrest would be the person’s age, the type of offense, the location, etc. In a rectangular dataset, this corresponds to the columns in a table. This language of entities and attributes is commonly used in the database literature. In statistics you may see experimental units or samples for entities and covariates for attributes. In other instances observations for entities and variables for attributes. In Machine Learning you may see example for entities and features for attributes. For the most part, all of these are exchangable. This table summarizes the terminology: Field Entities Attributes Databases Entities Attributes Machine Learning Examples Features Statistics Observations/Samples Variables/Covariates This chapter is concerned with the types of data we may encounter as attributes in data analyses. 5.4 Categorical attributes A categorical attribute for a given entity can take only one of a finite set of examples. For example, the sex variable can only have value M, F, or `` (we’ll talk about missing data later in the semester). table(arrest_tab$sex) ## ## F M ## 19431 85095 The result of a coin flip is categorical: heads or tails. The outcome of rolling an 8-sided die is categorical: one, two, …, eight. Can you think of other examples? Categorical data may be unordered or ordered. In our example dataset all categorical data is unordered, e.g., sex, race, etc. Examples of ordered categorical data are grades in a class, Likert scale categories, e.g., strongly agree, agree, neutral, disagree, strongly disagree, etc. 5.4.1 Factors in R We said that R is designed for data analysis. My favorite example of how that manifests itself is the factor datatype. If you look at your dataset now, arrest_tab$sex is a vector of strings: class(arrest_tab$sex) ## [1] &quot;character&quot; summary(arrest_tab$sex) ## Length Class Mode ## 104528 character character However, as a measurement, or attribute, it should only take one of two values (or three depending on how you record missing, unknown or unspecified). So, in R, that categorical data type is called a factor. Notice what the summary function does after turning the sex attribute into a factor: arrest_tab$sex &lt;- factor(arrest_tab$sex) summary(arrest_tab$sex) ## F M NA&#39;s ## 19431 85095 2 This distinction shows up in many other places where functions have different behavior when called on different types of values. The possible values a factor can take are called levels: levels(arrest_tab$sex) ## [1] &quot;F&quot; &quot;M&quot; Exercise: you should transform the race attribute into a factor as well. How many levels does it have? 5.5 Discrete numeric attributes These are attributes that can take specific values from elements of ordered, discrete (possibly infinite) sets. The most common set in this case would be the non-negative positive integers. This data is commonly the result of counting processes. In our example dataset, age, measured in years, is a discrete attribute. Frequently, we obtain datasets as the result of summarizing, or aggregating other underlying data. In our case, we could construct a new dataset containing the number of arrests per neighborhood (we will see how to do this later) ## # A tibble: 6 x 2 ## neighborhood number_of_arrests ## &lt;chr&gt; &lt;int&gt; ## 1 Abell 62 ## 2 Allendale 297 ## 3 Arcadia 78 ## 4 Arlington 694 ## 5 Armistead Gardens 153 ## 6 Ashburton 78 In this new dataset, the entities are each neighborhood, the number_of_arrests attribute is a discrete numeric attribute. Other examples: the number of students in a class is discrete, the number of friends for a specific Facebook user. Can you think of other examples? Distinctions between ordered categorical and discrete numerical data is that ordered categorical data do not have magnitude. For instance, is an ‘A’ in a class twice as good as a ‘C’? Is a ‘C’ twice as good as a ‘D’? Not necessarily. Grades don’t have an inherent magnitude. However, if we encode grades as ‘F=0,D=1,C=2,B=3,A=4’, etc. they do have magnitude. In that case, an ‘A’ is twice as good as a ‘C’, and a ‘C’ is twice as good as a ‘D’. So in summary, if ordered data has magnitude, then discrete numeric if not, ordered categorical. 5.6 Continuous numeric data These are attributes that can take any value in a continuous set. For example, a person’s height, in say inches, can take any number (within the range of human heights). Here is another dataset we can use to look at this datatype. In this case, entities are cars and we look at continuous numeric attributes speed and stopping distance: The distinction between continuous and discrete is a bit tricky since measurements that have finite precision are, in a sense, discrete. Remember, however, that continuity is not a property of the specific dataset you have in hand, but rather of the process you are measuring. The number of arrests in a neighborhood cannot, in principle, be fractional, regardless of the precision at which we measure this. If we had the appropriate tool, we could measure a person’s height with infinite precision. This distinction is very important when we build statistical models of datasets for analysis. For now, think of discrete data as the result of counting, and continuous data the result of some physical measurement. 5.7 Other examples Consider a dataset of images like the super-famous MNIST dataset of handwritten digits. This dataset contains images of handwritten digits. So each image is an entity. Each image has a label attribute which states which of the digits 0,1,…9 is represented by the image. What type of data is this (categorical, continuous numeric, or discrete numeric)? Now, each image is represented by grayscale values in a 28x28 grid. That’s 784 attributes, one for each square in the grid, containing a grayscale value. Now what type of data are these other 784 attributes? 5.8 Other important datatypes The three datatypes we saw above encompass a fairly large swath of data you will come across. Our arrest dataset contains other important datatypes that we will run across frequently: Text: Arbitrary strings that do not encode a categorical attribute. Datetime: Date and time of some event or observation (e.g., arrestDate, arrestTime) Geolocation: Latitude and Longitude of some event or observation (e.g., Location.) 5.9 Units Something that we tend to forget but is extremely important for the modeling and interpretation of data is that attributes are for the most part measurements and that they have units. For example, age of a person can be measured in different units: years, months, etc. These can be converted to one another, but nonetheless in a given dataset, that attribute or measurement will be recorded in some specific units. Similar arguments go for distances and times, for example. In other cases, we may have unitless measurements (we will see later an example of this when we do dimensionality reduction). In these cases, it is worth thinking about why your measurements are unit-less. When performing analyses that try to summarize the effect of some measurement or attribute on another, units matter a lot! We will see the importance of this in our regression section. For now, make sure you make a mental note of units for each measurement you come across. This will force you to think about where and how your data was obtained, which will become very important when modeling and interpreting the results of these models. "],
["principles-basic-operations.html", "6 Principles: Basic Operations 6.1 Operations that select attributes 6.2 Operations that select entities 6.3 Pipelines of operations", " 6 Principles: Basic Operations Now that we have a data frame describing our data, let’s learn a few fundamental operations we perform on data frames on almost any analysis. We divide these first set of operations into two groups: operations on attributes and operations on entitites. These operations are defined in the dplyr package, part of the tidyverse, and are described in more detail in the “R for Data Science” textbook available in the course logistics page: http://r4ds.had.co.nz/transform.html. 6.1 Operations that select attributes 6.1.1 select In our data set we have a large number of attributes describing each arrest. Now, suppose we only want to study patterns in these arrests based on a smaller number of attributes for purposes of efficiency, since we would operate over less data, or interpretability. In that case we would like to create a data frame that contains only those attributes of interest. We use the select function for this. Let’s create a data frame containing only the age, sex and district attributes select(arrest_tab, age, sex, district) ## # A tibble: 104,528 x 3 ## age sex district ## &lt;int&gt; &lt;fct&gt; &lt;chr&gt; ## 1 23 M &lt;NA&gt; ## 2 37 M SOUTHERN ## 3 46 M NORTHEASTERN ## 4 50 M WESTERN ## 5 33 M NORTHERN ## 6 41 M SOUTHERN ## 7 29 M WESTERN ## 8 20 M NORTHEASTERN ## 9 24 M &lt;NA&gt; ## 10 53 M NORTHWESTERN ## # ... with 104,518 more rows The first argument to the select function is the data frame we want to operate on, the remaining arguments describe the attributes we want to include in the resulting data frame. Note a few other things: The first argument to select is a data frame, and the value returned by select is also a data frame As always you can learn more about the function using ?select Attribute descriptor arguments can be fairly sophisticated. For example, we can use positive integers to indicate attribute (column) indices: select(arrest_tab, 1, 3, 4) ## # A tibble: 104,528 x 3 ## arrest race sex ## &lt;int&gt; &lt;chr&gt; &lt;fct&gt; ## 1 11126858 B M ## 2 11127013 B M ## 3 11126887 B M ## 4 11126873 B M ## 5 11126968 B M ## 6 11127041 B M ## 7 11126932 B M ## 8 11126940 W M ## 9 11127051 B M ## 10 11127018 B M ## # ... with 104,518 more rows R includes a useful operator to describe ranges. E.g., 1:5 would be attributes 1 through 5: select(arrest_tab, 1:5) ## # A tibble: 104,528 x 5 ## arrest age race sex arrestDate ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; ## 1 11126858 23 B M 01/01/2011 ## 2 11127013 37 B M 01/01/2011 ## 3 11126887 46 B M 01/01/2011 ## 4 11126873 50 B M 01/01/2011 ## 5 11126968 33 B M 01/01/2011 ## 6 11127041 41 B M 01/01/2011 ## 7 11126932 29 B M 01/01/2011 ## 8 11126940 20 W M 01/01/2011 ## 9 11127051 24 B M 01/01/2011 ## 10 11127018 53 B M 01/01/2011 ## # ... with 104,518 more rows We can also use other helper functions to create attribute descriptors. For example, to choose all attributes that begin with the letter a we can the starts_with function which uses partial string matching: select(arrest_tab, starts_with(&quot;a&quot;)) ## # A tibble: 104,528 x 5 ## arrest age arrestDate arrestTime arrestLocation ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;time&gt; &lt;chr&gt; ## 1 11126858 23 01/01/2011 00&#39;00&quot; &lt;NA&gt; ## 2 11127013 37 01/01/2011 01&#39;00&quot; 2000 Wilkens Ave ## 3 11126887 46 01/01/2011 01&#39;00&quot; 2800 Mayfield Ave ## 4 11126873 50 01/01/2011 04&#39;00&quot; 2100 Ashburton St ## 5 11126968 33 01/01/2011 05&#39;00&quot; 4000 Wilsby Ave ## 6 11127041 41 01/01/2011 05&#39;00&quot; 2900 Spellman Rd ## 7 11126932 29 01/01/2011 05&#39;00&quot; 800 N Monroe St ## 8 11126940 20 01/01/2011 05&#39;00&quot; 5200 Moravia Rd ## 9 11127051 24 01/01/2011 07&#39;00&quot; 2400 Gainsdbourgh Ct ## 10 11127018 53 01/01/2011 15&#39;00&quot; 3300 Woodland Ave ## # ... with 104,518 more rows We can also use the attribute descriptor arguments to drop attributes. For instance using descriptor -age returns the arrest data frame with all but the age attribute included: select(arrest_tab, -age) ## # A tibble: 104,528 x 14 ## arrest race sex arrestDate arrestTime arrestLocation incidentOffense ## &lt;int&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;time&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1.11e⁷ B M 01/01/2011 00&#39;00&quot; &lt;NA&gt; Unknown Offense ## 2 1.11e⁷ B M 01/01/2011 01&#39;00&quot; 2000 Wilkens … 79-Other ## 3 1.11e⁷ B M 01/01/2011 01&#39;00&quot; 2800 Mayfield… Unknown Offense ## 4 1.11e⁷ B M 01/01/2011 04&#39;00&quot; 2100 Ashburto… 79-Other ## 5 1.11e⁷ B M 01/01/2011 05&#39;00&quot; 4000 Wilsby A… Unknown Offense ## 6 1.11e⁷ B M 01/01/2011 05&#39;00&quot; 2900 Spellman… 81-Recovered P… ## 7 1.11e⁷ B M 01/01/2011 05&#39;00&quot; 800 N Monroe … 79-Other ## 8 1.11e⁷ W M 01/01/2011 05&#39;00&quot; 5200 Moravia … Unknown Offense ## 9 1.11e⁷ B M 01/01/2011 07&#39;00&quot; 2400 Gainsdbo… 54-Armed Person ## 10 1.11e⁷ B M 01/01/2011 15&#39;00&quot; 3300 Woodland… 54-Armed Person ## # ... with 104,518 more rows, and 7 more variables: ## # incidentLocation &lt;chr&gt;, charge &lt;chr&gt;, chargeDescription &lt;chr&gt;, ## # district &lt;chr&gt;, post &lt;int&gt;, neighborhood &lt;chr&gt;, `Location 1` &lt;chr&gt; 6.1.2 rename To improve interpretability during an analysis we may want to rename attributes. We use the rename function for this: rename(arrest_tab, arrest_date=arrestDate) ## # A tibble: 104,528 x 15 ## arrest age race sex arrest_date arrestTime arrestLocation ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;time&gt; &lt;chr&gt; ## 1 11126858 23 B M 01/01/2011 00&#39;00&quot; &lt;NA&gt; ## 2 11127013 37 B M 01/01/2011 01&#39;00&quot; 2000 Wilkens Ave ## 3 11126887 46 B M 01/01/2011 01&#39;00&quot; 2800 Mayfield Ave ## 4 11126873 50 B M 01/01/2011 04&#39;00&quot; 2100 Ashburton St ## 5 11126968 33 B M 01/01/2011 05&#39;00&quot; 4000 Wilsby Ave ## 6 11127041 41 B M 01/01/2011 05&#39;00&quot; 2900 Spellman Rd ## 7 11126932 29 B M 01/01/2011 05&#39;00&quot; 800 N Monroe St ## 8 11126940 20 W M 01/01/2011 05&#39;00&quot; 5200 Moravia Rd ## 9 11127051 24 B M 01/01/2011 07&#39;00&quot; 2400 Gainsdbourgh Ct ## 10 11127018 53 B M 01/01/2011 15&#39;00&quot; 3300 Woodland Ave ## # ... with 104,518 more rows, and 8 more variables: incidentOffense &lt;chr&gt;, ## # incidentLocation &lt;chr&gt;, charge &lt;chr&gt;, chargeDescription &lt;chr&gt;, ## # district &lt;chr&gt;, post &lt;int&gt;, neighborhood &lt;chr&gt;, `Location 1` &lt;chr&gt; Like select, the first argument to the function is the data frame we are operating on. The remaining arguemnts specify attributes to rename and the name they will have in the resulting data frame. Note that arguments in this case are named (have the form lhs=rhs). We can have selection and renaming by using named arguments in select: select(arrest_tab, age, sex, arrest_date=arrestDate) ## # A tibble: 104,528 x 3 ## age sex arrest_date ## &lt;int&gt; &lt;fct&gt; &lt;chr&gt; ## 1 23 M 01/01/2011 ## 2 37 M 01/01/2011 ## 3 46 M 01/01/2011 ## 4 50 M 01/01/2011 ## 5 33 M 01/01/2011 ## 6 41 M 01/01/2011 ## 7 29 M 01/01/2011 ## 8 20 M 01/01/2011 ## 9 24 M 01/01/2011 ## 10 53 M 01/01/2011 ## # ... with 104,518 more rows Also like select, the result of calling rename is a data frame. In fact, this will be the case for almost all operations in the tidyverse they operate on data frames (specified as the first ment in the function call) and return data frames. 6.2 Operations that select entities Next, we look at operations that select entities from a data frame. We will see a few operations to do this: selecting specific entities (rows) by position, selecting them based on attribute properties, and random sampling. 6.2.1 slice We can choose specific entities by their row position. For instance, to choose entities in rows 1,3 and 10, we would use the following: slice(arrest_tab, c(1, 3, 10)) ## # A tibble: 3 x 15 ## arrest age race sex arrestDate arrestTime arrestLocation ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;time&gt; &lt;chr&gt; ## 1 11126858 23 B M 01/01/2011 00&#39;00&quot; &lt;NA&gt; ## 2 11126887 46 B M 01/01/2011 01&#39;00&quot; 2800 Mayfield Ave ## 3 11127018 53 B M 01/01/2011 15&#39;00&quot; 3300 Woodland Ave ## # ... with 8 more variables: incidentOffense &lt;chr&gt;, ## # incidentLocation &lt;chr&gt;, charge &lt;chr&gt;, chargeDescription &lt;chr&gt;, ## # district &lt;chr&gt;, post &lt;int&gt;, neighborhood &lt;chr&gt;, `Location 1` &lt;chr&gt; As before, the first argument is the data frame to operate on. The second argument is a vector of indices. We used the c function (for concatenate) to create a vector of indices. We can also use the range operator here: slice(arrest_tab, 1:5) ## # A tibble: 5 x 15 ## arrest age race sex arrestDate arrestTime arrestLocation ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;time&gt; &lt;chr&gt; ## 1 11126858 23 B M 01/01/2011 00&#39;00&quot; &lt;NA&gt; ## 2 11127013 37 B M 01/01/2011 01&#39;00&quot; 2000 Wilkens Ave ## 3 11126887 46 B M 01/01/2011 01&#39;00&quot; 2800 Mayfield Ave ## 4 11126873 50 B M 01/01/2011 04&#39;00&quot; 2100 Ashburton St ## 5 11126968 33 B M 01/01/2011 05&#39;00&quot; 4000 Wilsby Ave ## # ... with 8 more variables: incidentOffense &lt;chr&gt;, ## # incidentLocation &lt;chr&gt;, charge &lt;chr&gt;, chargeDescription &lt;chr&gt;, ## # district &lt;chr&gt;, post &lt;int&gt;, neighborhood &lt;chr&gt;, `Location 1` &lt;chr&gt; To create general sequences of indices we would use the seq function. For example, to select entities in even positions we would use the following: slice(arrest_tab, seq(2, nrow(arrest_tab), by=2)) ## # A tibble: 52,264 x 15 ## arrest age race sex arrestDate arrestTime arrestLocation ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;time&gt; &lt;chr&gt; ## 1 11127013 37 B M 01/01/2011 01&#39;00&quot; 2000 Wilkens Ave ## 2 11126873 50 B M 01/01/2011 04&#39;00&quot; 2100 Ashburton St ## 3 11127041 41 B M 01/01/2011 05&#39;00&quot; 2900 Spellman Rd ## 4 11126940 20 W M 01/01/2011 05&#39;00&quot; 5200 Moravia Rd ## 5 11127018 53 B M 01/01/2011 15&#39;00&quot; 3300 Woodland Ave ## 6 11126892 25 B M 01/01/2011 20&#39;00&quot; 2800 Violet Ave ## 7 11126970 50 B M 01/01/2011 40&#39;00&quot; 2600 Oswego Ave ## 8 11126929 40 B M 01/01/2011 40&#39;00&quot; 3900 Greenmount Ave ## 9 11126930 30 B M 01/01/2011 40&#39;00&quot; 900 N Calhoun St ## 10 11126960 53 B M 01/01/2011 40&#39;00&quot; 900 N Calhoun St ## # ... with 52,254 more rows, and 8 more variables: incidentOffense &lt;chr&gt;, ## # incidentLocation &lt;chr&gt;, charge &lt;chr&gt;, chargeDescription &lt;chr&gt;, ## # district &lt;chr&gt;, post &lt;int&gt;, neighborhood &lt;chr&gt;, `Location 1` &lt;chr&gt; 6.2.2 filter We can also select entities based on attribute properties. For example, to select arrests where age is less than 18 years old, we would use the following: filter(arrest_tab, age &lt; 18) ## # A tibble: 463 x 15 ## arrest age race sex arrestDate arrestTime arrestLocation ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;time&gt; &lt;chr&gt; ## 1 11127698 17 B M 01/03/2011 15:00 &lt;NA&gt; ## 2 11130540 17 B M 01/07/2011 18:40 500 N Athol St ## 3 11131601 17 A M 01/10/2011 22:00 &lt;NA&gt; ## 4 11133119 17 B M 01/13/2011 01:00 &lt;NA&gt; ## 5 11133667 17 B F 01/13/2011 13:40 1400 N Wilmer Ct ## 6 11133622 17 B M 01/13/2011 18:40 1400 Wilmer St ## 7 11135112 14 B M 01/17/2011 21:57 &lt;NA&gt; ## 8 11135776 17 B M 01/18/2011 15:00 &lt;NA&gt; ## 9 11135897 17 B M 01/18/2011 15:26 900 Seagull Ave ## 10 11135844 16 B M 01/18/2011 16:00 2300 N Charles St ## # ... with 453 more rows, and 8 more variables: incidentOffense &lt;chr&gt;, ## # incidentLocation &lt;chr&gt;, charge &lt;chr&gt;, chargeDescription &lt;chr&gt;, ## # district &lt;chr&gt;, post &lt;int&gt;, neighborhood &lt;chr&gt;, `Location 1` &lt;chr&gt; You know by now what the first argument is… The second argument is an expression that evaluates to a logical value (TRUE or FALSE), if the expression evaluates to TRUE for a given entity (row) then that entity (row) is part of the resulting data frame. Operators used frequently include: ==, !=: tests equality and inequality respectively (categorical, numerical, datetimes, etc.) &lt;, &gt;, &lt;=, &gt;=: tests order relationships for ordered data types (not categorical) !, &amp;, |: not, and, or, logical operators To select arrests with ages between 18 and 25 we can use filter(arrest_tab, age &gt;= 18 &amp; age &lt;= 25) ## # A tibble: 35,770 x 15 ## arrest age race sex arrestDate arrestTime arrestLocation ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;time&gt; &lt;chr&gt; ## 1 11126858 23 B M 01/01/2011 00:00 &lt;NA&gt; ## 2 11126940 20 W M 01/01/2011 00:05 5200 Moravia Rd ## 3 11127051 24 B M 01/01/2011 00:07 2400 Gainsdbourgh Ct ## 4 11126892 25 B M 01/01/2011 00:20 2800 Violet Ave ## 5 11126963 24 B M 01/01/2011 00:40 3900 Greenmount Ave ## 6 11126942 20 B M 01/01/2011 01:22 1900 Ashburton Ave ## 7 11127087 23 B M 01/01/2011 01:30 2600 Aisquith St ## 8 11127065 22 A M 01/01/2011 01:40 &lt;NA&gt; ## 9 11126941 20 W M 01/01/2011 02:00 300 S Bentalou St ## 10 11126955 20 B M 01/01/2011 02:20 900 Myrtle Ave ## # ... with 35,760 more rows, and 8 more variables: incidentOffense &lt;chr&gt;, ## # incidentLocation &lt;chr&gt;, charge &lt;chr&gt;, chargeDescription &lt;chr&gt;, ## # district &lt;chr&gt;, post &lt;int&gt;, neighborhood &lt;chr&gt;, `Location 1` &lt;chr&gt; The filter function can take multiple logical expressions. In this case they are combined with &amp;. So the above is equivalent to filter(arrest_tab, age &gt;= 18, age &lt;= 25) ## # A tibble: 35,770 x 15 ## arrest age race sex arrestDate arrestTime arrestLocation ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;time&gt; &lt;chr&gt; ## 1 11126858 23 B M 01/01/2011 00:00 &lt;NA&gt; ## 2 11126940 20 W M 01/01/2011 00:05 5200 Moravia Rd ## 3 11127051 24 B M 01/01/2011 00:07 2400 Gainsdbourgh Ct ## 4 11126892 25 B M 01/01/2011 00:20 2800 Violet Ave ## 5 11126963 24 B M 01/01/2011 00:40 3900 Greenmount Ave ## 6 11126942 20 B M 01/01/2011 01:22 1900 Ashburton Ave ## 7 11127087 23 B M 01/01/2011 01:30 2600 Aisquith St ## 8 11127065 22 A M 01/01/2011 01:40 &lt;NA&gt; ## 9 11126941 20 W M 01/01/2011 02:00 300 S Bentalou St ## 10 11126955 20 B M 01/01/2011 02:20 900 Myrtle Ave ## # ... with 35,760 more rows, and 8 more variables: incidentOffense &lt;chr&gt;, ## # incidentLocation &lt;chr&gt;, charge &lt;chr&gt;, chargeDescription &lt;chr&gt;, ## # district &lt;chr&gt;, post &lt;int&gt;, neighborhood &lt;chr&gt;, `Location 1` &lt;chr&gt; 6.2.3 sample_n and sample_frac Frequently we will want to choose entities from a data frame at random. The sample_n function selects a specific number of entities at random: sample_n(arrest_tab, 10) ## # A tibble: 10 x 15 ## arrest age race sex arrestDate arrestTime arrestLocation ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;time&gt; &lt;chr&gt; ## 1 NA 56 B M 03/25/2011 15:00 &lt;NA&gt; ## 2 11229048 24 B M 05/29/2011 09:50 4100 Eastmont St ## 3 11362680 46 B M 11/05/2011 11:30 &lt;NA&gt; ## 4 11254391 20 B M 06/28/2011 01:30 &lt;NA&gt; ## 5 11155318 68 B M 02/18/2011 17:30 2100 Greenmount Ave ## 6 12551811 21 B M 09/13/2012 15:05 100 W Hamburg St ## 7 11127543 55 B M 01/03/2011 10:30 &lt;NA&gt; ## 8 11319366 20 B M 09/12/2011 12:45 2500 Ashland Ave ## 9 11309198 43 B M 08/30/2011 13:00 &lt;NA&gt; ## 10 12555421 59 B M 09/19/2012 18:33 &lt;NA&gt; ## # ... with 8 more variables: incidentOffense &lt;chr&gt;, ## # incidentLocation &lt;chr&gt;, charge &lt;chr&gt;, chargeDescription &lt;chr&gt;, ## # district &lt;chr&gt;, post &lt;int&gt;, neighborhood &lt;chr&gt;, `Location 1` &lt;chr&gt; The sample_frac function selects a fraction of entitites at random: sample_frac(arrest_tab, .1) ## # A tibble: 10,453 x 15 ## arrest age race sex arrestDate arrestTime arrestLocation ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;time&gt; &lt;chr&gt; ## 1 12557031 43 B M 09/21/2012 11:30 2200 Orem Ave ## 2 11306195 43 B M 08/25/2011 21:00 500 Gold St ## 3 12567626 21 W M 10/10/2012 01:03 &lt;NA&gt; ## 4 12561604 19 B M 09/30/2012 00:17 4400 Chalet Ct ## 5 12540277 35 W M 08/27/2012 12:35 &lt;NA&gt; ## 6 12517735 19 B M 07/22/2012 08:00 &lt;NA&gt; ## 7 NA 23 B M 06/09/2011 13:00 &lt;NA&gt; ## 8 11148079 57 B M 02/08/2011 13:00 1200 N Patterson Park ## 9 NA 46 W M 05/31/2011 21:30 0 E.Randall St ## 10 11288108 51 B F 08/04/2011 19:00 200 N Amity St ## # ... with 10,443 more rows, and 8 more variables: incidentOffense &lt;chr&gt;, ## # incidentLocation &lt;chr&gt;, charge &lt;chr&gt;, chargeDescription &lt;chr&gt;, ## # district &lt;chr&gt;, post &lt;int&gt;, neighborhood &lt;chr&gt;, `Location 1` &lt;chr&gt; 6.3 Pipelines of operations All of the functions implementing our first set of operations have the same argument/value structure. They take a data frame as a first argument and return a data frame. We refer to this as the data–&gt;transform–&gt;data pattern. This is the core a lot of what we will do in class as part of data analyses. Specifically, we will combine operations into pipelines that manipulate data frames. The dplyr package introduces syntactic sugar to make this pattern explicit. For instance, we can rewrite the sample_frac example using the “pipe” operator %&gt;%: arrest_tab %&gt;% sample_frac(.1) ## # A tibble: 10,453 x 15 ## arrest age race sex arrestDate arrestTime arrestLocation ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;time&gt; &lt;chr&gt; ## 1 12496622 32 B F 06/14/2012 18:40 1000 N Gay St ## 2 NA 24 B M 03/23/2011 07:00 &lt;NA&gt; ## 3 12503434 26 B F 06/26/2012 11:28 3200 Tioga Pkwy ## 4 11207166 39 B M 04/30/2011 20:40 1000 Bennett Pl ## 5 11170202 35 B M 03/13/2011 16:25 5500 O&#39;Donnell St ## 6 11307754 43 B F 08/29/2011 02:00 1800 E 29 St ## 7 11236850 45 W M 06/08/2011 04:00 &lt;NA&gt; ## 8 11200468 27 B M 04/22/2011 09:30 &lt;NA&gt; ## 9 11199762 31 U M 04/21/2011 12:30 300 S Gilmor St ## 10 11251398 29 B M 06/23/2011 14:45 &lt;NA&gt; ## # ... with 10,443 more rows, and 8 more variables: incidentOffense &lt;chr&gt;, ## # incidentLocation &lt;chr&gt;, charge &lt;chr&gt;, chargeDescription &lt;chr&gt;, ## # district &lt;chr&gt;, post &lt;int&gt;, neighborhood &lt;chr&gt;, `Location 1` &lt;chr&gt; The %&gt;% binary operator takes the value to its left and inserts it as the first argument of the function call to its right. So the expression LHS %&gt;% f(another_argument) is equivalent to the expression f(LHS, another_argument). Using the %&gt;% operator and the data–&gt;transform–&gt;data pattern of the functions we’ve seen so far, we can create pipelines. For example, let’s create a pipeline that: filters our dataset to arrests between the ages of 18 and 25 selects attributes sex, district and arrestDate (renamed as arrest_date) samples 50% of those arrests at random We will assign the result to variable analysis_tab analysis_tab &lt;- arrest_tab %&gt;% filter(age &gt;= 18, age &lt;= 25) %&gt;% select(sex, district, arrest_date=arrestDate) %&gt;% sample_frac(.5) analysis_tab ## # A tibble: 17,885 x 3 ## sex district arrest_date ## &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; ## 1 M EASTERN 05/14/2011 ## 2 F NORTHEASTERN 03/31/2012 ## 3 M WESTERN 06/06/2012 ## 4 M NORTHEASTERN 08/09/2011 ## 5 M WESTERN 12/15/2011 ## 6 F NORTHWESTERN 09/19/2012 ## 7 M WESTERN 03/24/2011 ## 8 M &lt;NA&gt; 08/24/2011 ## 9 F SOUTHEASTERN 01/13/2011 ## 10 M &lt;NA&gt; 05/27/2011 ## # ... with 17,875 more rows "],
["principles-more-operations.html", "7 Principles: More Operations 7.1 Operations that sort entities 7.2 Operations that create new attributes 7.3 Operations that summarize attribute values over entities 7.4 Operations that group entities 7.5 Vectors 7.6 Attributes as vectors 7.7 Functions", " 7 Principles: More Operations In the previous section we introduced our first few operations to manipulate data frames. Next, we learn a few more: sorting, creating new attributes, summarizing and grouping. Finally we will take a short detour through a discussion on vectors. 7.1 Operations that sort entities The first operation we will look at today is used to sort entities based on their attribute values. As an example, suppose we wanted to find the arrests with the 10 youngest subjects. If we had an operation that re-orders entities based on the value of their age attribute, we can then use the slice operation we saw before to create a data frame with just the entities of interest arrest_tab %&gt;% arrange(age) %&gt;% slice(1:10) ## # A tibble: 10 x 15 ## arrest age race sex arrestDate arrestTime arrestLocation ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;time&gt; &lt;chr&gt; ## 1 11139008 0 B F 01/24/2011 12:45 3700 Garrison Blvd ## 2 11176627 0 W M 03/22/2011 08:00 &lt;NA&gt; ## 3 NA 0 &lt;NA&gt; &lt;NA&gt; 03/28/2011 15:25 3600 Townsend Ave ## 4 NA 0 B M 03/30/2011 16:00 &lt;NA&gt; ## 5 NA 0 W F 04/07/2011 20:30 &lt;NA&gt; ## 6 11222179 0 W F 05/20/2011 02:00 &lt;NA&gt; ## 7 11249315 0 B M 06/21/2011 21:15 2700 Harford Rd ## 8 11313422 0 B M 09/04/2011 10:30 &lt;NA&gt; ## 9 11334234 0 B M 09/28/2011 21:16 500 E Lexington St ## 10 11379943 0 &lt;NA&gt; &lt;NA&gt; 12/02/2011 19:20 4600 Park Heights St ## # ... with 8 more variables: incidentOffense &lt;chr&gt;, ## # incidentLocation &lt;chr&gt;, charge &lt;chr&gt;, chargeDescription &lt;chr&gt;, ## # district &lt;chr&gt;, post &lt;int&gt;, neighborhood &lt;chr&gt;, `Location 1` &lt;chr&gt; The arrange operation sorts entities by increasing value of the attributes passed as arguments. The desc helper function is used to indicate sorting by decreasing value. For example, to find the arrests with the 10 oldest subjects we would use: arrest_tab %&gt;% arrange(desc(age)) %&gt;% slice(1:10) ## # A tibble: 10 x 15 ## arrest age race sex arrestDate arrestTime arrestLocation ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;time&gt; &lt;chr&gt; ## 1 11307549 87 B M 08/28/2011 15:00 3200 E Baltimore St ## 2 12532020 87 B M 08/14/2012 07:25 &lt;NA&gt; ## 3 11157007 86 B M 02/22/2011 15:15 700 W Saratoga St ## 4 11139456 85 W M 01/25/2011 08:05 3100 Pulaski Hwy ## 5 11163616 85 B M 03/04/2011 09:23 2000 Ellsworth St ## 6 11393537 84 B M 12/27/2011 14:15 2000 W Pratt St ## 7 NA 84 B M 01/11/2012 11:00 &lt;NA&gt; ## 8 12584062 84 B M 11/08/2012 06:40 &lt;NA&gt; ## 9 11220464 82 B M 05/17/2011 19:50 1300 Mchenry St ## 10 11331698 80 B M 09/26/2011 10:15 3000 Harlem Ave ## # ... with 8 more variables: incidentOffense &lt;chr&gt;, ## # incidentLocation &lt;chr&gt;, charge &lt;chr&gt;, chargeDescription &lt;chr&gt;, ## # district &lt;chr&gt;, post &lt;int&gt;, neighborhood &lt;chr&gt;, `Location 1` &lt;chr&gt; 7.2 Operations that create new attributes We will often see that for many analyses, be it for interpretation or for statistical modeling, we will create new attributes based on existing attributes in a dataset. Suppose I want to represent age in months rather than years in our dataset. To do so I would multiply 12 to the existing age attribute. The function mutate creates new attributes based on the result of a given expression: arrest_tab %&gt;% mutate(age_months = 12 * age) %&gt;% select(arrest, age, age_months) ## # A tibble: 104,528 x 3 ## arrest age age_months ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 11126858 23 276 ## 2 11127013 37 444 ## 3 11126887 46 552 ## 4 11126873 50 600 ## 5 11126968 33 396 ## 6 11127041 41 492 ## 7 11126932 29 348 ## 8 11126940 20 240 ## 9 11127051 24 288 ## 10 11127018 53 636 ## # ... with 104,518 more rows 7.3 Operations that summarize attribute values over entities Once we have a set of entities and attributes in a given data frame, we may need to summarize attribute values over the set of entities in the data frame. It collapses the data frame to a single row containing the desired attribute summaries. Continuing with the example we have seen below, we may want to know what the minmum, maximum and average age in the dataset is: summarize(arrest_tab, min_age=min(age), mean_age=mean(age), max_age=max(age)) ## # A tibble: 1 x 3 ## min_age mean_age max_age ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 33.2 87.0 The summarize functions takes a data frame and calls a summary function over attributes of the data frame. Common summary functions to use include: Operation(s) Result mean, median average and median attribute value, respectively sd standard deviation of attribute values min, max minimum and maximum attribute values, respectively n, n_distinct number of attribute values and number of distinct attribute values any, all for logical attributes (TRUE/FALSE): is any attribute value TRUE, or are all attribute values TRUE Let’s see the number of distinct districts in our dataset: summarize(arrest_tab, n_distinct(district)) ## # A tibble: 1 x 1 ## `n_distinct(district)` ## &lt;int&gt; ## 1 10 We may also refer to these summarization operation as aggregation since we are computing aggregates of attribute values. 7.4 Operations that group entities Summarization (therefore aggregation) goes hand in hand with data grouping, where summaries are computed conditioned on other attributes. The notion of conditioning is fundamental to data analysis and we will see it very frequently through the course. It is the basis of statistical analysis and Machine Learning models and it is essential in understanding the design of effective visualizations. The goal is to group entities with the same value of one or more attributes. The group_by function in essence annotates the rows of a data frame as belonging to a specific group based on the value of some chosen attributes. This call returns a data frame that is grouped by the value of the district attribute. group_by(arrest_tab, district) ## # A tibble: 104,528 x 15 ## # Groups: district [10] ## arrest age race sex arrestDate arrestTime arrestLocation ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;time&gt; &lt;chr&gt; ## 1 11126858 23 B M 01/01/2011 00&#39;00&quot; &lt;NA&gt; ## 2 11127013 37 B M 01/01/2011 01&#39;00&quot; 2000 Wilkens Ave ## 3 11126887 46 B M 01/01/2011 01&#39;00&quot; 2800 Mayfield Ave ## 4 11126873 50 B M 01/01/2011 04&#39;00&quot; 2100 Ashburton St ## 5 11126968 33 B M 01/01/2011 05&#39;00&quot; 4000 Wilsby Ave ## 6 11127041 41 B M 01/01/2011 05&#39;00&quot; 2900 Spellman Rd ## 7 11126932 29 B M 01/01/2011 05&#39;00&quot; 800 N Monroe St ## 8 11126940 20 W M 01/01/2011 05&#39;00&quot; 5200 Moravia Rd ## 9 11127051 24 B M 01/01/2011 07&#39;00&quot; 2400 Gainsdbourgh Ct ## 10 11127018 53 B M 01/01/2011 15&#39;00&quot; 3300 Woodland Ave ## # ... with 104,518 more rows, and 8 more variables: incidentOffense &lt;chr&gt;, ## # incidentLocation &lt;chr&gt;, charge &lt;chr&gt;, chargeDescription &lt;chr&gt;, ## # district &lt;chr&gt;, post &lt;int&gt;, neighborhood &lt;chr&gt;, `Location 1` &lt;chr&gt; Subsequent operations are then performed for each group independently. For example, when summarize is applied to a grouped data frame, summaries are computed for each group of entities, rather than the whole set of entities. For instance, let’s calculate minimum, maximum and average age for each district in our dataset: arrest_tab %&gt;% group_by(district) %&gt;% summarize(min_age=min(age), max_age=max(age), mean_age=mean(age)) ## # A tibble: 10 x 4 ## district min_age max_age mean_age ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 CENTRAL 0 86.0 33.0 ## 2 EASTERN 0 85.0 34.1 ## 3 NORTHEASTERN 0 78.0 30.4 ## 4 NORTHERN 14.0 80.0 33.1 ## 5 NORTHWESTERN 0 78.0 34.6 ## 6 SOUTHEASTERN 0 87.0 32.5 ## 7 SOUTHERN 0 84.0 32.3 ## 8 SOUTHWESTERN 0 80.0 32.4 ## 9 WESTERN 0 73.0 34.4 ## 10 &lt;NA&gt; 0 87.0 33.4 Note that after this operation we have effectively changed the entities represented in the result. The entities in our original dataset are arrests while the entities for the result of the last example are the districts. This is a general property of group_by and summarize: it defines a data set where entities are defined by distinct values of the attributes we use for grouping. Let’s look at another example combining some of the operations we have seen so far. Let’s compute the average age for subjects 21 years or older grouped by district and sex: arrest_tab %&gt;% filter(age &gt;= 21) %&gt;% group_by(district, sex) %&gt;% summarize(mean_age=mean(age)) ## # A tibble: 20 x 3 ## # Groups: district [?] ## district sex mean_age ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 CENTRAL F 35.7 ## 2 CENTRAL M 35.3 ## 3 EASTERN F 36.9 ## 4 EASTERN M 37.1 ## 5 NORTHEASTERN F 33.5 ## 6 NORTHEASTERN M 32.8 ## 7 NORTHERN F 35.9 ## 8 NORTHERN M 35.6 ## 9 NORTHWESTERN F 37.5 ## 10 NORTHWESTERN M 37.2 ## 11 SOUTHEASTERN F 33.3 ## 12 SOUTHEASTERN M 34.7 ## 13 SOUTHERN F 33.7 ## 14 SOUTHERN M 34.5 ## 15 SOUTHWESTERN F 35.4 ## 16 SOUTHWESTERN M 35.0 ## 17 WESTERN F 37.1 ## 18 WESTERN M 37.3 ## 19 &lt;NA&gt; F 34.7 ## 20 &lt;NA&gt; M 35.5 7.5 Vectors We briefly saw previously operators to create vectors in R. For instance, we can use seq to create a vector that consists of a sequence of integers: multiples_of_three &lt;- seq(3, 30, by=3) multiples_of_three ## [1] 3 6 9 12 15 18 21 24 27 30 Let’s how this is represented in R (the str is very handy to do this type of digging around): str(multiples_of_three) ## num [1:10] 3 6 9 12 15 18 21 24 27 30 So, this is a numeric vector of length 10. Like many other languages we use square brackets [] to index vectors: multiples_of_three[1] ## [1] 3 We can use ranges as before multiples_of_three[1:4] ## [1] 3 6 9 12 We can use vectors of non-negative integers for indexing: multiples_of_three[c(1,3,5)] ## [1] 3 9 15 Or even logical vectors: multiples_of_three[c(TRUE, FALSE, TRUE, FALSE, TRUE, FALSE, TRUE, FALSE, TRUE, FALSE)] ## [1] 3 9 15 21 27 In R, most operations are designed to work with vectors directly (we call that vectorized). For example, if I want to add two vectors together I would write: (look no for loop!): multiples_of_three + multiples_of_three ## [1] 6 12 18 24 30 36 42 48 54 60 This also works for other arithmetic and logical operations (e.g., -, *, /, &amp;, |). Give them a try! In data analysis the vector is probably the most fundamental data type (other than basic numbers, strings, etc.). Why? Consider getting data about one attribute, say height, for a group of people. What do you get? An array of numbers, all in the same unit (say feet, inches or centimeters). How about their name? Then you get an array of strings. Abstractly, we think of vectors as arrays of values, all of the same class or datatype. 7.6 Attributes as vectors In fact, in the data frames we have been working on, each column, corresponding to an attribute, is a vector. We use the pull function to extract a vector from a data frame. We can then operate index them, or operate on them as vectors age_vec &lt;- arrest_tab %&gt;% pull(age) age_vec[1:10] ## [1] 23 37 46 50 33 41 29 20 24 53 12 * age_vec[1:10] ## [1] 276 444 552 600 396 492 348 240 288 636 We previously saw how the $ operator serves the same function. age_vec &lt;- arrest_tab$age age_vec[1:10] ## [1] 23 37 46 50 33 41 29 20 24 53 The pull function however, can be used as part of a pipeline (using operator %&gt;%): arrest_tab %&gt;% pull(age) %&gt;% mean() ## [1] 33.19639 7.7 Functions Once we have established useful pipelines for a dataset we will want to abstract them into reusable functions that we can apply in other analyses. To do that we would write our own functions that encapsulate the pipelines we have created. As an example, take a function that executes the age by district/sex summarization we created before: summarize_district &lt;- function(df) { df %&gt;% filter(age &gt;= 21) %&gt;% group_by(district, sex) %&gt;% summarize(mean_age=mean(age)) } You can include multiple expressions in the function definition (with the brackts []). Notice there is no return statement in this function. When a function is called, it returns the value of the last expression in the function definition. In this example, it would be the data frame we get from applying the pipeline of operations. You can find more information about vectors, functions and other programming matters we might run into in class in Chapters 17-21 of R for Data Science "],
["basic-plotting-with-ggplot.html", "8 Basic plotting with ggplot 8.1 Plot Construction Details 8.2 Frequently Used Plots", " 8 Basic plotting with ggplot We will spend a good amount of time in the course discussing data visualization. It serves many important roles in data analysis. We use it to gain understanding of dataset characteristics throughout analyses and it is a key element of communicating insights we have derived from data analyses with our target audience. In this section, we will introduce basic functionality of the ggplot package to start our discussion of visualization throughout the course. The ggplot package is designed to work well with the tidyverse set of packages. As such, it is designed around the Entity-Attribute data model. Also, it can be included as part of data frame operation pipelines. Let’s start with a simple example. Let’s create a dot plot of the number of arrests per district in our dataset: arrest_tab %&gt;% group_by(district) %&gt;% summarize(num_arrests=n()) %&gt;% ggplot(mapping=aes(y=district, x=num_arrests)) + geom_point() The ggplot design is very elegant, takes some thinking to get used to, but is extremely powerful. The central premise is to characterize the building pieces behind ggplot plots as follows: The data that goes into a plot, a data frame of entities and attributes The mapping between data attributes and graphical (aesthetic) characteristics The geometric representation of these graphical characteristics So in our example we can fill in these three parts as follows: Data: We pass a data frame to the ggplot function with the %&gt;% operator at the end of the group_by-summarize pipeline. Mapping: Here we map the num_arrests attribute to the x position in the plot and the district attribute to the y position in the plot. Every ggplot will contain one or more aes calls. Geometry: Here we choose points as the geometric representations of our chosen graphical characteristics using the geom_point function. In general, the ggplot call will have the following structure: &lt;data_frame&gt; %&gt;% ggplot(mapping=aes(&lt;graphical_characteristic&gt;=&lt;attribute&gt;)) + geom_&lt;representation&gt;() 8.1 Plot Construction Details 8.1.1 Mappings Some of the graphical characteristics we will commonly map attributes to include: Argument Definition x position along x axis y position along y axis color color shape shape (applicable to e.g., points) size size label string used as label (applicable to text) 8.1.2 Representations Representations we will use frequently are Function Representation geom_point points geom_bar rectangles geom_text strings geom_smooth smoothed line (advanced) geom_hex hexagonal binning We can include multiple geometric representations in a single plot, for example points and text, by adding (+) multiple geom_&lt;representation&gt; functions. Also, we can include mappings inside a geom_ call to map characteristics to attributes strictly for that specific representation. For example geom_point(mapping=aes(color=&lt;attribute&gt;)) maps color to some attribute only for the point representation specified by that call. Mappings given in the ggplot call apply to all representations added to the plot. This cheat sheet is very handy: https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf 8.2 Frequently Used Plots We will look comprehensively at data visualization in more detail later in the course, but for now will list a few common plots we use in data analysis and how they are created using ggplot. Let’s switch data frame to the mpg dataset for our examples: mpg ## # A tibble: 234 x 11 ## manufacturer model displ year cyl trans drv cty hwy fl ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 audi a4 1.80 1999 4 auto(l… f 18 29 p ## 2 audi a4 1.80 1999 4 manual… f 21 29 p ## 3 audi a4 2.00 2008 4 manual… f 20 31 p ## 4 audi a4 2.00 2008 4 auto(a… f 21 30 p ## 5 audi a4 2.80 1999 6 auto(l… f 16 26 p ## 6 audi a4 2.80 1999 6 manual… f 18 26 p ## 7 audi a4 3.10 2008 6 auto(a… f 18 27 p ## 8 audi a4 quat… 1.80 1999 4 manual… 4 18 26 p ## 9 audi a4 quat… 1.80 1999 4 auto(l… 4 16 25 p ## 10 audi a4 quat… 2.00 2008 4 manual… 4 20 28 p ## # ... with 224 more rows, and 1 more variable: class &lt;chr&gt; 8.2.1 Scatter plot Used to visualize the relationship between two attributes. mpg %&gt;% ggplot(mapping=aes(x=displ, y=hwy)) + geom_point(mapping=aes(color=cyl)) 8.2.2 Bar graph Used to visualize the relationship between a continuous variable to a categorical (or discrete) attribute mpg %&gt;% group_by(cyl) %&gt;% summarize(mean_mpg=mean(hwy)) %&gt;% ggplot(mapping=aes(x=cyl, y=mean_mpg)) + geom_bar(stat=&quot;identity&quot;) 8.2.3 Histogram Used to visualize the distribution of the values of a numeric attribute mpg %&gt;% ggplot(mapping=aes(x=hwy)) + geom_histogram() 8.2.4 Boxplot Used to visualize the distribution of a numeric attribute based on a categorical attribute mpg %&gt;% ggplot(mapping=aes(x=class, y=hwy)) + geom_boxplot() "],
["brief-introduction-to-rmarkdown.html", "9 Brief Introduction to Rmarkdown", " 9 Brief Introduction to Rmarkdown Rstudio has created an impressive eco-system for publishing with R. It has concentrated around two systems: Rmarkdown for creating documents that include both text and data analysis code, publishable in multiple formats, and shiny, a framework for creating interactive html applications that allow users to explore data analyses. Rstudio has provided substantial tutorials and documentation for Rmarkdown: http://rmarkdown.rstudio.com/ The tutorial included in that website is quite good: http://rmarkdown.rstudio.com/lesson-1.html. In class we will go over some of the key points using the same Rmarkdown file as an example: http://rmarkdown.rstudio.com/demos/1-example.Rmd "],
["best-practices-for-data-science-projects.html", "10 Best Practices for Data Science Projects", " 10 Best Practices for Data Science Projects See this slidedeck for discussion of reproducibility, bias, ethics and responsibility along with some practical tips: Best Practices Slidedeck "],
["tidy-data-i-the-er-model.html", "11 Tidy Data I: The ER Model 11.1 Overview 11.2 The Entity-Relationship and Relational Models 11.3 Tidy Data", " 11 Tidy Data I: The ER Model Some of this material is based on Amol Deshpande’s material: https://github.com/umddb/datascience-fall14/blob/master/lecture-notes/models.md 11.1 Overview In this section we will discuss principles of preparing and organizing data in a way that is amenable for analysis, both in modeling and visualization. We think of a data model as a collection of concepts that describes how data is represented and accessed. Thinking abstractly of data structure, beyond a specific implementation, makes it easier to share data across programs and systems, and integrate data from different sources. Once we have thought about structure, we can then think about semantics: what does data represent? Structure: We have assumed that data is organized in rectangular data structures (tables with rows and columns) Semantics: We have discussed the notion of values, attributes, and entities. So far, we have used the following data semantics: a dataset is a collection of values, numeric or categorical, organized into entities (observations) and attributes (variables). Each attribute contains values of a specific measurement across entities, and entities collect all measurements across attributes. In the database literature, we call this exercise of defining structure and semantics as data modeling. In this course we use the term data representational modeling, to distinguish from data statistical modeling. The context should be sufficient to distinguish the two uses of the term data modeling. Data representational modeling is the process of representing/capturing structure in data based on defining: Data model: A collection of concepts that describes how data is represented and accessed Schema: A description of a specific collection of data, using a given data model The purpose of defining abstract data representation models is that it allows us to know the structure of the data/information (to some extent) and thus be able to write general purpose code. Lack of a data model makes it difficult to share data across programs, organizations, systems that need to be able to integrate information from multiple sources. We can also design algorithms and code that can significantly increase efficiency if we can assume general data structure. For instance, we can preprocess data to make access efficient (e.g., building a B-Tree on a field). A data model typically consists of: Modeling Constructs: A collection of concepts used to represent the structure in the data. Typically we need to represent types of entities, their attributes, types of relationships between entities, and relationship attributes Integrity Constraints: Constraints to ensure data integrity (i.e., avoid errors) Manipulation Languages: Constructs for manipulating the data We desire that models are sufficiently expressive so they can capture real-world data well, easy to use, and lend themselves to defining computational methods that have good performance. Some examples of data models are Relational, Entity-relationship model, XML… Object-oriented, Object-relational, RDF… Current favorites in the industry: JSON, Protocol Buffers, Avro, Thrift, Property Graph Why have so many models been defined? There is an inherent tension between descriptive power and ease of use/efficiency. More powerful, expressive, models can be applied to represent more datasets but also tend to be harder to use and query efficiently. Typically there are multiple levels of modeling. Physical modeling concerns itself with how the data is physically stored. Logical or Conceptual modeling concerns itself with type of information stored, the different entities, their attributes, and the relationships among those. There may be several layers of logical/conceptual models to restrict the information flow (for security and/or ease-of-use): Data independence: The idea that you can change the representation of data w/o changing programs that operate on it. Physical data independence: I can change the layout of data on disk and my programs won’t change index the data partition/distribute/replicate the data compress the data sort the data 11.2 The Entity-Relationship and Relational Models The fundamental objects in this formalism are entities and their attributes, as we have seen before, and relationships and relationship attributes. Entities are objects represented in a dataset: people, places, things, etc. Relationships model just that, relationships between entities. Here, rectangles are entitites, diamonds and edges indicate relationships. Circles describe either entity or relationship attributes. Arrows are used indicate multiplicity of relationships (one-to-one, many-to-one, one-to-many, many-to-many): Relationships are defined over pairs of entities. As such, relationship \\(R\\) over sets of entities \\(E_1\\) and \\(E_2\\) is defined over the cartesian product \\(E_1 \\times E_2\\). For example, if \\(e_1 \\in E_1\\) and \\(e_2 \\in E_2\\), then \\((e_1, e_2) \\in R\\). Arrows specify how entities participate in relationships. In particular, an arrow pointing from an entity set \\(E_1\\) (square) into a relationship over \\(E_1\\) and \\(E_2\\) (diamond) specifies that entities in \\(E_1\\) appear in only one relationship pair. That is, there is a single entity \\(e_2 \\in E_2\\) such that \\((e_1,e_2) \\in R\\). Think about what relationships are shown in this diagram? In databases and general datasets we work on, both Entities and Relationships are represented as Relations (tables) such that a unique entity/relationship is represented by a single tuple (the list of attribute values that represent an entity or relationship). This leads to the natural question of how are unique entities determined or defined. Here is where the concept of a key comes in. This is an essential aspect of the Entity-Relationship and Relational models. 11.2.1 Formal introduction to keys Attribute set \\(K\\) is a superkey of relation \\(R\\) if values for \\(K\\) are sufficient to identify a unique tuple of each possible relation \\(r(R)\\) Example: {ID} and {ID,name} are both superkeys of instructor Superkey \\(K\\) is a candidate key if \\(K\\) is minimal Example: {ID} is a candidate key for Instructor One of the candidate keys is selected to be the primary key Typically one that is small and immutable (doesn’t change often) Primary key typically highlighted Foreign key: Primary key of a relation that appears in another relation {ID} from student appears in takes, advisor student called referenced relation takes is the referencing relation Typically shown by an arrow from referencing to referenced Foreign key constraint: the tuple corresponding to that primary key must exist Imagine: Tuple: ('student101', 'CMSC302')in takes But no tuple corresponding to ‘student101’ in student Also called referential integrity constraint 11.2.1.1 Keys: Examples Married(person1-ssn, person2-ssn, date-married, date-divorced) Account(cust-ssn, account-number, cust-name, balance, cust-address) RA(student-id, project-id, superviser-id, appt-time, appt-start-date, appt-end-date) Person(Name, DOB, Born, Education, Religion, …) Information typically found on Wikipedia Pages President(name, start-date, end-date, vice-president, preceded-by, succeeded-by) Info listed on Wikipedia page summary Rider(Name, Born, Team-name, Coach, Sponsor, Year) Tour de France: Historical Rider Participation Information 11.3 Tidy Data Later in the course we will use the term Tidy Data to refer to datasets that are represented in a form that is amenable for manipulation and statistical modeling. It is very closely related to the concept of normal forms in the ER model and the process of normalization in the database literature. Here we assume we are working in the ER data model represented as relations: rectangular data structures where Each attribute (or variable) forms a column Each entity (or observation) forms a row Each type of entity (observational unit) forms a table Here is an example of a tidy dataset: library(nycflights13) head(flights) ## # A tibble: 6 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2.00 830 ## 2 2013 1 1 533 529 4.00 850 ## 3 2013 1 1 542 540 2.00 923 ## 4 2013 1 1 544 545 -1.00 1004 ## 5 2013 1 1 554 600 -6.00 812 ## 6 2013 1 1 554 558 -4.00 740 ## # ... with 12 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, ## # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, ## # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, ## # time_hour &lt;dttm&gt; it has one entity per row, a single attribute per column. Notice only information about flights are included here (e.g., no airport or airline information other than the name) in these observations. "],
["sql-i-single-table-queries.html", "12 SQL I: Single Table Queries 12.1 Group-by and summarize 12.2 Subqueries", " 12 SQL I: Single Table Queries The Structured-Query-Language (SQL) is the predominant language used in database systems. It is tailored to the Relational data representation model. SQL is a declarative language, we don’t write a procedure to compute a relation, we declare what the relation we want to compute looks like. The actual execution is determined and optimized by the database engine. However, there are clear mappings between parts of SQL queries and the operations we have defined so far as implemented in the tidyverse. The basic construct in SQL is the so-called SFW construct: select-from-where which specifies: select: which attributes you want the answer to have from: which relation (table) you want the answer to be computed from where: what conditions you want to be satisfied by the rows (tuples) of the answer E.g.: movies produced by disney in 1990: note the rename select m.title, m.year from movie m where m.studioname = &#39;disney&#39; and m.year = 1990 The select clause can contain expressions (this is paralleled by the mutate operation we saw previously) select title || ' (' || to_char(year) || ')' as titleyear select 2014 - year The where clause support a large number of different predicates and combinations thereof (this is parallel to the filter operation) year between 1990 and 1995 title like 'star wars%' title like 'star wars _' We can include ordering, e.g., find distinct movies sorted by title select distinct title from movie where studioname = &#39;disney&#39; and year = 1990 order by title; 12.1 Group-by and summarize SQL has an idiom for grouping and summarizing (conditioning as we called it before). Remember this is a very important concept that shows up in many data processing platforms What it does: Partition the tuples by the group attributes (year in this case), and do something (compute avg in this case) for each group Number of resulting tuples == Number of groups E.g., compute the average movie length by year select name, avg(length) from movie group by year 12.2 Subqueries You can nest queries as an expression in an SFW query. We refer to these “subqueries” as “nested subquery”: E.g., find movie with the maximum length select title, year from movie where movie.length = (select max(length) from movie); E.g., find movies with at least 5 stars: an example of a correlated subquery select * from movies m where 5 &gt;= (select count(*) from starsIn si where si.title = m.title and si.year = m.year); The nested subquery counts the number of actors for that movie. E.g., rank movies by their length. select title, year, (select count(*) from movies m2 where m1.length &lt;= m2.length) as rank from movies m1; Key insight: A movie is ranked 5th if there are exactly 4 movies with longer length. Most database systems support some sort of a rank keyword for doing this. Notice that the above query doesn’t work in presence of ties etc. "],
["two-table-operations.html", "13 Two-table operations 13.1 Left Join 13.2 Right Join 13.3 Inner Join 13.4 Full Join 13.5 Join conditions 13.6 Filtering Joins 13.7 SQL Constructs: Multi-table Queries", " 13 Two-table operations So far we have looked at data operations defined over single tables and data frames. In this section we look at efficient methods to combine data from multiple tables. The fundamental operation here is the join, which is a workhorse of database system design and impementation. The join operation combines rows from two tables to create a new single table, based on matching criteria specified over attributes of each of the two tables. Consider the example of joining the flights and airlines table: library(nycflights13) data(flights) data(airlines) Let’s take a look at the flights table again: flights ## # A tibble: 336,776 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2.00 830 ## 2 2013 1 1 533 529 4.00 850 ## 3 2013 1 1 542 540 2.00 923 ## 4 2013 1 1 544 545 -1.00 1004 ## 5 2013 1 1 554 600 -6.00 812 ## 6 2013 1 1 554 558 -4.00 740 ## 7 2013 1 1 555 600 -5.00 913 ## 8 2013 1 1 557 600 -3.00 709 ## 9 2013 1 1 557 600 -3.00 838 ## 10 2013 1 1 558 600 -2.00 753 ## # ... with 336,766 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; And add the airlines table: airlines ## # A tibble: 16 x 2 ## carrier name ## &lt;chr&gt; &lt;chr&gt; ## 1 9E Endeavor Air Inc. ## 2 AA American Airlines Inc. ## 3 AS Alaska Airlines Inc. ## 4 B6 JetBlue Airways ## 5 DL Delta Air Lines Inc. ## 6 EV ExpressJet Airlines Inc. ## 7 F9 Frontier Airlines Inc. ## 8 FL AirTran Airways Corporation ## 9 HA Hawaiian Airlines Inc. ## 10 MQ Envoy Air ## 11 OO SkyWest Airlines Inc. ## 12 UA United Air Lines Inc. ## 13 US US Airways Inc. ## 14 VX Virgin America ## 15 WN Southwest Airlines Co. ## 16 YV Mesa Airlines Inc. Here, we want to add airline information to each flight. We can do so by joining the attributes of the respective airline from the airlines table with the flights table based on the values of attributes flights$carrier and airlines$carrier. Specifically, every row of flights with a specific value for flights$carrier, is joined with the the corresponding row in airlines with the same value for airlines$carrier. We will see four different ways of performing this operation that differ on how non-matching observations are handled. 13.1 Left Join In a left join, all observations on left operand (LHS) are retained: flights %&gt;% left_join(airlines, by=&quot;carrier&quot;) ## # A tibble: 336,776 x 20 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2.00 830 ## 2 2013 1 1 533 529 4.00 850 ## 3 2013 1 1 542 540 2.00 923 ## 4 2013 1 1 544 545 -1.00 1004 ## 5 2013 1 1 554 600 -6.00 812 ## 6 2013 1 1 554 558 -4.00 740 ## 7 2013 1 1 555 600 -5.00 913 ## 8 2013 1 1 557 600 -3.00 709 ## 9 2013 1 1 557 600 -3.00 838 ## 10 2013 1 1 558 600 -2.00 753 ## # ... with 336,766 more rows, and 13 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, name &lt;chr&gt; RHS variables for LHS observations with no matching RHS observations are coded as NA. 13.2 Right Join All observations on right operand (RHS) are retained: flights %&gt;% right_join(airlines, by=&quot;carrier&quot;) ## # A tibble: 336,776 x 20 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 810 810 0 1048 ## 2 2013 1 1 1451 1500 -9.00 1634 ## 3 2013 1 1 1452 1455 -3.00 1637 ## 4 2013 1 1 1454 1500 -6.00 1635 ## 5 2013 1 1 1507 1515 -8.00 1651 ## 6 2013 1 1 1530 1530 0 1650 ## 7 2013 1 1 1546 1540 6.00 1753 ## 8 2013 1 1 1550 1550 0 1844 ## 9 2013 1 1 1552 1600 -8.00 1749 ## 10 2013 1 1 1554 1600 -6.00 1701 ## # ... with 336,766 more rows, and 13 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, name &lt;chr&gt; LHS variables for RHS observations with no matching LHS observations are coded as NA. 13.3 Inner Join Only observations matching on both tables are retained flights %&gt;% inner_join(airlines, by=&quot;carrier&quot;) ## # A tibble: 336,776 x 20 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2.00 830 ## 2 2013 1 1 533 529 4.00 850 ## 3 2013 1 1 542 540 2.00 923 ## 4 2013 1 1 544 545 -1.00 1004 ## 5 2013 1 1 554 600 -6.00 812 ## 6 2013 1 1 554 558 -4.00 740 ## 7 2013 1 1 555 600 -5.00 913 ## 8 2013 1 1 557 600 -3.00 709 ## 9 2013 1 1 557 600 -3.00 838 ## 10 2013 1 1 558 600 -2.00 753 ## # ... with 336,766 more rows, and 13 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, name &lt;chr&gt; 13.4 Full Join All observations are retained, regardless of matching condition flights %&gt;% full_join(airlines, by=&quot;carrier&quot;) ## # A tibble: 336,776 x 20 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2.00 830 ## 2 2013 1 1 533 529 4.00 850 ## 3 2013 1 1 542 540 2.00 923 ## 4 2013 1 1 544 545 -1.00 1004 ## 5 2013 1 1 554 600 -6.00 812 ## 6 2013 1 1 554 558 -4.00 740 ## 7 2013 1 1 555 600 -5.00 913 ## 8 2013 1 1 557 600 -3.00 709 ## 9 2013 1 1 557 600 -3.00 838 ## 10 2013 1 1 558 600 -2.00 753 ## # ... with 336,766 more rows, and 13 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, name &lt;chr&gt; All values coded as NA for non-matching observations as appropriate. 13.5 Join conditions All join operations are based on a matching condition: flights %&gt;% left_join(airlines, by=&quot;carrier&quot;) specifies to join observations where flights$carrier equals airlines$carrier. In this case, where no conditions are specified using the by argument: flights %&gt;% left_join(airlines) a natural join is perfomed. In this case all variables with the same name in both tables are used in join condition. You can also specify join conditions on arbitrary attributes using the by argument. flights %&gt;% left_join(airlines, by=c(&quot;carrier&quot; = &quot;name&quot;)) 13.6 Filtering Joins We’ve just seen mutating joins that create new tables. Filtering joins use join conditions to filter a specific table. flights %&gt;% anti_join(airlines, by=&quot;carrier&quot;) ## # A tibble: 0 x 19 ## # ... with 19 variables: year &lt;int&gt;, month &lt;int&gt;, day &lt;int&gt;, ## # dep_time &lt;int&gt;, sched_dep_time &lt;int&gt;, dep_delay &lt;dbl&gt;, arr_time &lt;int&gt;, ## # sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, ## # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, ## # distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Filters the flights table to only include flights from airlines that are not included in the airlines table. 13.7 SQL Constructs: Multi-table Queries Key idea: - Do a join to get an appropriate table - Use the constructs for single-table queries You will get used to doing all at once. For the first part, where we use a join to get an appropriate table, the general SQL construct includes: - The name of the first table to join - The type of join to do - The name of the second table to join - The join condition(s) Examples: select title, year, me.name as producerName from movies m join movieexec me where m.producer = me.id; Consider the query: select title, year, producer, count(starName) from movies join starsIn where title = starsIn.movieTitle and year = starsIn.movieYear group by title, year, producer What about movies with no stars ? Need to use outer joins select title, year, producer, count(starName) from movies left outer join starsIn on title = starsIn.movieTitle and year = starsIn.movieYear group by title, year, producer As we saw before, all tuples from ‘movies’ that have no matches in starsIn are included with NULLs (in dplyr this was NA). So, if a tuple (m1, 1990) has no match in starsIn, we get (m1, 1990, NULL) in the result and the count(starName) works correctly then. Note however that count(*) would not work correctly (NULLs can have unintuitive behavior) In most systems JOIN corresponds to an inner join, and include LEFT JOIN and RIGHT JOIN as well. "],
["sql-system-constructs.html", "14 SQL System Constructs 14.1 SQL as a Data Definition Language 14.2 Set Operations and Comparisons 14.3 Views 14.4 NULLs", " 14 SQL System Constructs Database management systems are software applications designed for very efficient manipulation of data targeting a relatively small number of operations. Since they are also defined to operate over a fairly restrictive data model, they are extremely useful in situations where data consistency and safety are required. Here are some examples of capabilities found in DBMS that help in that regard: Transactions A transaction is a sequence of queries and update statements executed as a single unit For example, transferring money from one account to another Both the deduction from one account and credit to the other account should happen, or neither should Triggers A trigger is a statement that is executed automatically by the system as a side effect of a modification to the database Integrity Constraints Predicates on the database that must always hold Key Constraints: Specifiying something is a primary key or unique 14.1 SQL as a Data Definition Language The Structured Query Language (SQL) is both a Data Definition Language and a Data Manipulation Language CREATE TABLE &lt;name&gt; ( &lt;field&gt; &lt;domain&gt;, ... ) INSERT INTO &lt;name&gt; (&lt;field names&gt;) VALUES (&lt;field values&gt;) DELETE FROM &lt;name&gt; WHERE &lt;condition&gt; UPDATE &lt;name&gt; SET &lt;field name&gt; = &lt;value&gt; WHERE &lt;condition&gt; SELECT &lt;fields&gt; FROM &lt;name&gt; WHERE &lt;condition&gt; We can create tables and specify primary key attributes which enforce integrity constraints at the system level CREATE TABLE customer ( ssn CHAR(9) PRIMARY KEY, cname CHAR(15), address CHAR(30), city CHAR(10), UNIQUE (cname, address, city)); Attribute constraints: Constraints on the values of attributes bname char(15) not null balance int not null, check (balance &gt;= 0) Referential integrity: prevent dangling tuples CREATE TABLE branch(bname CHAR(15) PRIMARY KEY, ...); CREATE TABLE loan(..., FOREIGN KEY bname REFERENCES branch); Can tell the system what to do if a referenced tuple is being deleted Global Constraints Single-table CREATE TABLE branch (..., bcity CHAR(15), assets INT, CHECK (NOT(bcity = ‘Bkln’) OR assets &gt; 5M)) Multi-table CREATE ASSERTION loan-constraint CHECK (NOT EXISTS ( SELECT * FROM loan AS L WHERE NOT EXISTS( SELECT * FROM borrower B, depositor D, account A WHERE B.cname = D.cname AND D.acct_no = A.acct_no AND L.lno = B.lno))) 14.2 Set Operations and Comparisons Set operations select name from movieExec union/intersect/minus select name from movieStar Set Comparisons select * from movies where year in [1990, 1995, 2000]; select * from movies where year not in ( select extract(year from birthdate) from MovieStar ); 14.3 Views create view DisneyMovies select * from movie m where m.studioname = &#39;disney&#39;; Can use it in any place where a tablename is used. Views are used quite extensively to: (1) simplify queries, (2) hide data (by giving users access only to specific views). Views may be materialized or not. 14.4 NULLs Value of any attribute can be NULL if value is unknown, or it is not applicable, or hidden, etc. It can lead to counterintuitive behavior. For example, the following query does not return movies where length = NULL select * from movies where length &gt;= 120 or length &lt;= 120` Aggregate operations can be especially tricky when NULLs are present. "],
["db-parting-shots.html", "15 DB Parting Shots 15.1 Database Query Optimization 15.2 JSON Data Model", " 15 DB Parting Shots 15.1 Database Query Optimization Earlier we made the distinction that SQL is a declarative language rather than a procedural language. A reason why data base systems rely on a declarative language is that it allows the system to decide how to evaluate a query most efficiently. Let’s think about this briefly. Consider a database where we have two tables Batting and Master and we want to evaluate this query: that what is the maximum batting “average” for a player from the state of California? select max(1.0 * b.H / b.AB) as best_ba from Batting as b join Master as m on b.playerId = m.playerId where b.AB &gt;= 100 and m.birthState = &quot;CA&quot; Table 15.1: 1 records best_ba 0.405701754385965 Now, let’s do the same computation using dplyr operations: library(Lahman) library(tidyverse) Here is one version that joins the two tables before filtering the rows that are included in the result. Batting %&gt;% inner_join(Master, by=&quot;playerID&quot;) %&gt;% filter(AB &gt;= 100, birthState == &quot;CA&quot;) %&gt;% mutate(AB=1.0 * H / AB) %&gt;% summarize(max(AB)) ## max(AB) ## 1 0.4057018 Here is a second version that filters the rows of the tables before joining the two tables. Batting %&gt;% filter(AB &gt;= 100) %&gt;% inner_join( Master %&gt;% filter(birthState == &quot;CA&quot;) ) %&gt;% mutate(AB = 1.0 * H / AB) %&gt;% summarize(max(AB)) ## Joining, by = &quot;playerID&quot; ## max(AB) ## 1 0.4057018 They both give the same result of course, but which one should be more efficient? In SQL we only write the one query describing our desired result, with the procedural versions with dplyr we need to think which of the two versions is more efficient. Database systems use query optimization to decide how to evaluate query efficiently. The goal of query optimization is to decide the most efficient query plan to use to evaluate a query out of the many possible candidate plans it could use. It needs to solve two problems: search the space of possible plans, approximate the cost of evaluating a specific plan. Let’s ignore the first, and discuss briefly the second. We should think of the two procedural versions above as two candidate plans that the DB system could use to evaluate the query. Query optimzation approximates what it would cost to evaluate each of the two plans and decides to use the most efficient plan. So, how does it approximate cost? A few ingredients are used: Access cost: how much will it cost to access rows that satisfy a given predicate (where clause)? Consider the Master table. In our query we only need to find rows for players born in California. Suppose we have an index based on attribute birthState, e.g. a hash table that allows us to find rows for players from a specific state very efficiently. In that case, accessing these rows using the index is much more efficient than scanning the entire table. This is why creating indexes for tables becomes important. Operation cost: how much will it cost to perform a join? There is a difference between comparing every pair of rows in order to compute a join, versus using indexes to find a small number of rows that satisfy the join condition efficiently? For example, if the Batting table has an index on playerId it will be cheaper to join with a filtered Master table, i.e., only considering rows for players born in California. Result size estimation: how many rows will we get after we perform a join? We can use information on key constraints to estimate this type of result. Additionally, these estimates also depend on the number of rows that satisfy certain predicates (e.g., number of players born in California) so systems often use histograms to make these estimates. As database system users we may create indices or key constraints that guide the query optimizer to choose more efficient queries. 15.2 JSON Data Model The Entity-Relational data model we have described so far is mostly defined for structured data: where a specific and consistent schema is assumed. Data models like XML and JSON are instead intended for semi-structured data. 15.2.0.1 XML: eXtensible Markup Language Data models like XML rely on flexible, self-describing schemas: &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;!-- Edited by XMLSpy --&gt; &lt;CATALOG&gt; &lt;CD&gt; &lt;TITLE&gt;Empire Burlesque&lt;/TITLE&gt; &lt;ARTIST&gt;Bob Dylan&lt;/ARTIST&gt; &lt;COUNTRY&gt;USA&lt;/COUNTRY&gt; &lt;COMPANY&gt;Columbia&lt;/COMPANY&gt; &lt;PRICE&gt;10.90&lt;/PRICE&gt; &lt;YEAR&gt;1985&lt;/YEAR&gt; &lt;/CD&gt; &lt;CD&gt; &lt;TITLE&gt;Hide your heart&lt;/TITLE&gt; &lt;ARTIST&gt;Bonnie Tyler&lt;/ARTIST&gt; &lt;COUNTRY&gt;UK&lt;/COUNTRY&gt; &lt;COMPANY&gt;CBS Records&lt;/COMPANY&gt; &lt;PRICE&gt;9.90&lt;/PRICE&gt; &lt;YEAR&gt;1988&lt;/YEAR&gt; &lt;/CD&gt; ... 15.2.0.2 JSON: Javascript Object Notation Very similar to XML and seems to be replacing it for many purposes { &quot;firstName&quot;: &quot;John&quot;, &quot;lastName&quot;: &quot;Smith&quot;, &quot;isAlive&quot;: true, &quot;age&quot;: 25, &quot;height_cm&quot;: 167.6, &quot;address&quot;: { &quot;streetAddress&quot;: &quot;21 2nd Street&quot;, &quot;city&quot;: &quot;New York&quot;, &quot;state&quot;: &quot;NY&quot;, &quot;postalCode&quot;: &quot;10021-3100&quot; }, &quot;phoneNumbers&quot;: [ { &quot;type&quot;: &quot;home&quot;, &quot;number&quot;: &quot;212 555-1234&quot; }, { &quot;type&quot;: &quot;office&quot;, &quot;number&quot;: &quot;646 555-4567&quot; } ], &quot;children&quot;: [], &quot;spouse&quot;: null } This is the format most contemporary data REST APIs use to transfer data. For instance, here is part of a JSON record from a Twitter stream: { &quot;created_at&quot;:&quot;Sun May 05 14:01:34+00002013&quot;, &quot;id&quot;:331046012875583488, &quot;id_str&quot;:&quot;331046012875583488&quot;, &quot;text&quot;:&quot;\\u0425\\u043e\\u0447\\u0443, \\u0447\\u0442\\u043e\\u0431 \\u0442\\u044b \\u0441\\u0434\\u0435\\u043b\\u0430\\u043b \\u0432\\u0441\\u0451 \\u043d\\u0435\\u043e\\u0431\\u0445\\u043e\\u0434\\u0438\\u043c\\u043e\\u0435.\\n \\\\,,\\\\ *_* \\/,,\\/&quot;, &quot;source&quot;:&quot;\\u003ca href=\\&quot;http:\\/\\/twitterfeed.com\\&quot;rel=\\&quot;nofollow\\&quot;\\u003etwitterfeed\\u003c\\/a\\u003e&quot;, &quot;in_reply_to_user_id_str&quot;:null, &quot;user&quot;:{ &quot;id&quot;:548422428, &quot;id_str&quot;:&quot;548422428&quot;, &quot;name&quot;:&quot;\\u0410\\u0439\\u0433\\u0435\\u0440\\u0438\\u043c \\u041f\\u043e\\u0433\\u043e\\u0434\\u0438\\u043d\\u0430&quot;, &quot;screen_name&quot;:&quot;paddybyrny&quot;, &quot;location&quot;:&quot;\\u0420\\u043e\\u0441\\u0441\\u0438\\u044f;\\u0412\\u043b\\u0430\\u0434\\u0438\\u0432\\u043e\\u0441\\u0442\\u043e\\u043a&quot;, &quot;followers_count&quot;:4188, &quot;friends_count&quot;:4281, &quot;lang&quot;:&quot;en&quot;, &quot;profile_background_image_url&quot;:&quot;http:\\/\\/a0.twimg.com\\/images\\/themes\\/theme1\\/bg.png&quot;, }, &quot;geo&quot;:null, &quot;coordinates&quot;:null, &quot;entities&quot;:{ &quot;hashtags&quot;:[],&quot;symbols&quot;:[],&quot;urls&quot;:[],&quot;user_mentions&quot;:[] },&quot;favorited&quot;:false,&quot;retweeted&quot;:false,&quot;filter_level&quot;:&quot;medium&quot;,&quot;lang&quot;:&quot;ru&quot;} "],
["ingesting-data.html", "16 Ingesting data 16.1 Structured ingestion 16.2 Scraping", " 16 Ingesting data Now that we have a better understanding of data analysis languages with tidyverse and SQL, we turn to the first significant challenge in data analysis, getting data into R in a shape that we can use to start our analysis. We will look at two types of data ingestion: structured ingestion, where we read data that is already structured, like a comma separated value (CSV) file, and scraping where we obtain data from text, usually in websites. There is an excellent discussion on data import here: http://r4ds.had.co.nz/data-import.html 16.1 Structured ingestion 16.1.1 CSV files (and similar) We saw in a previous chapter how we can use the read_csv file to read data from a CSV file into a data frame. Comma separated value (CSV) files are structured in a somewhat regular way, so reading into a data frame is straightforward. Each line in the CSV file corresponds to an observation (a row in a data frame). Each line contains values separated by a comma (,), corresponding to the variables of each observation. This ideal principle of how a CSV file is constructed is frequently violated by data contained in CSV files. To get a sense of how to deal with these cases look at the documentation of the read_csv function. For instance: the first line of the file may or may not contain the names of variables for the data frame (col_names argument). strings are quoted using ' instead of &quot; (quote argument) missing data is encoded with a non-standard code, e.g., - (na argument) values are separated by a character other than , (read_delim function) file may contain header information before the actual data so we have to skip some lines when loading the data (skip argument) You should read the documentation of the read_csv function to appreciate the complexities it can maneuver when reading data from structured text files. ?read_csv When loading a CSV file, we need to determine how to treat values for each attribute in the dataset. When we call read_csv, it guesses as to the best way to parse each attribute (e.g., is it a number, is it a factor, is it free text, how is missing data encoded). The readr package implements a set of core functions parse_* that parses vectors into different data types (e.g., parse_number, parse_datetime, parse_factor). When we call read_csv it will print it’s data types guesses and any problems it encounters. The problems function let’s you inspect parsing problems. E.g., df &lt;- read_csv(readr_example(&quot;challenge.csv&quot;)) ## Parsed with column specification: ## cols( ## x = col_integer(), ## y = col_character() ## ) ## Warning in rbind(names(probs), probs_f): number of columns of result is not ## a multiple of vector length (arg 1) ## Warning: 1000 parsing failures. ## row # A tibble: 5 x 5 col row col expected actual file expected &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; actual 1 1001 x no trailing characters .23837975086644292 &#39;/Library/Framewo… file 2 1002 x no trailing characters .41167997173033655 &#39;/Library/Framewo… row 3 1003 x no trailing characters .7460716762579978 &#39;/Library/Framewo… col 4 1004 x no trailing characters .723450553836301 &#39;/Library/Framewo… expected 5 1005 x no trailing characters .614524137461558 &#39;/Library/Framewo… ## ... ................. ... .......................................................................... ........ .......................................................................... ...... .......................................................................... .... .......................................................................... ... .......................................................................... ... .......................................................................... ........ .......................................................................... ## See problems(...) for more details. problems(df) ## # A tibble: 1,000 x 5 ## row col expected actual file ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1001 x no trailing characters .23837975086644292 &#39;/Library/Framew… ## 2 1002 x no trailing characters .41167997173033655 &#39;/Library/Framew… ## 3 1003 x no trailing characters .7460716762579978 &#39;/Library/Framew… ## 4 1004 x no trailing characters .723450553836301 &#39;/Library/Framew… ## 5 1005 x no trailing characters .614524137461558 &#39;/Library/Framew… ## 6 1006 x no trailing characters .473980569280684 &#39;/Library/Framew… ## 7 1007 x no trailing characters .5784610391128808 &#39;/Library/Framew… ## 8 1008 x no trailing characters .2415937229525298 &#39;/Library/Framew… ## 9 1009 x no trailing characters .11437866208143532 &#39;/Library/Framew… ## 10 1010 x no trailing characters .2983446326106787 &#39;/Library/Framew… ## # ... with 990 more rows The argument col_types is used to help the parser handle datatypes correctly. In class discussion: how to parse readr_example(&quot;challenge.csv&quot;) Other hints: You can read every attribute as character using col_types=cols(.default=col_character()). Combine this with type_convert to parse character attributes into other types: df &lt;- read_csv(readr_example(&quot;challenge.csv&quot;), col_types=cols(.default=col_character())) %&gt;% type_convert(cols(x=col_double(), y=col_date())) If nothing else works, you can read file lines using read_lines and then parse lines using string processing operations (which we will see shortly). 16.1.2 Excel spreadsheets Often you will need to ingest data that is stored in an Excel spreadsheet. The readxl package is used to do this. The main function for this package is the read_excel function. It contains similar arguments to the read_csv function we saw above. 16.2 Scraping Often, data we want to use is hosted as part of HTML files in webpages. The markup structure of HTML allows to parse data into tables we can use for analysis. Let’s use the Rotten Tomatoes ratings webpage for Diego Luna as an example: We can scrape ratings for his movies from this page. To do this we need to figure out how the HTML page’s markup can help us write R expressions to find this data in the page. Most web browsers have facilities to show page markup. In Google Chrome, you can use View&gt;Developer&gt;Developer Tools, and inspect the page markdown to find where the data is contained. In this example, we see that the data we want is in a &lt;table&gt; element in the page, with id filmographyTbl. Now that we have that information, we can use the rvest package to scrape this data: library(rvest) url &lt;- &quot;https://www.rottentomatoes.com/celebrity/diego_luna&quot; dl_tab &lt;- url %&gt;% read_html() %&gt;% html_node(&quot;#filmographyTbl&quot;) %&gt;% html_table() head(dl_tab) ## RATING TITLE ## 1 5% Flatliners ## 2 85% Rogue One: A Star Wars Story ## 3 88% Blood Father ## 4 62% Mr. Pig (Sr. Pig) ## 5 82% The Book of Life ## 6 39% Cesar Chavez ## CREDIT ## 1 Ray ## 2 Captain Cassian Andor ## 3 Jonah ## 4 Producer\\n \\n \\n Screenwriter\\n \\n \\n Director ## 5 Manolo ## 6 Producer\\n \\n \\n Director ## BOX OFFICE YEAR ## 1 $16.9M 2017 ## 2 $532.2M 2016 ## 3 — 2016 ## 4 — 2016 ## 5 — 2014 ## 6 $5.6M 2014 The main two functions we used here are html_node and html_table. html_node finds elements in the HTML page according to some selection criteria. Since we want the element with id=filmographyTbl we use the # selection operation since that corresponds to selection by id. Once the desired element in the page is selected, we can use the html_table function to parse the element’s text into a data frame. The argument to the html_node function uses CSS selector syntax: https://www.w3.org/TR/CSS2/selector.html On your own: If you wanted to extract the TV filmography from the page, how would you change this call? 16.2.1 Scraping from dirty HTML tables We saw above how to extract data from HTML tables. But what if the data we want to extract is not cleanly formatted as a HTML table, or is spread over multiple html pages? Let’s look at an example where we scrape titles and artists from billboard #1 songs: https://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_number-one_singles_of_2017 Let’s start by reading the HTML markup and finding the document node that contains the table we want to scrape library(rvest) url &lt;- &quot;https://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_number-one_singles_of_2017&quot; singles_tab_node &lt;- read_html(url) %&gt;% html_node(&quot;.plainrowheaders&quot;) singles_tab_node ## {xml_node} ## &lt;table class=&quot;wikitable plainrowheaders&quot; style=&quot;text-align: center&quot;&gt; ## [1] &lt;tr&gt;\\n&lt;th&gt;Issue date&lt;/th&gt;\\n&lt;th&gt;Song&lt;/th&gt;\\n&lt;th&gt;Artist(s)&lt;/th&gt;\\n&lt;th&gt;R ... ## [2] &lt;tr&gt;\\n&lt;th scope=&quot;row&quot;&gt;January 7&lt;/th&gt;\\n&lt;td&gt;&quot;&lt;a href=&quot;/wiki/Starboy_( ... ## [3] &lt;tr&gt;\\n&lt;th scope=&quot;row&quot;&gt;January 14&lt;/th&gt;\\n&lt;td&gt;&quot;&lt;a href=&quot;/wiki/Black_Be ... ## [4] &lt;tr&gt;\\n&lt;th scope=&quot;row&quot;&gt;January 21&lt;/th&gt;\\n&lt;td&gt;&quot;&lt;a href=&quot;/wiki/Bad_and_ ... ## [5] &lt;tr&gt;\\n&lt;th scope=&quot;row&quot;&gt;January 28&lt;/th&gt;\\n&lt;td bgcolor=&quot;#FFFF99&quot;&gt;&quot;&lt;a hr ... ## [6] &lt;tr&gt;\\n&lt;th scope=&quot;row&quot;&gt;February 4&lt;/th&gt;\\n&lt;td rowspan=&quot;2&quot;&gt;&quot;Bad and Bou ... ## [7] &lt;tr&gt;\\n&lt;th scope=&quot;row&quot;&gt;February 11&lt;/th&gt;\\n&lt;td&gt;&lt;sup id=&quot;cite_ref-7&quot; cl ... ## [8] &lt;tr&gt;\\n&lt;th scope=&quot;row&quot;&gt;February 18&lt;/th&gt;\\n&lt;td bgcolor=&quot;#FFFF99&quot; rowsp ... ## [9] &lt;tr&gt;\\n&lt;th scope=&quot;row&quot;&gt;February 25&lt;/th&gt;\\n&lt;td&gt;&lt;sup id=&quot;cite_ref-9&quot; cl ... ## [10] &lt;tr&gt;\\n&lt;th scope=&quot;row&quot;&gt;March 4&lt;/th&gt;\\n&lt;td&gt;&lt;sup id=&quot;cite_ref-10&quot; class ... ## [11] &lt;tr&gt;\\n&lt;th scope=&quot;row&quot;&gt;March 11&lt;/th&gt;\\n&lt;td&gt;&lt;sup id=&quot;cite_ref-11&quot; clas ... ## [12] &lt;tr&gt;\\n&lt;th scope=&quot;row&quot;&gt;March 18&lt;/th&gt;\\n&lt;td&gt;&lt;sup id=&quot;cite_ref-12&quot; clas ... ## [13] &lt;tr&gt;\\n&lt;th scope=&quot;row&quot;&gt;March 25&lt;/th&gt;\\n&lt;td&gt;&lt;sup id=&quot;cite_ref-13&quot; clas ... ## [14] &lt;tr&gt;\\n&lt;th scope=&quot;row&quot;&gt;April 1&lt;/th&gt;\\n&lt;td&gt;&lt;sup id=&quot;cite_ref-14&quot; class ... ## [15] &lt;tr&gt;\\n&lt;th scope=&quot;row&quot;&gt;April 8&lt;/th&gt;\\n&lt;td&gt;&lt;sup id=&quot;cite_ref-15&quot; class ... ## [16] &lt;tr&gt;\\n&lt;th scope=&quot;row&quot;&gt;April 15&lt;/th&gt;\\n&lt;td&gt;&lt;sup id=&quot;cite_ref-16&quot; clas ... ## [17] &lt;tr&gt;\\n&lt;th scope=&quot;row&quot;&gt;April 22&lt;/th&gt;\\n&lt;td&gt;&lt;sup id=&quot;cite_ref-17&quot; clas ... ## [18] &lt;tr&gt;\\n&lt;th scope=&quot;row&quot;&gt;April 29&lt;/th&gt;\\n&lt;td&gt;&lt;sup id=&quot;cite_ref-18&quot; clas ... ## [19] &lt;tr&gt;\\n&lt;th scope=&quot;row&quot;&gt;May 6&lt;/th&gt;\\n&lt;td&gt;&quot;&lt;a href=&quot;/wiki/Humble_(song) ... ## [20] &lt;tr&gt;\\n&lt;th scope=&quot;row&quot;&gt;May 13&lt;/th&gt;\\n&lt;td&gt;&quot;&lt;a href=&quot;/wiki/That%27s_Wha ... ## ... Since the rows of the table are not cleanly aligned, we need to extract each attribute separately. Let’s start with the dates in the first column. Since we noticed that the nodes containing dates have attribute scope we use the attribute CSS selector [scope]. dates &lt;- singles_tab_node %&gt;% html_nodes(&quot;[scope]&quot;) %&gt;% html_text() dates %&gt;% head() ## [1] &quot;January 7&quot; &quot;January 14&quot; &quot;January 21&quot; &quot;January 28&quot; &quot;February 4&quot; ## [6] &quot;February 11&quot; Next, we extract song titles, first we grab the tr (table row) nodes and extract from each the first td node using the td:first-of-type CSS selector. Notice that this gets us the header row which we remove using the magrittr::extract function. The title nodes also tell us how many rows this spans, which we grab from the rowspan attribute. title_nodes &lt;- singles_tab_node %&gt;% html_nodes(&quot;tr&quot;) %&gt;% html_node(&quot;td:first-of-type&quot;) %&gt;% magrittr::extract(-1) song_titles &lt;- title_nodes %&gt;% html_text() title_spans &lt;- title_nodes %&gt;% html_attr(&quot;rowspan&quot;) cbind(song_titles, title_spans) %&gt;% head(10) ## song_titles title_spans ## [1,] &quot;\\&quot;Starboy\\&quot;&quot; NA ## [2,] &quot;\\&quot;Black Beatles\\&quot;&quot; NA ## [3,] &quot;\\&quot;Bad and Boujee\\&quot;&quot; NA ## [4,] &quot;\\&quot;Shape of You\\&quot; &quot; NA ## [5,] &quot;\\&quot;Bad and Boujee\\&quot;&quot; &quot;2&quot; ## [6,] &quot;[7]&quot; NA ## [7,] &quot;\\&quot;Shape of You\\&quot; &quot; &quot;11&quot; ## [8,] &quot;[9]&quot; NA ## [9,] &quot;[10]&quot; NA ## [10,] &quot;[11]&quot; NA To get artist names we get the second data element (td) of each row using the td:nth-of-type(2) CSS selector (again removing the first entry in result coming from the header row) artist_nodes &lt;- singles_tab_node %&gt;% html_nodes(&quot;tr&quot;) %&gt;% html_node(&quot;td:nth-of-type(2)&quot;) %&gt;% magrittr::extract(-1) artists &lt;- artist_nodes %&gt;% html_text() artists %&gt;% head(10) ## [1] &quot;The Weeknd featuring Daft Punk&quot; ## [2] &quot;Rae Sremmurd featuring Gucci Mane&quot; ## [3] &quot;Migos featuring Lil Uzi Vert&quot; ## [4] &quot;Ed Sheeran&quot; ## [5] &quot;Migos featuring Lil Uzi Vert&quot; ## [6] NA ## [7] &quot;Ed Sheeran&quot; ## [8] NA ## [9] NA ## [10] NA Now that we’ve extracted each attribute separately we can combine them into a single data frame billboard_df &lt;- data_frame(month_day=dates, year=&quot;2017&quot;, song_title_raw=song_titles, title_span=title_spans, artist_raw=artists) billboard_df ## # A tibble: 52 x 5 ## month_day year song_title_raw title_span artist_raw ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 January 7 2017 &quot;\\&quot;Starboy\\&quot;&quot; &lt;NA&gt; The Weeknd featuring… ## 2 January 14 2017 &quot;\\&quot;Black Beatles\\&quot;&quot; &lt;NA&gt; Rae Sremmurd featuri… ## 3 January 21 2017 &quot;\\&quot;Bad and Boujee\\&quot;&quot; &lt;NA&gt; Migos featuring Lil … ## 4 January 28 2017 &quot;\\&quot;Shape of You\\&quot; &quot; &lt;NA&gt; Ed Sheeran ## 5 February 4 2017 &quot;\\&quot;Bad and Boujee\\&quot;&quot; 2 Migos featuring Lil … ## 6 February 11 2017 [7] &lt;NA&gt; &lt;NA&gt; ## 7 February 18 2017 &quot;\\&quot;Shape of You\\&quot; &quot; 11 Ed Sheeran ## 8 February 25 2017 [9] &lt;NA&gt; &lt;NA&gt; ## 9 March 4 2017 [10] &lt;NA&gt; &lt;NA&gt; ## 10 March 11 2017 [11] &lt;NA&gt; &lt;NA&gt; ## # ... with 42 more rows This is by no means a clean data frame yet, but we will discuss how to clean up data like this in later lectures. We can now abstract these operations into a function that scrapes the same data for other years. scrape_billboard &lt;- function(year, baseurl=&quot;https://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_number-one_singles_of_&quot;) { url &lt;- paste0(baseurl, year) # find table node singles_tab_node &lt;- read_html(url) %&gt;% html_node(&quot;.plainrowheaders&quot;) # extract dates dates &lt;- singles_tab_node %&gt;% html_nodes(&quot;[scope]&quot;) %&gt;% html_text() # extract titles and spans title_nodes &lt;- singles_tab_node %&gt;% html_nodes(&quot;tr&quot;) %&gt;% html_node(&quot;td:first-of-type&quot;) %&gt;% magrittr::extract(-1) song_titles &lt;- title_nodes %&gt;% html_text() title_spans &lt;- title_nodes %&gt;% html_attr(&quot;rowspan&quot;) # extract artists artist_nodes &lt;- singles_tab_node %&gt;% html_nodes(&quot;tr&quot;) %&gt;% html_node(&quot;td:nth-of-type(2)&quot;) %&gt;% magrittr::extract(-1) artists &lt;- artist_nodes %&gt;% html_text() # make data frame data_frame(month_day=dates, year=year, song_title_raw=song_titles, title_span=title_spans, artist_raw=artists) } scrape_billboard(&quot;2016&quot;) ## # A tibble: 53 x 5 ## month_day year song_title_raw title_span artist_raw ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 January 2 2016 &quot;\\&quot;Hello\\&quot;&quot; 3 Adele ## 2 January 9 2016 [4] &lt;NA&gt; &lt;NA&gt; ## 3 January 16 2016 [5] &lt;NA&gt; &lt;NA&gt; ## 4 January 23 2016 &quot;\\&quot;Sorry\\&quot;&quot; 3 Justin Bieber ## 5 January 30 2016 [7] &lt;NA&gt; &lt;NA&gt; ## 6 February 6 2016 [8] &lt;NA&gt; &lt;NA&gt; ## 7 February 13 2016 &quot;\\&quot;Love Yourself\\&quot; &quot; &lt;NA&gt; [9] ## 8 February 20 2016 &quot;\\&quot;Pillowtalk\\&quot;&quot; &lt;NA&gt; Zayn ## 9 February 27 2016 &quot;\\&quot;Love Yourself\\&quot; &quot; &lt;NA&gt; Justin Bieber ## 10 March 5 2016 &quot;\\&quot;Work\\&quot;&quot; 9 Rihanna featuring Dr… ## # ... with 43 more rows We can do this for a few years and create a (very dirty) dataset with songs for this current decade: billboard_tab &lt;- as.character(2010:2017) %&gt;% purrr::map_df(scrape_billboard) billboard_tab %&gt;% head(20) %&gt;% knitr::kable(&quot;html&quot;) month_day year song_title_raw title_span artist_raw January 7 2012 “Sexy and I Know It” 2 LMFAO January 14 2012 [13] NA NA January 21 2012 “We Found Love” 2 Rihanna featuring Calvin Harris January 28 2012 [15] NA NA February 4 2012 “Set Fire to the Rain” 2 Adele February 11 2012 [17] NA NA February 18 2012 “Stronger (What Doesn’t Kill You)” 2 Kelly Clarkson February 25 2012 [19] NA NA March 3 2012 “Part of Me” NA Katy Perry March 10 2012 “Stronger (What Doesn’t Kill You)” NA Kelly Clarkson March 17 2012 “We Are Young” 6 Fun featuring Janelle Monáe March 24 2012 [22] NA NA March 31 2012 [23] NA NA April 7 2012 [24] NA NA April 14 2012 [25] NA NA April 21 2012 [7] NA NA April 28 2012 “Somebody That I Used to Know” 8 Gotye featuring Kimbra May 5 2012 [27] NA NA May 12 2012 [28] NA NA May 19 2012 [29] NA NA The function purrr::map_df is an example of a very powerful idiom in functional programming: mapping functions on elements of vectors. Here, we first create a vector of years (as strings) using as.character(2010:2017) we pass that to purrr::map_df which applies the function we create, scrape_billboard on each entry of the year vector. Each of these calls evaluates to a data_frame which are then bound (using bind_rows) to create a single long data frame. The tidyverse package purrr defines a lot of these functional programming idioms. One more thing: here’s a very nice example of rvest at work: https://deanattali.com/blog/user2017/ "],
["tidying-data.html", "17 Tidying data 17.1 Tidy Data 17.2 Common problems in messy data", " 17 Tidying data This section is concerned with common problems in data preparation, namely use cases commonly found in raw datasets that need to be addressed to turn messy data into tidy data. These would be operations that you would perform on data obtained as a csv file from a collaborator or data repository, or as the result of scraping data from webpages or other sources. We derive many of our ideas from the paper Tidy Data by Hadley Wickham. Associated with that paper we will use two very powerful R libraries tidyr and dplyr which are extremely useful in writing scripts for data cleaning, preparation and summarization. A basic design principle behind these libraries is trying to effectively and efficiently capture very common use cases and operations performed in data cleaning. The paper frames these use cases and operations which are them implemented in software. 17.1 Tidy Data Here we assume we are working with a data model based on rectangular data structures where Each attribute (or variable) forms a column Each entity (or observation) forms a row Each type of entity (observational unit) forms a table Here is an example of a tidy dataset: library(nycflights13) head(flights) ## # A tibble: 6 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2.00 830 ## 2 2013 1 1 533 529 4.00 850 ## 3 2013 1 1 542 540 2.00 923 ## 4 2013 1 1 544 545 -1.00 1004 ## 5 2013 1 1 554 600 -6.00 812 ## 6 2013 1 1 554 558 -4.00 740 ## # ... with 12 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, ## # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, ## # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, ## # time_hour &lt;dttm&gt; it has one observation per row, a single variable per column. Notice only information about flights are included here (e.g., no airport information other than the name) in these observations. 17.2 Common problems in messy data The set of common operations we will study are based on these common problems found in datasets. We will see each one in detail: Column headers are values, not variable names (gather) Multiple variables stored in one column (split) Variables stored in both rows and column (rotate) Multiple types of observational units are stored in the same table (normalize) Single observational unit stored in multiple tables (join) We are using data from Hadley’s paper found in github. It’s included directory data: data_dir &lt;- &quot;data&quot; 17.2.1 Headers as values The first problem we’ll see is the case where a table header contains values. library(tidyverse) pew &lt;- read_csv(file.path(data_dir, &quot;pew.csv&quot;)) ## Parsed with column specification: ## cols( ## religion = col_character(), ## `&lt;$10k` = col_integer(), ## `$10-20k` = col_integer(), ## `$20-30k` = col_integer(), ## `$30-40k` = col_integer(), ## `$40-50k` = col_integer(), ## `$50-75k` = col_integer(), ## `$75-100k` = col_integer(), ## `$100-150k` = col_integer(), ## `&gt;150k` = col_integer(), ## `Don&#39;t know/refused` = col_integer() ## ) pew ## # A tibble: 18 x 11 ## religion `&lt;$10k` `$10-20k` `$20-30k` `$30-40k` `$40-50k` `$50-75k` ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Agnostic 27 34 60 81 76 137 ## 2 Atheist 12 27 37 52 35 70 ## 3 Buddhist 27 21 30 34 33 58 ## 4 Catholic 418 617 732 670 638 1116 ## 5 Don’t know/r… 15 14 15 11 10 35 ## 6 Evangelical … 575 869 1064 982 881 1486 ## 7 Hindu 1 9 7 9 11 34 ## 8 Historically… 228 244 236 238 197 223 ## 9 Jehovah&#39;s Wi… 20 27 24 24 21 30 ## 10 Jewish 19 19 25 25 30 95 ## 11 Mainline Prot 289 495 619 655 651 1107 ## 12 Mormon 29 40 48 51 56 112 ## 13 Muslim 6 7 9 10 9 23 ## 14 Orthodox 13 17 23 32 32 47 ## 15 Other Christ… 9 7 11 13 13 14 ## 16 Other Faiths 20 33 40 46 49 63 ## 17 Other World … 5 2 3 4 2 7 ## 18 Unaffiliated 217 299 374 365 341 528 ## # ... with 4 more variables: `$75-100k` &lt;int&gt;, `$100-150k` &lt;int&gt;, ## # `&gt;150k` &lt;int&gt;, `Don&#39;t know/refused` &lt;int&gt; This table has the number of survey respondents of a specific religion that report their income within some range. A tidy version of this table would consider the variables of each observation to be religion, income, frequency where frequency has the number of respondents for each religion and income range. The function to use in the tidyr package is gather: tidy_pew &lt;- gather(pew, income, frequency, -religion) tidy_pew ## # A tibble: 180 x 3 ## religion income frequency ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Agnostic &lt;$10k 27 ## 2 Atheist &lt;$10k 12 ## 3 Buddhist &lt;$10k 27 ## 4 Catholic &lt;$10k 418 ## 5 Don’t know/refused &lt;$10k 15 ## 6 Evangelical Prot &lt;$10k 575 ## 7 Hindu &lt;$10k 1 ## 8 Historically Black Prot &lt;$10k 228 ## 9 Jehovah&#39;s Witness &lt;$10k 20 ## 10 Jewish &lt;$10k 19 ## # ... with 170 more rows This says: gather all the columns from the pew (except religion) into key-value columns income and frequency. This table is much easier to use in other analyses. Another example: this table has a row for each song appearing in the billboard top 100. It contains track information, and the date it entered the top 100. It then shows the rank in each of the next 76 weeks. billboard &lt;- read_csv(file.path(data_dir, &quot;billboard.csv&quot;)) ## Parsed with column specification: ## cols( ## .default = col_integer(), ## artist = col_character(), ## track = col_character(), ## time = col_time(format = &quot;&quot;), ## date.entered = col_date(format = &quot;&quot;), ## wk66 = col_character(), ## wk67 = col_character(), ## wk68 = col_character(), ## wk69 = col_character(), ## wk70 = col_character(), ## wk71 = col_character(), ## wk72 = col_character(), ## wk73 = col_character(), ## wk74 = col_character(), ## wk75 = col_character(), ## wk76 = col_character() ## ) ## See spec(...) for full column specifications. billboard ## # A tibble: 317 x 81 ## year artist track time date.entered wk1 wk2 wk3 wk4 wk5 ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;tim&gt; &lt;date&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 2000 2 Pac Baby D… 04:22 2000-02-26 87 82 72 77 87 ## 2 2000 2Ge+her The Ha… 03:15 2000-09-02 91 87 92 NA NA ## 3 2000 3 Doors… Krypto… 03:53 2000-04-08 81 70 68 67 66 ## 4 2000 3 Doors… Loser 04:24 2000-10-21 76 76 72 69 67 ## 5 2000 504 Boyz Wobble… 03:35 2000-04-15 57 34 25 17 17 ## 6 2000 98^0 Give M… 03:24 2000-08-19 51 39 34 26 26 ## 7 2000 A*Teens Dancin… 03:44 2000-07-08 97 97 96 95 100 ## 8 2000 Aaliyah I Don&#39;… 04:15 2000-01-29 84 62 51 41 38 ## 9 2000 Aaliyah Try Ag… 04:03 2000-03-18 59 53 38 28 21 ## 10 2000 Adams, … Open M… 05:30 2000-08-26 76 76 74 69 68 ## # ... with 307 more rows, and 71 more variables: wk6 &lt;int&gt;, wk7 &lt;int&gt;, ## # wk8 &lt;int&gt;, wk9 &lt;int&gt;, wk10 &lt;int&gt;, wk11 &lt;int&gt;, wk12 &lt;int&gt;, wk13 &lt;int&gt;, ## # wk14 &lt;int&gt;, wk15 &lt;int&gt;, wk16 &lt;int&gt;, wk17 &lt;int&gt;, wk18 &lt;int&gt;, ## # wk19 &lt;int&gt;, wk20 &lt;int&gt;, wk21 &lt;int&gt;, wk22 &lt;int&gt;, wk23 &lt;int&gt;, ## # wk24 &lt;int&gt;, wk25 &lt;int&gt;, wk26 &lt;int&gt;, wk27 &lt;int&gt;, wk28 &lt;int&gt;, ## # wk29 &lt;int&gt;, wk30 &lt;int&gt;, wk31 &lt;int&gt;, wk32 &lt;int&gt;, wk33 &lt;int&gt;, ## # wk34 &lt;int&gt;, wk35 &lt;int&gt;, wk36 &lt;int&gt;, wk37 &lt;int&gt;, wk38 &lt;int&gt;, ## # wk39 &lt;int&gt;, wk40 &lt;int&gt;, wk41 &lt;int&gt;, wk42 &lt;int&gt;, wk43 &lt;int&gt;, ## # wk44 &lt;int&gt;, wk45 &lt;int&gt;, wk46 &lt;int&gt;, wk47 &lt;int&gt;, wk48 &lt;int&gt;, ## # wk49 &lt;int&gt;, wk50 &lt;int&gt;, wk51 &lt;int&gt;, wk52 &lt;int&gt;, wk53 &lt;int&gt;, ## # wk54 &lt;int&gt;, wk55 &lt;int&gt;, wk56 &lt;int&gt;, wk57 &lt;int&gt;, wk58 &lt;int&gt;, ## # wk59 &lt;int&gt;, wk60 &lt;int&gt;, wk61 &lt;int&gt;, wk62 &lt;int&gt;, wk63 &lt;int&gt;, ## # wk64 &lt;int&gt;, wk65 &lt;int&gt;, wk66 &lt;chr&gt;, wk67 &lt;chr&gt;, wk68 &lt;chr&gt;, ## # wk69 &lt;chr&gt;, wk70 &lt;chr&gt;, wk71 &lt;chr&gt;, wk72 &lt;chr&gt;, wk73 &lt;chr&gt;, ## # wk74 &lt;chr&gt;, wk75 &lt;chr&gt;, wk76 &lt;chr&gt; Challenge: This dataset has values as column names. Which column names are values? How do we tidy this dataset? 17.2.2 Multiple variables in one column The next problem we’ll see is the case when we see multiple variables in a single column. Consider the following dataset of tuberculosis cases: tb &lt;- read_csv(file.path(data_dir, &quot;tb.csv&quot;)) ## Parsed with column specification: ## cols( ## .default = col_integer(), ## iso2 = col_character() ## ) ## See spec(...) for full column specifications. tb ## # A tibble: 5,769 x 22 ## iso2 year m04 m514 m014 m1524 m2534 m3544 m4554 m5564 m65 mu ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 AD 1989 NA NA NA NA NA NA NA NA NA NA ## 2 AD 1990 NA NA NA NA NA NA NA NA NA NA ## 3 AD 1991 NA NA NA NA NA NA NA NA NA NA ## 4 AD 1992 NA NA NA NA NA NA NA NA NA NA ## 5 AD 1993 NA NA NA NA NA NA NA NA NA NA ## 6 AD 1994 NA NA NA NA NA NA NA NA NA NA ## 7 AD 1996 NA NA 0 0 0 4 1 0 0 NA ## 8 AD 1997 NA NA 0 0 1 2 2 1 6 NA ## 9 AD 1998 NA NA 0 0 0 1 0 0 0 NA ## 10 AD 1999 NA NA 0 0 0 1 1 0 0 NA ## # ... with 5,759 more rows, and 10 more variables: f04 &lt;int&gt;, f514 &lt;int&gt;, ## # f014 &lt;int&gt;, f1524 &lt;int&gt;, f2534 &lt;int&gt;, f3544 &lt;int&gt;, f4554 &lt;int&gt;, ## # f5564 &lt;int&gt;, f65 &lt;int&gt;, fu &lt;int&gt; This table has a row for each year and strain of tuberculosis (given by the first two columns). The remaining columns state the number of cases for a given demographic. For example, m1524 corresponds to males between 15 and 24 years old, and f1524 are females age 15-24. As you can see each of these columns has two variables: sex and age. Challenge: what else is untidy about this dataset? So, we have to do two operations to tidy this table, first we need to use gather the tabulation columns into a demo and n columns (for demographic and number of cases): tidy_tb &lt;- gather(tb, demo, n, -iso2, -year) tidy_tb ## # A tibble: 115,380 x 4 ## iso2 year demo n ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 AD 1989 m04 NA ## 2 AD 1990 m04 NA ## 3 AD 1991 m04 NA ## 4 AD 1992 m04 NA ## 5 AD 1993 m04 NA ## 6 AD 1994 m04 NA ## 7 AD 1996 m04 NA ## 8 AD 1997 m04 NA ## 9 AD 1998 m04 NA ## 10 AD 1999 m04 NA ## # ... with 115,370 more rows Next, we need to separate the values in the demo column into two variables sex and age tidy_tb &lt;- separate(tidy_tb, demo, c(&quot;sex&quot;, &quot;age&quot;), sep=1) tidy_tb ## # A tibble: 115,380 x 5 ## iso2 year sex age n ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 AD 1989 m 04 NA ## 2 AD 1990 m 04 NA ## 3 AD 1991 m 04 NA ## 4 AD 1992 m 04 NA ## 5 AD 1993 m 04 NA ## 6 AD 1994 m 04 NA ## 7 AD 1996 m 04 NA ## 8 AD 1997 m 04 NA ## 9 AD 1998 m 04 NA ## 10 AD 1999 m 04 NA ## # ... with 115,370 more rows This calls the separate function on table tidy_db, separating the demo variable into variables sex and age by separating each value after the first character (that’s the sep argument). We can put these two commands together in a pipeline: tidy_tb &lt;- tb %&gt;% gather(demo, n, -iso2, -year) %&gt;% separate(demo, c(&quot;sex&quot;, &quot;age&quot;), sep=1) tidy_tb ## # A tibble: 115,380 x 5 ## iso2 year sex age n ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 AD 1989 m 04 NA ## 2 AD 1990 m 04 NA ## 3 AD 1991 m 04 NA ## 4 AD 1992 m 04 NA ## 5 AD 1993 m 04 NA ## 6 AD 1994 m 04 NA ## 7 AD 1996 m 04 NA ## 8 AD 1997 m 04 NA ## 9 AD 1998 m 04 NA ## 10 AD 1999 m 04 NA ## # ... with 115,370 more rows 17.2.3 Variables stored in both rows and columns This is the messiest, commonly found type of data. Let’s take a look at an example, this is daily weather data from for one weather station in Mexico in 2010. weather &lt;- read_csv(file.path(data_dir, &quot;weather.csv&quot;)) ## Parsed with column specification: ## cols( ## .default = col_double(), ## id = col_character(), ## year = col_integer(), ## month = col_integer(), ## element = col_character(), ## d9 = col_character(), ## d12 = col_character(), ## d18 = col_character(), ## d19 = col_character(), ## d20 = col_character(), ## d21 = col_character(), ## d22 = col_character(), ## d24 = col_character() ## ) ## See spec(...) for full column specifications. weather ## # A tibble: 22 x 35 ## id year month element d1 d2 d3 d4 d5 d6 d7 ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 MX17004 2010 1 tmax NA NA NA NA NA NA NA ## 2 MX17004 2010 1 tmin NA NA NA NA NA NA NA ## 3 MX17004 2010 2 tmax NA 27.3 24.1 NA NA NA NA ## 4 MX17004 2010 2 tmin NA 14.4 14.4 NA NA NA NA ## 5 MX17004 2010 3 tmax NA NA NA NA 32.1 NA NA ## 6 MX17004 2010 3 tmin NA NA NA NA 14.2 NA NA ## 7 MX17004 2010 4 tmax NA NA NA NA NA NA NA ## 8 MX17004 2010 4 tmin NA NA NA NA NA NA NA ## 9 MX17004 2010 5 tmax NA NA NA NA NA NA NA ## 10 MX17004 2010 5 tmin NA NA NA NA NA NA NA ## # ... with 12 more rows, and 24 more variables: d8 &lt;dbl&gt;, d9 &lt;chr&gt;, ## # d10 &lt;dbl&gt;, d11 &lt;dbl&gt;, d12 &lt;chr&gt;, d13 &lt;dbl&gt;, d14 &lt;dbl&gt;, d15 &lt;dbl&gt;, ## # d16 &lt;dbl&gt;, d17 &lt;dbl&gt;, d18 &lt;chr&gt;, d19 &lt;chr&gt;, d20 &lt;chr&gt;, d21 &lt;chr&gt;, ## # d22 &lt;chr&gt;, d23 &lt;dbl&gt;, d24 &lt;chr&gt;, d25 &lt;dbl&gt;, d26 &lt;dbl&gt;, d27 &lt;dbl&gt;, ## # d28 &lt;dbl&gt;, d29 &lt;dbl&gt;, d30 &lt;dbl&gt;, d31 &lt;dbl&gt; So, we have two rows for each month, one with maximum daily temperature, one with minimum daily temperature, the columns starting with d correspond to the day in the where the measurements were made. Challenge: How would a tidy version of this data look like? weather %&gt;% gather(day, value, d1:d31, na.rm=TRUE) %&gt;% spread(element, value) ## # A tibble: 33 x 6 ## id year month day tmax tmin ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 MX17004 2010 1 d30 27.8 14.5 ## 2 MX17004 2010 2 d11 29.7 13.4 ## 3 MX17004 2010 2 d2 27.3 14.4 ## 4 MX17004 2010 2 d23 29.9 10.7 ## 5 MX17004 2010 2 d3 24.1 14.4 ## 6 MX17004 2010 3 d10 34.5 16.8 ## 7 MX17004 2010 3 d16 31.1 17.6 ## 8 MX17004 2010 3 d5 32.1 14.2 ## 9 MX17004 2010 4 d27 36.3 16.7 ## 10 MX17004 2010 5 d27 33.2 18.2 ## # ... with 23 more rows The new function we’ve used here is spread. It does the inverse of gather it spreads columns element and value into separate columns. 17.2.4 Multiple types in one table Remember that an important aspect of tidy data is that it contains exactly one kind of observation in a single table. Let’s see the billboard example again after the gather operation we did before: tidy_billboard &lt;- billboard %&gt;% gather(week, rank, wk1:wk76, na.rm=TRUE) tidy_billboard ## # A tibble: 5,307 x 7 ## year artist track time date.entered week rank ## * &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;tim&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2000 2 Pac Baby Don&#39;t Cry (Ke… 04:22 2000-02-26 wk1 87 ## 2 2000 2Ge+her The Hardest Part O… 03:15 2000-09-02 wk1 91 ## 3 2000 3 Doors Down Kryptonite 03:53 2000-04-08 wk1 81 ## 4 2000 3 Doors Down Loser 04:24 2000-10-21 wk1 76 ## 5 2000 504 Boyz Wobble Wobble 03:35 2000-04-15 wk1 57 ## 6 2000 98^0 Give Me Just One N… 03:24 2000-08-19 wk1 51 ## 7 2000 A*Teens Dancing Queen 03:44 2000-07-08 wk1 97 ## 8 2000 Aaliyah I Don&#39;t Wanna 04:15 2000-01-29 wk1 84 ## 9 2000 Aaliyah Try Again 04:03 2000-03-18 wk1 59 ## 10 2000 Adams, Yolanda Open My Heart 05:30 2000-08-26 wk1 76 ## # ... with 5,297 more rows Let’s sort this table by track to see a problem with this table: tidy_billboard &lt;- tidy_billboard %&gt;% arrange(track) tidy_billboard ## # A tibble: 5,307 x 7 ## year artist track time date.entered week rank ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;time&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2000 Nelly (Hot S**t) Country G... 04:17 2000-04-29 wk1 100 ## 2 2000 Nelly (Hot S**t) Country G... 04:17 2000-04-29 wk2 99 ## 3 2000 Nelly (Hot S**t) Country G... 04:17 2000-04-29 wk3 96 ## 4 2000 Nelly (Hot S**t) Country G... 04:17 2000-04-29 wk4 76 ## 5 2000 Nelly (Hot S**t) Country G... 04:17 2000-04-29 wk5 55 ## 6 2000 Nelly (Hot S**t) Country G... 04:17 2000-04-29 wk6 37 ## 7 2000 Nelly (Hot S**t) Country G... 04:17 2000-04-29 wk7 24 ## 8 2000 Nelly (Hot S**t) Country G... 04:17 2000-04-29 wk8 24 ## 9 2000 Nelly (Hot S**t) Country G... 04:17 2000-04-29 wk9 30 ## 10 2000 Nelly (Hot S**t) Country G... 04:17 2000-04-29 wk10 36 ## # ... with 5,297 more rows We have a lot of repeated information in many of these rows (the artist, track name, year, title and date entered). The problem is that this table contains information about both tracks and rank in billboard. That’s two different kinds of observations that should belong in two different tables in a tidy dataset. Let’s make a song table that only includes information about songs: song &lt;- tidy_billboard %&gt;% select(artist, track, year, time, date.entered) %&gt;% unique() song ## # A tibble: 317 x 5 ## artist track year time date.entered ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;time&gt; &lt;date&gt; ## 1 Nelly (Hot S**t) Country G... 2000 04:17 2000-04-29 ## 2 Nu Flavor 3 Little Words 2000 03:54 2000-06-03 ## 3 Jean, Wyclef 911 2000 04:00 2000-10-07 ## 4 Brock, Chad A Country Boy Can Su... 2000 03:54 2000-01-01 ## 5 Clark, Terri A Little Gasoline 2000 03:07 2000-12-16 ## 6 Son By Four A Puro Dolor (Purest... 2000 03:30 2000-04-08 ## 7 Carter, Aaron Aaron&#39;s Party (Come ... 2000 03:23 2000-08-26 ## 8 Nine Days Absolutely (Story Of... 2000 03:09 2000-05-06 ## 9 De La Soul All Good? 2000 05:02 2000-12-23 ## 10 Blink-182 All The Small Things 2000 02:52 1999-12-04 ## # ... with 307 more rows The unique function removes any duplicate rows in a table. That’s how we have a single row for each song. Next, we would like to remove all the song information from the rank table. But we need to do it in a way that still remembers which song each ranking observation corresponds to. To do that, let’s first give each song an identifier that we can use to link songs and rankings. So, we can produce the final version of our song table like this: song &lt;- tidy_billboard %&gt;% select(artist, track, year, time, date.entered) %&gt;% unique() %&gt;% mutate(song_id = row_number()) song ## # A tibble: 317 x 6 ## artist track year time date.entered song_id ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;time&gt; &lt;date&gt; &lt;int&gt; ## 1 Nelly (Hot S**t) Country G... 2000 04:17 2000-04-29 1 ## 2 Nu Flavor 3 Little Words 2000 03:54 2000-06-03 2 ## 3 Jean, Wyclef 911 2000 04:00 2000-10-07 3 ## 4 Brock, Chad A Country Boy Can Su... 2000 03:54 2000-01-01 4 ## 5 Clark, Terri A Little Gasoline 2000 03:07 2000-12-16 5 ## 6 Son By Four A Puro Dolor (Purest... 2000 03:30 2000-04-08 6 ## 7 Carter, Aaron Aaron&#39;s Party (Come ... 2000 03:23 2000-08-26 7 ## 8 Nine Days Absolutely (Story Of... 2000 03:09 2000-05-06 8 ## 9 De La Soul All Good? 2000 05:02 2000-12-23 9 ## 10 Blink-182 All The Small Things 2000 02:52 1999-12-04 10 ## # ... with 307 more rows The mutate function adds a new column to the table, in this case with column name song_id and value the row number the song appears in the table (from the row_number column). Now we can make a rank table, we combine the tidy billboard table with our new song table using a join (we’ll learn all about joins later). It checks the values on each row of the billboard table and looks for rows in the song table that have the exact same values, and makes a new row that combines the information from both tables. tidy_billboard %&gt;% left_join(song, c(&quot;artist&quot;, &quot;year&quot;, &quot;track&quot;, &quot;time&quot;, &quot;date.entered&quot;)) ## # A tibble: 5,307 x 8 ## year artist track time date.entered week rank song_id ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;tim&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 2000 Nelly (Hot S**t) Country… 04:17 2000-04-29 wk1 100 1 ## 2 2000 Nelly (Hot S**t) Country… 04:17 2000-04-29 wk2 99 1 ## 3 2000 Nelly (Hot S**t) Country… 04:17 2000-04-29 wk3 96 1 ## 4 2000 Nelly (Hot S**t) Country… 04:17 2000-04-29 wk4 76 1 ## 5 2000 Nelly (Hot S**t) Country… 04:17 2000-04-29 wk5 55 1 ## 6 2000 Nelly (Hot S**t) Country… 04:17 2000-04-29 wk6 37 1 ## 7 2000 Nelly (Hot S**t) Country… 04:17 2000-04-29 wk7 24 1 ## 8 2000 Nelly (Hot S**t) Country… 04:17 2000-04-29 wk8 24 1 ## 9 2000 Nelly (Hot S**t) Country… 04:17 2000-04-29 wk9 30 1 ## 10 2000 Nelly (Hot S**t) Country… 04:17 2000-04-29 wk10 36 1 ## # ... with 5,297 more rows That adds the song_id variable to the tidy_billboard table. So now we can remove the song information and only keep ranking information and the song_id. rank &lt;- tidy_billboard %&gt;% left_join(song, c(&quot;artist&quot;, &quot;year&quot;, &quot;track&quot;, &quot;time&quot;, &quot;date.entered&quot;)) %&gt;% select(song_id, week, rank) rank ## # A tibble: 5,307 x 3 ## song_id week rank ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 wk1 100 ## 2 1 wk2 99 ## 3 1 wk3 96 ## 4 1 wk4 76 ## 5 1 wk5 55 ## 6 1 wk6 37 ## 7 1 wk7 24 ## 8 1 wk8 24 ## 9 1 wk9 30 ## 10 1 wk10 36 ## # ... with 5,297 more rows Challenge: Let’s do a little better job at tidying the billboard dataset: When using gather to make the week and rank columns, remove any weeks where the song does not appear in the top 100. This is coded as missing (NA). See the na.rm argument to gather. Make week a numeric variable (i.e., remove wk). See what the extract_numeric function does. Instead of date.entered add a date column that states the actual date of each ranking. See how R deals with dates ?Date and how you can turn a string into a Date using as.Date. Sort the resulting table by date and rank. Make new song and rank tables. song will now not have the date.entered column, and rank will have the new date column you have just created. "],
["text-and-dates.html", "18 Text and Dates 18.1 Text 18.2 Handling dates", " 18 Text and Dates In this chapter we briefly discuss common patterns to handle text and date data and point to useful resources. 18.1 Text Frequently, data scraped or ingested will contain text that will need to be processed to either extract data, correct errors or resolve duplicate records. In this section we will look at a few common patterns: 1) tools for string operations, 2) tools using regular expressions, and 3) deriving attributes from text. For further reading consult: http://r4ds.had.co.nz/strings.html 18.1.1 String operations The stringr package contains a number of useful, commonly used string manipulation operations. library(tidyverse) library(stringr) short_string &lt;- &quot;I love Spring&quot; long_string &lt;- &quot;There&#39;s is nothing I love more than 320 in the Spring&quot; Here are a few common ones: string length: str_len str_length(c(short_string, long_string)) ## [1] 13 53 combining strings: str_c str_c(short_string, long_string, sep=&quot;. &quot;) ## [1] &quot;I love Spring. There&#39;s is nothing I love more than 320 in the Spring&quot; subsetting strings: str_sub str_sub(c(short_string, long_string), 2, 5) ## [1] &quot; lov&quot; &quot;here&quot; trim strings: str_trim str_trim(&quot; I am padded &quot;, side=&quot;both&quot;) ## [1] &quot;I am padded&quot; 18.1.2 Regular expressions By far, the most powerful tools for extracting and cleaning text data are regular expressions. The stringr package provides a great number of tools based on regular expression matching. First, some basics strs &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;) str_view(strs, &quot;an&quot;) Match any character: . Match the ‘dot’ character: \\\\. str_view(strs, &quot;.a.&quot;) str_view(c(strs, &quot;a.c&quot;), &quot;a\\\\.c&quot;) Anchor start (^), end ($) str_view(strs, &quot;^a&quot;) str_view(strs, &quot;a$&quot;) str_view(c(&quot;apple pie&quot;, &quot;apple&quot;, &quot;apple cake&quot;), &quot;apple&quot;) str_view(c(&quot;apple pie&quot;, &quot;apple&quot;, &quot;apple cake&quot;), &quot;^apple$&quot;) Character classes and alternatives \\d: match any digit \\s: match any whitespace (e.g., space, tab, newline) [abc]: match set of characters (e.g, a, b, or c) [^abc]: match anything except this set of characters |: match any of one or more patterns Match vowels or digits str_view(c(&quot;t867nine&quot;, &quot;gray9&quot;), &quot;[aeiou]|[0-9]&quot;) Repetition ?: zero or one +: one or more *: zero or more str_view(c(&quot;color&quot;, &quot;colour&quot;), &quot;colou?r&quot;) Grouping and backreferences Parentheses define groups, which can be referenced using \\1, \\2, etc. fruit &lt;- c(&quot;banana&quot;, &quot;coconut&quot;, &quot;cucumber&quot;, &quot;jujube&quot;, &quot;papaya&quot;, &quot;salal berry&quot;) str_view(fruit, &quot;(..)\\\\1&quot;) 18.1.3 Tools using regular expressions Determine which strings match a pattern: str_detect: given a vector of strings, return TRUE for those that match a regular expression, FALSE otherwise data(words) print(head(words)) ## [1] &quot;a&quot; &quot;able&quot; &quot;about&quot; &quot;absolute&quot; &quot;accept&quot; &quot;account&quot; data_frame(word=words, result=str_detect(words, &quot;^[aeiou]&quot;)) %&gt;% sample_n(30) ## # A tibble: 30 x 2 ## word result ## &lt;chr&gt; &lt;lgl&gt; ## 1 exact T ## 2 get F ## 3 policy F ## 4 cent F ## 5 make F ## 6 clear F ## 7 heat F ## 8 card F ## 9 direct F ## 10 sign F ## # ... with 20 more rows Similarly, str_count returns the number of matches in a string instead of just TRUE or FALSE Filter string vectors to include only those that match a regular expression data(sentences) print(head(sentences)) ## [1] &quot;The birch canoe slid on the smooth planks.&quot; ## [2] &quot;Glue the sheet to the dark blue background.&quot; ## [3] &quot;It&#39;s easy to tell the depth of a well.&quot; ## [4] &quot;These days a chicken leg is a rare dish.&quot; ## [5] &quot;Rice is often served in round bowls.&quot; ## [6] &quot;The juice of lemons makes fine punch.&quot; colors &lt;- c(&quot;red&quot;, &quot;orange&quot;, &quot;yellow&quot;, &quot;green&quot;, &quot;blue&quot;, &quot;purple&quot;) colors_re &lt;- str_c(colors, collapse=&quot;|&quot;) print(colors_re) ## [1] &quot;red|orange|yellow|green|blue|purple&quot; sentences_with_color &lt;- str_subset(sentences, colors_re) %&gt;% head(10) str_view_all(sentences_with_color, colors_re) Extracting matches: str_extract, str_extract_all str_extract(sentences_with_color, colors_re) ## [1] &quot;blue&quot; &quot;blue&quot; &quot;red&quot; &quot;red&quot; &quot;red&quot; &quot;blue&quot; &quot;yellow&quot; ## [8] &quot;red&quot; &quot;red&quot; &quot;green&quot; Grouped matches: str_match noun_re &lt;- &quot;(a|the) ([^ ]+)&quot; noun_matches &lt;- sentences %&gt;% str_subset(noun_re) %&gt;% str_match(noun_re) %&gt;% head(10) noun_matches ## [,1] [,2] [,3] ## [1,] &quot;the smooth&quot; &quot;the&quot; &quot;smooth&quot; ## [2,] &quot;the sheet&quot; &quot;the&quot; &quot;sheet&quot; ## [3,] &quot;the depth&quot; &quot;the&quot; &quot;depth&quot; ## [4,] &quot;a chicken&quot; &quot;a&quot; &quot;chicken&quot; ## [5,] &quot;the parked&quot; &quot;the&quot; &quot;parked&quot; ## [6,] &quot;the sun&quot; &quot;the&quot; &quot;sun&quot; ## [7,] &quot;the huge&quot; &quot;the&quot; &quot;huge&quot; ## [8,] &quot;the ball&quot; &quot;the&quot; &quot;ball&quot; ## [9,] &quot;the woman&quot; &quot;the&quot; &quot;woman&quot; ## [10,] &quot;a helps&quot; &quot;a&quot; &quot;helps&quot; The result is a string matrix, with one row for each string in the input vector. The first column includes the complete match to the regular expression (just like str_extract), the remaining columns has the matches for the groups defined in the pattern. To extract the first group matches one would index one of the columns. For example, the matches for the second group are noun_matches[,3] ## [1] &quot;smooth&quot; &quot;sheet&quot; &quot;depth&quot; &quot;chicken&quot; &quot;parked&quot; &quot;sun&quot; &quot;huge&quot; ## [8] &quot;ball&quot; &quot;woman&quot; &quot;helps&quot; Splitting strings: str_split split strings in a vector based on a match. For instance, to split sentences into words: sentences %&gt;% head(5) %&gt;% str_split(&quot; &quot;) ## [[1]] ## [1] &quot;The&quot; &quot;birch&quot; &quot;canoe&quot; &quot;slid&quot; &quot;on&quot; &quot;the&quot; &quot;smooth&quot; ## [8] &quot;planks.&quot; ## ## [[2]] ## [1] &quot;Glue&quot; &quot;the&quot; &quot;sheet&quot; &quot;to&quot; &quot;the&quot; ## [6] &quot;dark&quot; &quot;blue&quot; &quot;background.&quot; ## ## [[3]] ## [1] &quot;It&#39;s&quot; &quot;easy&quot; &quot;to&quot; &quot;tell&quot; &quot;the&quot; &quot;depth&quot; &quot;of&quot; &quot;a&quot; &quot;well.&quot; ## ## [[4]] ## [1] &quot;These&quot; &quot;days&quot; &quot;a&quot; &quot;chicken&quot; &quot;leg&quot; &quot;is&quot; &quot;a&quot; ## [8] &quot;rare&quot; &quot;dish.&quot; ## ## [[5]] ## [1] &quot;Rice&quot; &quot;is&quot; &quot;often&quot; &quot;served&quot; &quot;in&quot; &quot;round&quot; &quot;bowls.&quot; 18.1.4 Extracting attributes from text Handling free text in data pipelines and or statistical models is tricky. Frequently we extract attributes from text in order to perform analysis. We draw from https://www.tidytextmining.com/tidytext.html for this discussion. We usually think of text datasets (called a text corpus) in terms of documents: the instances of free text in our dataset, and terms the specific, e.g., words, they contain. In terms of the representation models we have used so far, we can think of documents as entities, described by attributes based on words, or words as entitites, described by attributes based on documents. To tidy text data, we tend to create one-token-per-row data frames that list the instances of terms in documents in a dataset Here’s a simple example using Jane Austen text library(janeaustenr) library(tidyverse) original_books &lt;- austen_books() %&gt;% group_by(book) %&gt;% mutate(linenumber = row_number(), chapter = cumsum(str_detect(text, regex(&quot;^chapter [\\\\divxlc]&quot;, ignore_case=TRUE)))) %&gt;% ungroup() original_books ## # A tibble: 73,422 x 4 ## text book linenumber chapter ## &lt;chr&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; ## 1 SENSE AND SENSIBILITY Sense &amp; Sensibility 1 0 ## 2 &quot;&quot; Sense &amp; Sensibility 2 0 ## 3 by Jane Austen Sense &amp; Sensibility 3 0 ## 4 &quot;&quot; Sense &amp; Sensibility 4 0 ## 5 (1811) Sense &amp; Sensibility 5 0 ## 6 &quot;&quot; Sense &amp; Sensibility 6 0 ## 7 &quot;&quot; Sense &amp; Sensibility 7 0 ## 8 &quot;&quot; Sense &amp; Sensibility 8 0 ## 9 &quot;&quot; Sense &amp; Sensibility 9 0 ## 10 CHAPTER 1 Sense &amp; Sensibility 10 1 ## # ... with 73,412 more rows Let’s re-structure it as a one-token-per-row column using the unnest_tokens function in the tidytext package library(tidytext) tidy_books &lt;- original_books %&gt;% unnest_tokens(word, text) tidy_books ## # A tibble: 725,055 x 4 ## book linenumber chapter word ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 Sense &amp; Sensibility 1 0 sense ## 2 Sense &amp; Sensibility 1 0 and ## 3 Sense &amp; Sensibility 1 0 sensibility ## 4 Sense &amp; Sensibility 3 0 by ## 5 Sense &amp; Sensibility 3 0 jane ## 6 Sense &amp; Sensibility 3 0 austen ## 7 Sense &amp; Sensibility 5 0 1811 ## 8 Sense &amp; Sensibility 10 1 chapter ## 9 Sense &amp; Sensibility 10 1 1 ## 10 Sense &amp; Sensibility 13 1 the ## # ... with 725,045 more rows Let’s remove stop words from the data frame data(stop_words) tidy_books &lt;- tidy_books %&gt;% anti_join(stop_words, by=&quot;word&quot;) tidy_books ## # A tibble: 217,609 x 4 ## book linenumber chapter word ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 Sense &amp; Sensibility 1 0 sense ## 2 Sense &amp; Sensibility 1 0 sensibility ## 3 Sense &amp; Sensibility 3 0 jane ## 4 Sense &amp; Sensibility 3 0 austen ## 5 Sense &amp; Sensibility 5 0 1811 ## 6 Sense &amp; Sensibility 10 1 chapter ## 7 Sense &amp; Sensibility 10 1 1 ## 8 Sense &amp; Sensibility 13 1 family ## 9 Sense &amp; Sensibility 13 1 dashwood ## 10 Sense &amp; Sensibility 13 1 settled ## # ... with 217,599 more rows Now, we can use this dataset to compute attributes for entities of interest. For instance, let’s create a data frame with words as entities, with an attribute containing the number of times the word appears in this corpus frequent_words &lt;- tidy_books %&gt;% count(word, sort=TRUE) %&gt;% filter(n &gt; 600) Which can then use like other data frames as we have used previously. For example to plot most frequent words: frequent_words %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(x=word, y=n)) + geom_col() + theme_bw() + labs(x=NULL, y=&quot;frequency&quot;) + coord_flip() 18.2 Handling dates The lubridate package provides common operations for parsing and operating on dates and times. See http://r4ds.had.co.nz/dates-and-times.html for more information. A number of functions for parsing dates in a variety of formats are provided, along with functions to extract specific components from parsed date objects library(lubridate) datetime &lt;- ymd_hms(&quot;2016-07-08 12:34:56&quot;) year(datetime) ## [1] 2016 month(datetime) ## [1] 7 day(datetime) ## [1] 8 mday(datetime) ## [1] 8 yday(datetime) ## [1] 190 wday(datetime) ## [1] 6 They can also return month and day of the week names, abbreviated, as ordered factors month(datetime, label=TRUE) ## [1] Jul ## 12 Levels: Jan &lt; Feb &lt; Mar &lt; Apr &lt; May &lt; Jun &lt; Jul &lt; Aug &lt; Sep &lt; ... &lt; Dec We can also create attributes of type datetime from string attributes. Here’s an example using the flights dataset flights_with_dt &lt;- flights %&gt;% mutate(dep_dt=make_datetime(year, month, day, dep_time %/% 100, dep_time %% 100)) %&gt;% select(year, month, day, dep_time, dep_dt) flights_with_dt ## # A tibble: 336,776 x 5 ## year month day dep_time dep_dt ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dttm&gt; ## 1 2013 1 1 517 2013-01-01 05:17:00 ## 2 2013 1 1 533 2013-01-01 05:33:00 ## 3 2013 1 1 542 2013-01-01 05:42:00 ## 4 2013 1 1 544 2013-01-01 05:44:00 ## 5 2013 1 1 554 2013-01-01 05:54:00 ## 6 2013 1 1 554 2013-01-01 05:54:00 ## 7 2013 1 1 555 2013-01-01 05:55:00 ## 8 2013 1 1 557 2013-01-01 05:57:00 ## 9 2013 1 1 557 2013-01-01 05:57:00 ## 10 2013 1 1 558 2013-01-01 05:58:00 ## # ... with 336,766 more rows With this attribute in place we can extract day of the week and plot the number of flights per day of the week flights_with_dt %&gt;% mutate(wday=wday(dep_dt, label=TRUE)) %&gt;% ggplot(aes(x=wday)) + geom_bar() "]
]
