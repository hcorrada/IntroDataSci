[
["index.html", "Lecture Notes: Introduction to Data Science CMSC320, University of Maryland, College Park 1 Preamble", " Lecture Notes: Introduction to Data Science CMSC320, University of Maryland, College Park Héctor Corrada Bravo 2019-01-31 1 Preamble These are lecture notes for CMSC320, Introduction to Data Science at the University of Maryland, College Park. Refer to the Course Web Page for further information. "],
["introduction-and-overview.html", "2 Introduction and Overview 2.1 What is Data Science? 2.2 Why Data Science? 2.3 Data Science in Society 2.4 Course Organization 2.5 General Workflow", " 2 Introduction and Overview 2.1 What is Data Science? Data science encapsulates the interdisciplinary activities required to create data-centric artifacts and applications that address specific scientific, socio-political, business, or other questions. Let’s look at the constiuent parts of this statement: 2.1.1 Data Measureable units of information gathered or captured from activity of people, places and things. 2.1.2 Specific Questions Seeking to understand a phenomenon, natural, social or other, can we formulate specific questions for which an answer posed in terms of patterns observed, tested and or modeled in data is appropriate. 2.1.3 Interdisciplinary Activities Formulating a question, assessing the appropriateness of the data and findings used to find an answer require understanding of the specific subject area. Deciding on the appropriateness of models and inferences made from models based on the data at hand requires understanding of statistical and computational methods. 2.1.4 Data-Centric Artifacts and Applications Answers to questions derived from data are usually shared and published in meaningful, succinct but sufficient, reproducible artifacts (papers, books, movies, comics). Going a step further, interactive applications that let others explore data, models and inferences are great. 2.2 Why Data Science? The granularity, size and accessibility data, comprising both physical, social, commercial and political spheres has exploded in the last decade or more. “I keep saying that the sexy job in the next 10 years will be statisticians” Hal Varian, Chief Economist at Google (http://www.nytimes.com/2009/08/06/technology/06stats.html?_r=0) “The ability to take data—to be able to understand it, to process it, to extract value from it, to visualize it, to communicate it—that’s going to be a hugely important skill in the next decades, not only at the professional level but even at the educational level for elementary school kids, for high school kids, for college kids.” “Because now we really do have essentially free and ubiquitous data. So the complimentary scarce factor is the ability to understand that data and extract value from it.” Hal Varian (http://www.mckinsey.com/insights/innovation/hal_varian_on_how_the_web_challenges_managers) 2.3 Data Science in Society Because of the large amount of data produced across many spheres of human social and creative activity, many societal questions may be addressed by establishing patterns in data. In the humanities, this can range from unproblematic quesitions of how to dissect a large creative corpora, say music, literature, based on raw characteristics of those works, text, sound and image. To more problematic questions, of analysis of intent, understanding, appreciation and valuation of these creative corpora. In the social sciences, issues of fairness and transparency in the current era of big data are especially problematic. Is data collected representative of population for which inferences are drawn? Are methods employed learning latent unfair factors from ostensibly fair data? These are issues that the research community is now starting to address. In all settings, issues of ethical collection of data, application of models, and deployment of data-centric artifacts are essential to grapple with. Issues of privacy are equally important. 2.4 Course Organization This course will cover basics of how to represent, model and communicate about data and data analyses using the R data analysis environment for Data Science. The course is roughly divided into five areas: Area 0: tools and skills ○ Toolset (Rstudio/tidyverse/rmarkdown) ○ Best practices ○ Debugging data science Area I: types and operations ○ Data tables and data types ○ Operations on tables ○ Basic plotting ○ Tidy data / the ER model ○ Relational Operations ○ SQL ○ Advanced: other data models, db consistency and concurrency Area II: wrangling ○ Data acquisition (load and scrape) ○ EDA Vis / grammar of graphics ○ Data cleaning (text, dates) ○ EDA: Summary statistics ○ Data analysis with optimization (derivatives) ○ Data transformations ○ Missing data Area III: modeling ○ Univariate probability and statistics ○ Hypothesis testing ○ Multivariate probablity and statistics (joint and conditional probability, Bayes thm) ○ Data Analysis with geometry (vectors, inner products, gradients and matrices) ○ Linear regression ○ Logistic regression ○ Gradient descent (batch and stochastic) ○ Trees and random forests ○ K-NN ○ Naïve Bayes ○ Clustering ○ PCA Area IV: essential applications ○ Text mining ○ Graphs ○ Forecasting Area V: communication ○ Designing data visualizations for communication not exploration ○ Interactive visualization ○ Writing about data, analysis, and inferences 2.5 General Workflow The data science activities we will cover are roughly organized into a general workflow that will help us navigate this material (Figure from Zumel and Mount). 2.5.1 Defining the Goal What is the question/problem? Who wants to answer/solve it? What do they know/do now? How well can we expect to answer/solve it? How well do they want us to answer/solve it? 2.5.2 Data Collection and Management What data is available? Is it good enough? Is it enough? What are sensible measurements to derive from this data? Units, transformations, rates, ratios, etc. 2.5.3 Modeling What kind of problem is it? E.g., classification, clustering, regression, etc. What kind of model should I use? Do I have enough data for it? Does it really answer the question? 2.5.4 Model Evaluation Did it work? How well? Can I interpret the model? What have I learned? 2.5.5 Presentation Again, what are the measurements that tell the real story? How can I describe and visualize them effectively? 2.5.6 Deployment Where will it be hosted? Who will use it? Who will maintain it? "],
["an-illustrative-analysis.html", "3 An Illustrative Analysis 3.1 Gathering data 3.2 Manipulating the data 3.3 Visualizing the data 3.4 Modeling data 3.5 Visualizing model result 3.6 Abstracting the analysis 3.7 Making analyses accessible 3.8 Summary", " 3 An Illustrative Analysis http://fivethirtyeight.com has a clever series of articles on the types of movies different actors make in their careers: https://fivethirtyeight.com/tag/hollywood-taxonomy/ I’d like to do a similar analysis. Let’s do this in order: Let’s do this analysis for Diego Luna Let’s use a clustering algorithm to determine the different types of movies they make Then, let’s write an application that performs this analysis for any actor and test it with Gael García Bernal Let’s make the application interactive so that a user can change the actor and the number of movie clusters the method learns. For now, we will go step by step through this analysis without showing how we perform this analysis using R. As the course progresses, we will learn how to carry out these steps. 3.1 Gathering data 3.1.1 Movie ratings For this analysis we need to get the movies Diego Luna was in, along with their Rotten Tomatoes ratings. For that we scrape this webpage: https://www.rottentomatoes.com/celebrity/diego_luna. Once we scrape the data from the Rotten Tomatoes website and clean it up, this is part of what we have so far: RATING TITLE CREDIT BOX OFFICE YEAR 95 If Beale Street Could Talk Pedrocito — 2019 4 Flatliners Ray $16.9M 2017 84 Rogue One: A Star Wars Story Captain Cassian Andor $532.2M 2016 89 Blood Father Jonah — 2016 81 The Book of Life Manolo — 2014 100 I Stay with You (Me quedo contigo) Actor — 2014 66 Elysium Julio $90.9M 2013 This data includes, for each of the movies Diego Luna has acted in, the rotten tomatoes rating, the movie title, Diego Luna’s role in the movie, the U.S. domestic gross and the year of release. 3.1.2 Movie budgets and revenue For the movie budgets and revenue data we scrape this webpage: http://www.the-numbers.com/movie/budgets/all (Note 01.2018: after the initial version of this analysis, this website added pagination to this URL. We will be using the CSV file scraped originally in Summer 2017 for this analysis and leave the issue of dealing with pagination as an exercise.) ## Parsed with column specification: ## cols( ## release_date = col_date(format = &quot;&quot;), ## movie = col_character(), ## production_budget = col_double(), ## domestic_gross = col_double(), ## worldwide_gross = col_double() ## ) This is part of what we have for that table after loading and cleaning up: release_date movie production_budget domestic_gross worldwide_gross 2009-12-18 Avatar 425 760.50762 2783.9190 2015-12-18 Star Wars Ep. VII: The Force Awakens 306 936.66223 2058.6622 2007-05-24 Pirates of the Caribbean: At World’s End 300 309.42043 963.4204 2015-11-06 Spectre 300 200.07417 879.6209 2012-07-20 The Dark Knight Rises 275 448.13910 1084.4391 2013-07-02 The Lone Ranger 275 89.30212 260.0021 2012-03-09 John Carter 275 73.05868 282.7781 2010-11-24 Tangled 260 200.82194 586.5819 2007-05-04 Spider-Man 3 258 336.53030 890.8753 2015-05-01 Avengers: Age of Ultron 250 459.00587 1404.7059 This data is for 5358 movies, including its release date, title, production budget and total gross. The latter two are in millions of U.S. dollars. One thing we might want to check is if the budget and gross entries in this table are inflation adjusted or not. To do this, we can make a plot of domestic gross, which we are using for the subsequent analyses. ## ## Attaching package: &#39;lubridate&#39; ## The following object is masked from &#39;package:base&#39;: ## ## date Although we don’t know for sure, since the source of our data does not state this specifically, it looks like the domestic gross measurement is not inflation adjusted since gross increases over time. 3.2 Manipulating the data Next, we combine the datasets we obtained to get closer to the data we need to make the plot we want. We combine the two datasets using the movie title, so that the end result has the information in both tables for each movie. RATING TITLE CREDIT BOX OFFICE YEAR release_date production_budget domestic_gross worldwide_gross 4 Flatliners Ray $16.9M 2017 1990-08-10 26.0 61.30815 61.30815 84 Rogue One: A Star Wars Story Captain Cassian Andor $532.2M 2016 2016-12-16 200.0 532.17732 1050.98849 81 The Book of Life Manolo — 2014 2014-10-17 50.0 50.15154 97.65154 66 Elysium Julio $90.9M 2013 2013-08-09 120.0 93.05012 286.19209 51 Contraband Gonzalo $66.5M 2012 2012-01-13 25.0 66.52800 98.40685 93 Milk Jack Lira $31.8M 2008 2008-11-26 20.0 31.84130 57.29337 69 Criminal Rodrigo $0.8M 2004 2016-04-15 31.5 14.70870 38.77126 60 The Terminal Enrique Cruz $77.1M 2004 2004-06-18 75.0 77.07396 218.67396 79 Open Range Button $58.3M 2003 2003-08-15 26.0 58.33125 68.61399 75 Frida Alejandro Gomez $25.7M 2002 2002-10-25 12.0 25.88500 56.13124 3.3 Visualizing the data Now that we have the data we need, we can make a plot: Figure 3.1: Ratings and U.S. Domestic Gross of Diego Luna’s movies. We see that there is one clear outlier in Diego Luna’s movies, which probably is the one Star Wars movie he acted in. The remaining movies could potentially be grouped into two types of movies, those with higher rating and those with lower ratings. 3.4 Modeling data We can use a clustering algorithm to partition Diego Luna’s movies. We can use the data we obtained so far and see if the k-means clustering algorithm partitions these movies into three sensible groups using the movie’s rating and domestic gross. Let’s see how the movies are grouped: TITLE RATING domestic_gross cluster Rogue One: A Star Wars Story 84 532.17732 1 The Book of Life 81 50.15154 2 Milk 93 31.84130 2 Criminal 69 14.70870 2 Open Range 79 58.33125 2 Frida 75 25.88500 2 Flatliners 4 61.30815 3 Elysium 66 93.05012 3 Contraband 51 66.52800 3 The Terminal 60 77.07396 3 3.5 Visualizing model result Let’s remake the same plot as before, but use color to indicate each movie’s cluster assignment given by the k-means algorithm. The algorithm did make the Star Wars movie it’s own group since it’s so different that the other movies. The grouping of the remaining movies is not as clean. To make the plot and clustering more interpretable, let’s annotate the graph with some movie titles. In the k-means algorithm, each group of movies is represented by an average rating and an average domestic gross. What we can do is find the movie in each group that is closest to the average and use that movie title to annotate each group in the plot. Roughly, movies are clustered into Star Wars and low vs. high rated movies. The latter seem to have some difference in domestic gross. For example, movies like “The Terminal” have lower rating but make slightly more money than movies like “Frida”. We could use statistical modeling to see if that’s the case, but will skip that for now. Do note also, that the clustering algorithm we used seems to be assigning one of the movies incorrectly, which warrants further investigation. 3.6 Abstracting the analysis While not a tremendous success, we decide we want to carry on with this analysis. We would like to do this for other actors’ movies. One of the big advantages of using R is that we can write a piece of code that takes an actor’s name as input, and reproduces the steps of this analysis for that actor. We call these functions, we’ll see them and use them a lot in this course. For our analysis, this function must do the following: Scrape movie ratings from Rotten Tomatoes Clean up the scraped data Join with the budget data we downloaded previously Perform the clustering algorithm Make the final plot With this in mind, we can write functions for each of these steps, and then make one final function that puts all of these together. For instance, let’s write the scraping function. It will take an actor’s name and output the scraped data. Let’s test it with Gael García Bernal: RATING TITLE CREDIT BOXOFFICE YEAR No Score Yet Wasp Network Actor — 2019 90% Museum (Museo) Juan Nuñez — 2018 88% The Kindergarten Teacher Instructor — 2018 Good start. We can then write functions for each of the steps we did with Diego Luna before. Then put all of these steps into one function that calls our new functions to put all of our analysis together: We can test this with Gael García Bernal analyze_actor(&quot;Gael Garcia Bernal&quot;) 3.7 Making analyses accessible Now that we have written a function to analyze an actor’s movies, we can make these analyses easier to produce by creating an interactive application that wraps our new function. The shiny R package makes creating this type of application easy. 3.8 Summary In this analysis we saw examples of the common steps and operations in a data analysis: Data ingestion: we scraped and cleaned data from publicly accessible sites Data manipulation: we integrated data from multiple sources to prepare our analysis Data visualization: we made plots to explore patterns in our data Data modeling: we made a model to capture the grouping patterns in data automatically, using visualization to explore the results of this modeling Publishing: we abstracted our analysis into an application that allows us and others to perform this analysis over more datasets and explore the result of modeling using a variety of parameters "],
["setting-up-the-r-data-science-toolbox.html", "4 Setting up the R Data Science Toolbox 4.1 Some history 4.2 Setting up R 4.3 Setting up Rstudio 4.4 A first look at Rstudio 4.5 R packages 4.6 Additional R resources 4.7 Literate Programming 4.8 Finishing your setup", " 4 Setting up the R Data Science Toolbox Here we setup R, RStudio and anything else we will use in the course. 4.1 Some history R is an offspring of S, a language created in AT&amp;T Labs by John Chambers (now at Stanford) and others in 1976 with the goal of creating an environment for statistical computing and data analysis. The standard for the language in current use was settled in 1998. That same year, “S” won the ACM Software System award, awarded to software systems “that have a lasting influence, reflected in contributions to concepts, in commercial acceptance, or both”. In 1991, Robert Gentleman and Ross Ihaka created R to provide an open source implementation of the S language and environment. They also redesigned the language to enforce lexical scoping rules. It has been maintained by the R core group since 1997, and in 2015 an R consortium, including Microsoft, Google, and others, was created. Along with Python it is one of the most popular environments for data analysis (e.g., figure below from KDNuggets 2017 software survey We use R for this class because we find that besides it being a state-of-the-art data analysis environment, it provides a clean end-to-end platform for teaching material across the data management-modeling-communication spectrum that we study in class. However, be aware that as you move on in the Data Science field you most likely will need to add Python to your toolbelt. 4.2 Setting up R R is a free, open source, environment for data analysis. It is available as a free binary download for Mac, Linux and Windows. For the more adventorous, it can also be compiled from source. To install R in your computer go to https://cran.r-project.org/index.html and download and install the appropriate binary file. This will install the base R system: the R programming language, a few packages for common data analyses and a development environment. 4.3 Setting up Rstudio We will actually use Rstudio to interact with R. Rstudio is a very powerful application to make data analysis with R easier to do. To install go to https://www.rstudio.com/products/rstudio/download/ and download the appropriate version of Rstudio. 4.4 A first look at Rstudio Let’s take a first look at Rstudio. The first thing you will notice is that Rstudio is divided into panes. Let’s take a look first at the Console. 4.4.1 Interactive Console The most immediate way to interact with R is through the interactive console. Here we can write R instructions to perform our data analyses. We want to start using data so the first instructions we will look at deal with loading data. When you installed R, a few illustrative datasets were installed as well. Let’s take a look at the list of datasets you now have access to. Write the following command in the console This will list names and descriptions of datasets available in your R installation. Let’s try to find out more information about these datasets. In R, the first attempt to get help with something is to use the ? operation. So, to get help about the swiss dataset we can enter the following in the console This will make the documentation for the swiss dataset open in another pane. On your own: Find more information about a different dataset using the ? operator. 4.4.2 Data Viewer According to the documentation we just saw for swiss, this is a data.frame with 47 observations and 6 variables. The data.frame is the basic structure we will use to represent data throughout the course. We will see this again repeatedly, and use a couple of other names (e.g., tibble) to refer to this. Intuitively, you can think of the data.frame like a spreadsheet, with rows representing observations, and columns representing variables that describe those observations. Let’s see what the swiss data looks like using the Rstudio data viewer. The Data Viewer lets you reorder data by the values in a column. It also lets you filter rows of the data by values as well. On your own: Use the Data Viewer to explore another of the datasets you saw listed before. 4.4.3 Names, values and functions Let’s make a very short pause to talk about something you may have noticed. In the console, we’ve now written a few instructions, e.g. View(swiss). Let’s take a closer look at how these instructions are put together. expressions: first of all, we call these instructions expressions, which are just text that R can evaluate into a value. View(swiss) is an expression. values: so, what’s a value? They are numbers, strings, data frames, etc. This is the data we will be working with. The number 2 is a value. So is the string &quot;Hector&quot;. So, what value is produced when R evaluates the expression View(swiss)? Nothing, which we also treat as a value. That wasn’t very interesting, but it does have a side effect: it shows the swiss dataset in the Data viewer. How about a simpler expression: swiss, what value is produced when R evaluates the expression swiss? The data.frame containing that data. Try it out in the console. names: so if swiss isn’t a value, what is it? It is a name. We use these to refer to values. So, when we write the expression swiss, we tell R we want the value referenced by the name swiss, that is, the data itself! functions: Besides numbers, strings, data frames, etc. another important type of value is the function. Functions are a series of instructions that take some input value and produce a different value. The name View refers to the function that takes a data frame as input, and displays it in the Data viewer. Functions are called using the parentheses we saw before: View(swiss), the parentheses say that you are passing input swiss to the function View. We’ll see later how we can write our own functions. 4.4.4 Plotting Next, I want to show the Plots pane in Rstudio. Let’s make a plot using the swiss dataset: It’s not pretty, but it was very easy to produce. There’s a couple of things going on here… plot is a function, it takes two inputs, the data to put in the x and y axes, evaluates to nothing, but creates a plot of the data swiss$Education is how we refer to the Education column in the swiss data frame. On your own: Make a plot using other variables in the swiss dataset. 4.4.5 Editor So far, we’ve made some good progress: we know how to write expressions on the R console so that they are evaluated, we are starting to get a basic understanding of how these expressions are constructed, we can use the Data viewer to explore data frames, and made one plot that was displayed in the Plots pane. To finish this quick tour, I want to look at two more Rstudio panes: the file editor, and the File viewer. As you have noticed, everytime we want to evaluate an expression on the console, we have to write it in. For example, if we want to change the plot we made above to include a different variable, we have to write the whole thing again. Also, what if I forgot what expression I used to make a specific plot? Even better, what if I wanted somebody else to make the plot I just made? By far, one of the biggest advantages of using R over Excel or other similar programs, is that we can write expressions in scripts that are easy to share with others, making analyses easier to reproduce. Let’s write a script that we can use to make the same plot we just made. In the Rstudio menu select File&gt;New File&gt;R Script This will open a tab in the File editor in which we can write expressions: We can then evaluate the expressions in the file one at a time, or all at the same time. We can then save these expressions in a script. In the Rstudio menu select File&gt;Save and save as a text file. The convention is to use the .R or .r file extension, e.g., swiss_plot.r. On your own: Add expressions for additional plots to the script and save again. Run the new expressions. 4.4.6 Files viewer Rstudio includes a Files viewer that you can use to find and load files. You can find the Files near the Plots viewer 4.5 R packages Another of R’s advantages for data analysis is that it has attracted a large number of extremely useful additions provided by users worldwide. These are housed in CRAN. In this course we will make a lot of use of a set of packages bundled together into the tidyverse by Hadley Wickham and others. These packages make preparing, modeling and visualizing certain kinds data (which covers the vast majority of use cases) quite fun and pleasent. There is a webpage for the general tidyverse project: http://tidyverse.org, which includes pages for each of the packages included there. Let’s install the tidyverse into your R environment. There are two ways of installing packages. In the console, you can use the expression: In Rstudio, you can use the Packages tab: On your own: Install the following additional packages which we will use later on: rvest, stringr, nycflights13 and broom. 4.6 Additional R resources Resources for learning and reading about R are listed in our here. Of note are the swirl project and DataCamp’s [introduction to R] course. 4.7 Literate Programming One last note before we get started. R has great support for literate programming, where source code that contains both code, the result of evaluating that code, and text explaining that code co-exist in a single document. This is extremely valuable in data analysis, as many choices made by data analysts are worth explaning in text, and interpretation of the results of analyses can co-exist with the computations used in that analysis. This document you are reading contains both text and code. In class, we will use Rmarkdown for this purpose. 4.8 Finishing your setup Complete your exit ticket as instructed. "],
["part-data-representation-modeling-ingestion-and-cleaning.html", "(Part) Data representation modeling, ingestion and cleaning", " (Part) Data representation modeling, ingestion and cleaning "],
["measurements-and-data-types.html", "5 Measurements and Data Types 5.1 A data analysis to get us going 5.2 Getting data 5.3 Entities and attributes 5.4 Categorical attributes 5.5 Discrete numeric attributes 5.6 Continuous numeric data 5.7 Other examples 5.8 Other important datatypes 5.9 Units 5.10 Quick questions", " 5 Measurements and Data Types Now that we have our tools ready, let’s start doing some analysis. First, let’s go over some principles of R as a data analysis environment. R is a computational environment for data analysis. It is designed around a functional language, as opposed to procedural languages like Java or C, that has desirable properties for the type of operations and workflows that are frequently performed in the course of analyzing datasets. In this exercise we will start learning some of those desirable properties while performing an analysis of a real dataset. 5.1 A data analysis to get us going I’m going to do an analysis of Baltimore crime to guide our discussion of R. We’ll use data downloaded from Baltimore City’s awesome open data site (this was downloaded a couple of years ago so if you download now, you will get different results). The repository for this particular data is here. https://data.baltimorecity.gov/Crime/BPD-Arrests/3i3v-ibrt 5.2 Getting data We’ve prepared the data previously into a comma-separated value file (.csv file): each line contains attribute values (separated by commas) describing arrests in the City of Baltimore. The read_csv command is part of the readr R package and allows you to read a dataset stored in a csv file. This function is extremely versatile, and you can read more about it by using the standard help system in R: ?read_csv. The result of running calling this function is the data itself, so, by running the function in the console, the result of the function is printed. Note: To download this dataset to follow along you can use the following code: if (!dir.exists(&quot;data&quot;)) dir.create(&quot;data&quot;) download.file(&quot;https://www.hcbravo.org/IntroDataSci/misc/BPD_Arrests.csv&quot;, destfile=&quot;data/BPD_Arrests.csv&quot;) To make use of this dataset we want to assign the result of calling read_csv (i.e., the dataset) to a variable: library(tidyverse) arrest_tab &lt;- read_csv(&quot;data/BPD_Arrests.csv&quot;) arrest_tab ## # A tibble: 104,528 x 15 ## arrest age race sex arrestDate arrestTime arrestLocation ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;time&gt; &lt;chr&gt; ## 1 1.11e7 23 B M 01/01/2011 00&#39;00&quot; &lt;NA&gt; ## 2 1.11e7 37 B M 01/01/2011 01&#39;00&quot; 2000 Wilkens … ## 3 1.11e7 46 B M 01/01/2011 01&#39;00&quot; 2800 Mayfield… ## 4 1.11e7 50 B M 01/01/2011 04&#39;00&quot; 2100 Ashburto… ## 5 1.11e7 33 B M 01/01/2011 05&#39;00&quot; 4000 Wilsby A… ## 6 1.11e7 41 B M 01/01/2011 05&#39;00&quot; 2900 Spellman… ## 7 1.11e7 29 B M 01/01/2011 05&#39;00&quot; 800 N Monroe … ## 8 1.11e7 20 W M 01/01/2011 05&#39;00&quot; 5200 Moravia … ## 9 1.11e7 24 B M 01/01/2011 07&#39;00&quot; 2400 Gainsdbo… ## 10 1.11e7 53 B M 01/01/2011 15&#39;00&quot; 3300 Woodland… ## # … with 104,518 more rows, and 8 more variables: incidentOffense &lt;chr&gt;, ## # incidentLocation &lt;chr&gt;, charge &lt;chr&gt;, chargeDescription &lt;chr&gt;, ## # district &lt;chr&gt;, post &lt;dbl&gt;, neighborhood &lt;chr&gt;, `Location 1` &lt;chr&gt; Now we can ask what type of value is stored in the arrest_tab variable: class(arrest_tab) ## [1] &quot;spec_tbl_df&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; The data.frame is a workhorse data structure in R. It encapsulates the idea of entities (in rows) and attribute values (in columns). We call these rectangular datasets. The other types tbl_df and tbl are added by tidyverse for improved functionality. We can ask other features of this dataset: # This is a comment in R, by the way # How many rows (entities) does this dataset contain? nrow(arrest_tab) ## [1] 104528 # How many columns (attributes)? ncol(arrest_tab) ## [1] 15 # What are the names of those columns? colnames(arrest_tab) ## [1] &quot;arrest&quot; &quot;age&quot; &quot;race&quot; ## [4] &quot;sex&quot; &quot;arrestDate&quot; &quot;arrestTime&quot; ## [7] &quot;arrestLocation&quot; &quot;incidentOffense&quot; &quot;incidentLocation&quot; ## [10] &quot;charge&quot; &quot;chargeDescription&quot; &quot;district&quot; ## [13] &quot;post&quot; &quot;neighborhood&quot; &quot;Location 1&quot; Now, in Rstudio you can view the data frame using View(arrest_tab). 5.2.1 Names, values and functions Let’s review the concepts of names values and functions again. In the console, we’ve now written a few instructions, e.g. View(arrest_tab). Let’s take a closer look at how these instructions are put together. expressions: first of all, we call these instructions expressions, which are just text that R can evaluate into a value. View(arrest_tab) is an expression. values: so, what’s a value? They are numbers, strings, data frames, etc. This is the data we will be working with. The number 2 is a value. So is the string &quot;Hector&quot;. So, what value is produced when R evaluates the expression View(arrest_tab)? Nothing, which we also treat as a value. That wasn’t very interesting, but it does have a side effect: it shows the arrest_tab dataset in the Data viewer. How about a simpler expression: arrest_tab, what value is produced when R evaluates the expression arrest_tab? The data.frame containing that data. Try it out in the console. names: so if arrest_tab isn’t a value, what is it? It is a name. We use these to refer to values. So, when we write the expression arrest_tab, we tell R we want the value referenced by the name arrest_tab, that is, the data itself! functions: Besides numbers, strings, data frames, etc. another important type of value is the function. Functions are a series of instructions that take some input value and produce a different value. The name View refers to the function that takes a data frame as input, and displays it in the Data viewer. Functions are called using the parentheses we saw before: View(arrest_tab), the parentheses say that you are passing input arrest_tab to the function View. We’ll see later how we can write our own functions. 5.3 Entities and attributes As a reminder, we are using the term entities to refer to the objects to which data in a dataset refers to. For instance, in our example dataset, each arrest is an entity. In a rectangular dataset (a data frame) this corresponds to rows in a table. We then say that a dataset contains attributes for each entity. For instance, attributes of each arrest would be the person’s age, the type of offense, the location, etc. In a rectangular dataset, this corresponds to the columns in a table. This language of entities and attributes is commonly used in the database literature. In statistics you may see experimental units or samples for entities and covariates for attributes. In other instances observations for entities and variables for attributes. In Machine Learning you may see example for entities and features for attributes. For the most part, all of these are exchangable. This table summarizes the terminology: Field Entities Attributes Databases Entities Attributes Machine Learning Examples Features Statistics Observations/Samples Variables/Covariates This chapter is concerned with the types of data we may encounter as attributes in data analyses. 5.4 Categorical attributes A categorical attribute for a given entity can take only one of a finite set of examples. For example, the sex variable can only have value M, F, or `` (we’ll talk about missing data later in the semester). table(arrest_tab$sex) ## ## F M ## 19431 85095 The result of a coin flip is categorical: heads or tails. The outcome of rolling an 8-sided die is categorical: one, two, …, eight. Can you think of other examples? Categorical data may be unordered or ordered. In our example dataset all categorical data is unordered, e.g., sex, race, etc. Examples of ordered categorical data are grades in a class, Likert scale categories, e.g., strongly agree, agree, neutral, disagree, strongly disagree, etc. 5.4.1 Factors in R We said that R is designed for data analysis. My favorite example of how that manifests itself is the factor datatype. If you look at your dataset now, arrest_tab$sex is a vector of strings: class(arrest_tab$sex) ## [1] &quot;character&quot; summary(arrest_tab$sex) ## Length Class Mode ## 104528 character character However, as a measurement, or attribute, it should only take one of two values (or three depending on how you record missing, unknown or unspecified). So, in R, that categorical data type is called a factor. Notice what the summary function does after turning the sex attribute into a factor: arrest_tab$sex &lt;- factor(arrest_tab$sex) summary(arrest_tab$sex) ## F M NA&#39;s ## 19431 85095 2 This distinction shows up in many other places where functions have different behavior when called on different types of values. The possible values a factor can take are called levels: levels(arrest_tab$sex) ## [1] &quot;F&quot; &quot;M&quot; Exercise: you should transform the race attribute into a factor as well. How many levels does it have? 5.5 Discrete numeric attributes These are attributes that can take specific values from elements of ordered, discrete (possibly infinite) sets. The most common set in this case would be the non-negative positive integers. This data is commonly the result of counting processes. In our example dataset, age, measured in years, is a discrete attribute. Frequently, we obtain datasets as the result of summarizing, or aggregating other underlying data. In our case, we could construct a new dataset containing the number of arrests per neighborhood (we will see how to do this later) ## # A tibble: 6 x 2 ## neighborhood number_of_arrests ## &lt;chr&gt; &lt;int&gt; ## 1 Abell 62 ## 2 Allendale 297 ## 3 Arcadia 78 ## 4 Arlington 694 ## 5 Armistead Gardens 153 ## 6 Ashburton 78 In this new dataset, the entities are each neighborhood, the number_of_arrests attribute is a discrete numeric attribute. Other examples: the number of students in a class is discrete, the number of friends for a specific Facebook user. Can you think of other examples? Distinctions between ordered categorical and discrete numerical data is that ordered categorical data do not have magnitude. For instance, is an ‘A’ in a class twice as good as a ‘C’? Is a ‘C’ twice as good as a ‘D’? Not necessarily. Grades don’t have an inherent magnitude. However, if we encode grades as ‘F=0,D=1,C=2,B=3,A=4’, etc. they do have magnitude. In that case, an ‘A’ is twice as good as a ‘C’, and a ‘C’ is twice as good as a ‘D’. So in summary, if ordered data has magnitude, then discrete numeric if not, ordered categorical. 5.6 Continuous numeric data These are attributes that can take any value in a continuous set. For example, a person’s height, in say inches, can take any number (within the range of human heights). Here is another dataset we can use to look at this datatype. In this case, entities are cars and we look at continuous numeric attributes speed and stopping distance: The distinction between continuous and discrete is a bit tricky since measurements that have finite precision are, in a sense, discrete. Remember, however, that continuity is not a property of the specific dataset you have in hand, but rather of the process you are measuring. The number of arrests in a neighborhood cannot, in principle, be fractional, regardless of the precision at which we measure this. If we had the appropriate tool, we could measure a person’s height with infinite precision. This distinction is very important when we build statistical models of datasets for analysis. For now, think of discrete data as the result of counting, and continuous data the result of some physical measurement. Here’s a question: is age in our dataset a continuous or discrete numeric value? 5.7 Other examples Consider a dataset of images like the super-famous MNIST dataset of handwritten digits. This dataset contains images of handwritten digits. So each image is an entity. Each image has a label attribute which states which of the digits 0,1,…9 is represented by the image. What type of data is this (categorical, continuous numeric, or discrete numeric)? Now, each image is represented by grayscale values in a 28x28 grid. That’s 784 attributes, one for each square in the grid, containing a grayscale value. Now what type of data are these other 784 attributes? 5.8 Other important datatypes The three datatypes we saw above encompass a fairly large swath of data you will come across. Our arrest dataset contains other important datatypes that we will run across frequently: Text: Arbitrary strings that do not encode a categorical attribute. Datetime: Date and time of some event or observation (e.g., arrestDate, arrestTime) Geolocation: Latitude and Longitude of some event or observation (e.g., Location.) 5.9 Units Something that we tend to forget but is extremely important for the modeling and interpretation of data is that attributes are for the most part measurements and that they have units. For example, age of a person can be measured in different units: years, months, etc. These can be converted to one another, but nonetheless in a given dataset, that attribute or measurement will be recorded in some specific units. Similar arguments go for distances and times, for example. In other cases, we may have unitless measurements (we will see later an example of this when we do dimensionality reduction). In these cases, it is worth thinking about why your measurements are unit-less. When performing analyses that try to summarize the effect of some measurement or attribute on another, units matter a lot! We will see the importance of this in our regression section. For now, make sure you make a mental note of units for each measurement you come across. This will force you to think about where and how your data was obtained, which will become very important when modeling and interpreting the results of these models. 5.10 Quick questions True or False. In a rectangular dataset rows correspond to attributes describing an entity. True or False. In a rectangular dataset rows correspond to entities, the units of observation we are interested in analyzing Suppose I have collected a dataset of sales of handheld devices around the world with attributes country, date, device model, total sales. Specify the data type for each of these attributes. "],
["principles-basic-operations.html", "6 Principles: Basic Operations 6.1 Operations that select attributes 6.2 Operations that select entities 6.3 Pipelines of operations", " 6 Principles: Basic Operations Now that we have a data frame describing our data, let’s learn a few fundamental operations we perform on data frames on almost any analysis. We divide these first set of operations into two groups: operations on attributes and operations on entitites. These operations are defined in the dplyr package, part of the tidyverse, and are described in more detail in the “R for Data Science” textbook available in the course logistics page: http://r4ds.had.co.nz/transform.html. 6.1 Operations that select attributes 6.1.1 select In our data set we have a large number of attributes describing each arrest. Now, suppose we only want to study patterns in these arrests based on a smaller number of attributes for purposes of efficiency, since we would operate over less data, or interpretability. In that case we would like to create a data frame that contains only those attributes of interest. We use the select function for this. Let’s create a data frame containing only the age, sex and district attributes select(arrest_tab, age, sex, district) ## # A tibble: 104,528 x 3 ## age sex district ## &lt;dbl&gt; &lt;fct&gt; &lt;chr&gt; ## 1 23 M &lt;NA&gt; ## 2 37 M SOUTHERN ## 3 46 M NORTHEASTERN ## 4 50 M WESTERN ## 5 33 M NORTHERN ## 6 41 M SOUTHERN ## 7 29 M WESTERN ## 8 20 M NORTHEASTERN ## 9 24 M &lt;NA&gt; ## 10 53 M NORTHWESTERN ## # … with 104,518 more rows The first argument to the select function is the data frame we want to operate on, the remaining arguments describe the attributes we want to include in the resulting data frame. Note a few other things: The first argument to select is a data frame, and the value returned by select is also a data frame As always you can learn more about the function using ?select Attribute descriptor arguments can be fairly sophisticated. For example, we can use positive integers to indicate attribute (column) indices: select(arrest_tab, 1, 3, 4) ## # A tibble: 104,528 x 3 ## arrest race sex ## &lt;dbl&gt; &lt;chr&gt; &lt;fct&gt; ## 1 11126858 B M ## 2 11127013 B M ## 3 11126887 B M ## 4 11126873 B M ## 5 11126968 B M ## 6 11127041 B M ## 7 11126932 B M ## 8 11126940 W M ## 9 11127051 B M ## 10 11127018 B M ## # … with 104,518 more rows R includes a useful operator to describe ranges. E.g., 1:5 would be attributes 1 through 5: select(arrest_tab, 1:5) ## # A tibble: 104,528 x 5 ## arrest age race sex arrestDate ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; ## 1 11126858 23 B M 01/01/2011 ## 2 11127013 37 B M 01/01/2011 ## 3 11126887 46 B M 01/01/2011 ## 4 11126873 50 B M 01/01/2011 ## 5 11126968 33 B M 01/01/2011 ## 6 11127041 41 B M 01/01/2011 ## 7 11126932 29 B M 01/01/2011 ## 8 11126940 20 W M 01/01/2011 ## 9 11127051 24 B M 01/01/2011 ## 10 11127018 53 B M 01/01/2011 ## # … with 104,518 more rows We can also use other helper functions to create attribute descriptors. For example, to choose all attributes that begin with the letter a we can the starts_with function which uses partial string matching: select(arrest_tab, starts_with(&quot;a&quot;)) ## # A tibble: 104,528 x 5 ## arrest age arrestDate arrestTime arrestLocation ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;time&gt; &lt;chr&gt; ## 1 11126858 23 01/01/2011 00&#39;00&quot; &lt;NA&gt; ## 2 11127013 37 01/01/2011 01&#39;00&quot; 2000 Wilkens Ave ## 3 11126887 46 01/01/2011 01&#39;00&quot; 2800 Mayfield Ave ## 4 11126873 50 01/01/2011 04&#39;00&quot; 2100 Ashburton St ## 5 11126968 33 01/01/2011 05&#39;00&quot; 4000 Wilsby Ave ## 6 11127041 41 01/01/2011 05&#39;00&quot; 2900 Spellman Rd ## 7 11126932 29 01/01/2011 05&#39;00&quot; 800 N Monroe St ## 8 11126940 20 01/01/2011 05&#39;00&quot; 5200 Moravia Rd ## 9 11127051 24 01/01/2011 07&#39;00&quot; 2400 Gainsdbourgh Ct ## 10 11127018 53 01/01/2011 15&#39;00&quot; 3300 Woodland Ave ## # … with 104,518 more rows We can also use the attribute descriptor arguments to drop attributes. For instance using descriptor -age returns the arrest data frame with all but the age attribute included: select(arrest_tab, -age) ## # A tibble: 104,528 x 14 ## arrest race sex arrestDate arrestTime arrestLocation incidentOffense ## &lt;dbl&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;time&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1.11e7 B M 01/01/2011 00&#39;00&quot; &lt;NA&gt; Unknown Offense ## 2 1.11e7 B M 01/01/2011 01&#39;00&quot; 2000 Wilkens … 79-Other ## 3 1.11e7 B M 01/01/2011 01&#39;00&quot; 2800 Mayfield… Unknown Offense ## 4 1.11e7 B M 01/01/2011 04&#39;00&quot; 2100 Ashburto… 79-Other ## 5 1.11e7 B M 01/01/2011 05&#39;00&quot; 4000 Wilsby A… Unknown Offense ## 6 1.11e7 B M 01/01/2011 05&#39;00&quot; 2900 Spellman… 81-Recovered P… ## 7 1.11e7 B M 01/01/2011 05&#39;00&quot; 800 N Monroe … 79-Other ## 8 1.11e7 W M 01/01/2011 05&#39;00&quot; 5200 Moravia … Unknown Offense ## 9 1.11e7 B M 01/01/2011 07&#39;00&quot; 2400 Gainsdbo… 54-Armed Person ## 10 1.11e7 B M 01/01/2011 15&#39;00&quot; 3300 Woodland… 54-Armed Person ## # … with 104,518 more rows, and 7 more variables: incidentLocation &lt;chr&gt;, ## # charge &lt;chr&gt;, chargeDescription &lt;chr&gt;, district &lt;chr&gt;, post &lt;dbl&gt;, ## # neighborhood &lt;chr&gt;, `Location 1` &lt;chr&gt; 6.1.2 rename To improve interpretability during an analysis we may want to rename attributes. We use the rename function for this: rename(arrest_tab, arrest_date=arrestDate) ## # A tibble: 104,528 x 15 ## arrest age race sex arrest_date arrestTime arrestLocation ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;time&gt; &lt;chr&gt; ## 1 1.11e7 23 B M 01/01/2011 00&#39;00&quot; &lt;NA&gt; ## 2 1.11e7 37 B M 01/01/2011 01&#39;00&quot; 2000 Wilkens … ## 3 1.11e7 46 B M 01/01/2011 01&#39;00&quot; 2800 Mayfield… ## 4 1.11e7 50 B M 01/01/2011 04&#39;00&quot; 2100 Ashburto… ## 5 1.11e7 33 B M 01/01/2011 05&#39;00&quot; 4000 Wilsby A… ## 6 1.11e7 41 B M 01/01/2011 05&#39;00&quot; 2900 Spellman… ## 7 1.11e7 29 B M 01/01/2011 05&#39;00&quot; 800 N Monroe … ## 8 1.11e7 20 W M 01/01/2011 05&#39;00&quot; 5200 Moravia … ## 9 1.11e7 24 B M 01/01/2011 07&#39;00&quot; 2400 Gainsdbo… ## 10 1.11e7 53 B M 01/01/2011 15&#39;00&quot; 3300 Woodland… ## # … with 104,518 more rows, and 8 more variables: incidentOffense &lt;chr&gt;, ## # incidentLocation &lt;chr&gt;, charge &lt;chr&gt;, chargeDescription &lt;chr&gt;, ## # district &lt;chr&gt;, post &lt;dbl&gt;, neighborhood &lt;chr&gt;, `Location 1` &lt;chr&gt; Like select, the first argument to the function is the data frame we are operating on. The remaining arguemnts specify attributes to rename and the name they will have in the resulting data frame. Note that arguments in this case are named (have the form lhs=rhs). We can have selection and renaming by using named arguments in select: select(arrest_tab, age, sex, arrest_date=arrestDate) ## # A tibble: 104,528 x 3 ## age sex arrest_date ## &lt;dbl&gt; &lt;fct&gt; &lt;chr&gt; ## 1 23 M 01/01/2011 ## 2 37 M 01/01/2011 ## 3 46 M 01/01/2011 ## 4 50 M 01/01/2011 ## 5 33 M 01/01/2011 ## 6 41 M 01/01/2011 ## 7 29 M 01/01/2011 ## 8 20 M 01/01/2011 ## 9 24 M 01/01/2011 ## 10 53 M 01/01/2011 ## # … with 104,518 more rows Also like select, the result of calling rename is a data frame. In fact, this will be the case for almost all operations in the tidyverse they operate on data frames (specified as the first ment in the function call) and return data frames. 6.2 Operations that select entities Next, we look at operations that select entities from a data frame. We will see a few operations to do this: selecting specific entities (rows) by position, selecting them based on attribute properties, and random sampling. 6.2.1 slice We can choose specific entities by their row position. For instance, to choose entities in rows 1,3 and 10, we would use the following: slice(arrest_tab, c(1, 3, 10)) ## # A tibble: 3 x 15 ## arrest age race sex arrestDate arrestTime arrestLocation ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;time&gt; &lt;chr&gt; ## 1 1.11e7 23 B M 01/01/2011 00&#39;00&quot; &lt;NA&gt; ## 2 1.11e7 46 B M 01/01/2011 01&#39;00&quot; 2800 Mayfield… ## 3 1.11e7 53 B M 01/01/2011 15&#39;00&quot; 3300 Woodland… ## # … with 8 more variables: incidentOffense &lt;chr&gt;, incidentLocation &lt;chr&gt;, ## # charge &lt;chr&gt;, chargeDescription &lt;chr&gt;, district &lt;chr&gt;, post &lt;dbl&gt;, ## # neighborhood &lt;chr&gt;, `Location 1` &lt;chr&gt; As before, the first argument is the data frame to operate on. The second argument is a vector of indices. We used the c function (for concatenate) to create a vector of indices. We can also use the range operator here: slice(arrest_tab, 1:5) ## # A tibble: 5 x 15 ## arrest age race sex arrestDate arrestTime arrestLocation ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;time&gt; &lt;chr&gt; ## 1 1.11e7 23 B M 01/01/2011 00&#39;00&quot; &lt;NA&gt; ## 2 1.11e7 37 B M 01/01/2011 01&#39;00&quot; 2000 Wilkens … ## 3 1.11e7 46 B M 01/01/2011 01&#39;00&quot; 2800 Mayfield… ## 4 1.11e7 50 B M 01/01/2011 04&#39;00&quot; 2100 Ashburto… ## 5 1.11e7 33 B M 01/01/2011 05&#39;00&quot; 4000 Wilsby A… ## # … with 8 more variables: incidentOffense &lt;chr&gt;, incidentLocation &lt;chr&gt;, ## # charge &lt;chr&gt;, chargeDescription &lt;chr&gt;, district &lt;chr&gt;, post &lt;dbl&gt;, ## # neighborhood &lt;chr&gt;, `Location 1` &lt;chr&gt; To create general sequences of indices we would use the seq function. For example, to select entities in even positions we would use the following: slice(arrest_tab, seq(2, nrow(arrest_tab), by=2)) ## # A tibble: 52,264 x 15 ## arrest age race sex arrestDate arrestTime arrestLocation ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;time&gt; &lt;chr&gt; ## 1 1.11e7 37 B M 01/01/2011 01&#39;00&quot; 2000 Wilkens … ## 2 1.11e7 50 B M 01/01/2011 04&#39;00&quot; 2100 Ashburto… ## 3 1.11e7 41 B M 01/01/2011 05&#39;00&quot; 2900 Spellman… ## 4 1.11e7 20 W M 01/01/2011 05&#39;00&quot; 5200 Moravia … ## 5 1.11e7 53 B M 01/01/2011 15&#39;00&quot; 3300 Woodland… ## 6 1.11e7 25 B M 01/01/2011 20&#39;00&quot; 2800 Violet A… ## 7 1.11e7 50 B M 01/01/2011 40&#39;00&quot; 2600 Oswego A… ## 8 1.11e7 40 B M 01/01/2011 40&#39;00&quot; 3900 Greenmou… ## 9 1.11e7 30 B M 01/01/2011 40&#39;00&quot; 900 N Calhoun… ## 10 1.11e7 53 B M 01/01/2011 40&#39;00&quot; 900 N Calhoun… ## # … with 52,254 more rows, and 8 more variables: incidentOffense &lt;chr&gt;, ## # incidentLocation &lt;chr&gt;, charge &lt;chr&gt;, chargeDescription &lt;chr&gt;, ## # district &lt;chr&gt;, post &lt;dbl&gt;, neighborhood &lt;chr&gt;, `Location 1` &lt;chr&gt; 6.2.2 filter We can also select entities based on attribute properties. For example, to select arrests where age is less than 18 years old, we would use the following: filter(arrest_tab, age &lt; 18) ## # A tibble: 463 x 15 ## arrest age race sex arrestDate arrestTime arrestLocation ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;time&gt; &lt;chr&gt; ## 1 1.11e7 17 B M 01/03/2011 15:00 &lt;NA&gt; ## 2 1.11e7 17 B M 01/07/2011 18:40 500 N Athol St ## 3 1.11e7 17 A M 01/10/2011 22:00 &lt;NA&gt; ## 4 1.11e7 17 B M 01/13/2011 01:00 &lt;NA&gt; ## 5 1.11e7 17 B F 01/13/2011 13:40 1400 N Wilmer… ## 6 1.11e7 17 B M 01/13/2011 18:40 1400 Wilmer St ## 7 1.11e7 14 B M 01/17/2011 21:57 &lt;NA&gt; ## 8 1.11e7 17 B M 01/18/2011 15:00 &lt;NA&gt; ## 9 1.11e7 17 B M 01/18/2011 15:26 900 Seagull A… ## 10 1.11e7 16 B M 01/18/2011 16:00 2300 N Charle… ## # … with 453 more rows, and 8 more variables: incidentOffense &lt;chr&gt;, ## # incidentLocation &lt;chr&gt;, charge &lt;chr&gt;, chargeDescription &lt;chr&gt;, ## # district &lt;chr&gt;, post &lt;dbl&gt;, neighborhood &lt;chr&gt;, `Location 1` &lt;chr&gt; You know by now what the first argument is… The second argument is an expression that evaluates to a logical value (TRUE or FALSE), if the expression evaluates to TRUE for a given entity (row) then that entity (row) is part of the resulting data frame. Operators used frequently include: ==, !=: tests equality and inequality respectively (categorical, numerical, datetimes, etc.) &lt;, &gt;, &lt;=, &gt;=: tests order relationships for ordered data types (not categorical) !, &amp;, |: not, and, or, logical operators To select arrests with ages between 18 and 25 we can use filter(arrest_tab, age &gt;= 18 &amp; age &lt;= 25) ## # A tibble: 35,770 x 15 ## arrest age race sex arrestDate arrestTime arrestLocation ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;time&gt; &lt;chr&gt; ## 1 1.11e7 23 B M 01/01/2011 00:00 &lt;NA&gt; ## 2 1.11e7 20 W M 01/01/2011 00:05 5200 Moravia … ## 3 1.11e7 24 B M 01/01/2011 00:07 2400 Gainsdbo… ## 4 1.11e7 25 B M 01/01/2011 00:20 2800 Violet A… ## 5 1.11e7 24 B M 01/01/2011 00:40 3900 Greenmou… ## 6 1.11e7 20 B M 01/01/2011 01:22 1900 Ashburto… ## 7 1.11e7 23 B M 01/01/2011 01:30 2600 Aisquith… ## 8 1.11e7 22 A M 01/01/2011 01:40 &lt;NA&gt; ## 9 1.11e7 20 W M 01/01/2011 02:00 300 S Bentalo… ## 10 1.11e7 20 B M 01/01/2011 02:20 900 Myrtle Ave ## # … with 35,760 more rows, and 8 more variables: incidentOffense &lt;chr&gt;, ## # incidentLocation &lt;chr&gt;, charge &lt;chr&gt;, chargeDescription &lt;chr&gt;, ## # district &lt;chr&gt;, post &lt;dbl&gt;, neighborhood &lt;chr&gt;, `Location 1` &lt;chr&gt; The filter function can take multiple logical expressions. In this case they are combined with &amp;. So the above is equivalent to filter(arrest_tab, age &gt;= 18, age &lt;= 25) ## # A tibble: 35,770 x 15 ## arrest age race sex arrestDate arrestTime arrestLocation ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;time&gt; &lt;chr&gt; ## 1 1.11e7 23 B M 01/01/2011 00:00 &lt;NA&gt; ## 2 1.11e7 20 W M 01/01/2011 00:05 5200 Moravia … ## 3 1.11e7 24 B M 01/01/2011 00:07 2400 Gainsdbo… ## 4 1.11e7 25 B M 01/01/2011 00:20 2800 Violet A… ## 5 1.11e7 24 B M 01/01/2011 00:40 3900 Greenmou… ## 6 1.11e7 20 B M 01/01/2011 01:22 1900 Ashburto… ## 7 1.11e7 23 B M 01/01/2011 01:30 2600 Aisquith… ## 8 1.11e7 22 A M 01/01/2011 01:40 &lt;NA&gt; ## 9 1.11e7 20 W M 01/01/2011 02:00 300 S Bentalo… ## 10 1.11e7 20 B M 01/01/2011 02:20 900 Myrtle Ave ## # … with 35,760 more rows, and 8 more variables: incidentOffense &lt;chr&gt;, ## # incidentLocation &lt;chr&gt;, charge &lt;chr&gt;, chargeDescription &lt;chr&gt;, ## # district &lt;chr&gt;, post &lt;dbl&gt;, neighborhood &lt;chr&gt;, `Location 1` &lt;chr&gt; 6.2.3 sample_n and sample_frac Frequently we will want to choose entities from a data frame at random. The sample_n function selects a specific number of entities at random: sample_n(arrest_tab, 10) ## # A tibble: 10 x 15 ## arrest age race sex arrestDate arrestTime arrestLocation ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;time&gt; &lt;chr&gt; ## 1 NA 29 B M 05/13/2011 16:00 &lt;NA&gt; ## 2 1.13e7 22 B M 07/25/2011 18:41 1300 Sherwood… ## 3 1.13e7 18 B F 08/05/2011 16:40 600 Willow Ave ## 4 1.24e7 44 W M 02/28/2012 01:30 &lt;NA&gt; ## 5 1.25e7 19 B M 08/25/2012 00:05 400 Park Ave ## 6 1.14e7 34 W M 12/11/2011 08:30 200 Smallwood… ## 7 1.12e7 40 B M 04/15/2011 13:30 3000 Mallview… ## 8 1.25e7 23 B M 05/03/2012 00:41 &lt;NA&gt; ## 9 1.13e7 18 B M 08/02/2011 10:15 2300 E Presto… ## 10 1.12e7 21 B M 03/29/2011 11:40 &lt;NA&gt; ## # … with 8 more variables: incidentOffense &lt;chr&gt;, incidentLocation &lt;chr&gt;, ## # charge &lt;chr&gt;, chargeDescription &lt;chr&gt;, district &lt;chr&gt;, post &lt;dbl&gt;, ## # neighborhood &lt;chr&gt;, `Location 1` &lt;chr&gt; The sample_frac function selects a fraction of entitites at random: sample_frac(arrest_tab, .1) ## # A tibble: 10,453 x 15 ## arrest age race sex arrestDate arrestTime arrestLocation ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;time&gt; &lt;chr&gt; ## 1 1.12e7 23 B F 06/05/2011 12:00 4027- Boarman… ## 2 1.12e7 30 B M 02/16/2011 09:00 &lt;NA&gt; ## 3 NA 23 B M 02/25/2011 02:00 1900 N Charle… ## 4 1.26e7 35 W F 12/12/2012 09:58 100 S Conklin… ## 5 1.25e7 57 W M 08/08/2012 08:08 &lt;NA&gt; ## 6 NA 22 W M 05/27/2011 17:28 &lt;NA&gt; ## 7 1.11e7 23 B M 01/18/2011 16:00 &lt;NA&gt; ## 8 1.26e7 39 B M 11/27/2012 10:20 1400 N Dallas… ## 9 1.12e7 50 B M 05/25/2011 01:31 4700 Chatford… ## 10 1.13e7 19 B F 07/14/2011 20:20 &lt;NA&gt; ## # … with 10,443 more rows, and 8 more variables: incidentOffense &lt;chr&gt;, ## # incidentLocation &lt;chr&gt;, charge &lt;chr&gt;, chargeDescription &lt;chr&gt;, ## # district &lt;chr&gt;, post &lt;dbl&gt;, neighborhood &lt;chr&gt;, `Location 1` &lt;chr&gt; 6.3 Pipelines of operations All of the functions implementing our first set of operations have the same argument/value structure. They take a data frame as a first argument and return a data frame. We refer to this as the data–&gt;transform–&gt;data pattern. This is the core a lot of what we will do in class as part of data analyses. Specifically, we will combine operations into pipelines that manipulate data frames. The dplyr package introduces syntactic sugar to make this pattern explicit. For instance, we can rewrite the sample_frac example using the “pipe” operator %&gt;%: arrest_tab %&gt;% sample_frac(.1) ## # A tibble: 10,453 x 15 ## arrest age race sex arrestDate arrestTime arrestLocation ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;time&gt; &lt;chr&gt; ## 1 1.12e7 23 B M 05/02/2011 15:30 &lt;NA&gt; ## 2 1.14e7 40 B M 12/06/2011 16:05 1900 Ramsey St ## 3 1.24e7 35 B M 03/28/2012 09:30 1300 Greenmou… ## 4 1.13e7 20 B F 07/18/2011 20:11 3600 Erdman A… ## 5 1.13e7 20 B M 10/03/2011 18:00 4500 Park Hei… ## 6 1.13e7 48 B M 09/05/2011 00:15 &lt;NA&gt; ## 7 1.26e7 48 B M 11/19/2012 21:20 200 Amity St ## 8 1.13e7 25 B M 08/18/2011 19:30 800 E Preston… ## 9 1.14e7 34 B M 11/01/2011 18:00 2100 W Pratt … ## 10 1.13e7 32 B M 08/26/2011 12:00 &lt;NA&gt; ## # … with 10,443 more rows, and 8 more variables: incidentOffense &lt;chr&gt;, ## # incidentLocation &lt;chr&gt;, charge &lt;chr&gt;, chargeDescription &lt;chr&gt;, ## # district &lt;chr&gt;, post &lt;dbl&gt;, neighborhood &lt;chr&gt;, `Location 1` &lt;chr&gt; The %&gt;% binary operator takes the value to its left and inserts it as the first argument of the function call to its right. So the expression LHS %&gt;% f(another_argument) is equivalent to the expression f(LHS, another_argument). Using the %&gt;% operator and the data–&gt;transform–&gt;data pattern of the functions we’ve seen so far, we can create pipelines. For example, let’s create a pipeline that: filters our dataset to arrests between the ages of 18 and 25 selects attributes sex, district and arrestDate (renamed as arrest_date) samples 50% of those arrests at random We will assign the result to variable analysis_tab analysis_tab &lt;- arrest_tab %&gt;% filter(age &gt;= 18, age &lt;= 25) %&gt;% select(sex, district, arrest_date=arrestDate) %&gt;% sample_frac(.5) analysis_tab ## # A tibble: 17,885 x 3 ## sex district arrest_date ## &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; ## 1 M NORTHEASTERN 06/20/2011 ## 2 M &lt;NA&gt; 08/23/2012 ## 3 M SOUTHERN 07/03/2011 ## 4 M SOUTHERN 07/10/2012 ## 5 M WESTERN 09/15/2011 ## 6 F EASTERN 06/25/2011 ## 7 M EASTERN 10/30/2012 ## 8 M NORTHWESTERN 04/01/2011 ## 9 M NORTHEASTERN 10/03/2012 ## 10 M &lt;NA&gt; 07/20/2011 ## # … with 17,875 more rows "],
["principles-more-operations.html", "7 Principles: More Operations 7.1 Operations that sort entities 7.2 Operations that create new attributes 7.3 Operations that summarize attribute values over entities 7.4 Operations that group entities 7.5 Vectors 7.6 Attributes as vectors 7.7 Functions", " 7 Principles: More Operations In the previous section we introduced our first few operations to manipulate data frames. Next, we learn a few more: sorting, creating new attributes, summarizing and grouping. Finally we will take a short detour through a discussion on vectors. 7.1 Operations that sort entities The first operation we will look at today is used to sort entities based on their attribute values. As an example, suppose we wanted to find the arrests with the 10 youngest subjects. If we had an operation that re-orders entities based on the value of their age attribute, we can then use the slice operation we saw before to create a data frame with just the entities of interest arrest_tab %&gt;% arrange(age) %&gt;% slice(1:10) ## # A tibble: 10 x 15 ## arrest age race sex arrestDate arrestTime arrestLocation ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;time&gt; &lt;chr&gt; ## 1 1.11e7 0 B F 01/24/2011 12:45 3700 Garrison… ## 2 1.12e7 0 W M 03/22/2011 08:00 &lt;NA&gt; ## 3 NA 0 &lt;NA&gt; &lt;NA&gt; 03/28/2011 15:25 3600 Townsend… ## 4 NA 0 B M 03/30/2011 16:00 &lt;NA&gt; ## 5 NA 0 W F 04/07/2011 20:30 &lt;NA&gt; ## 6 1.12e7 0 W F 05/20/2011 02:00 &lt;NA&gt; ## 7 1.12e7 0 B M 06/21/2011 21:15 2700 Harford … ## 8 1.13e7 0 B M 09/04/2011 10:30 &lt;NA&gt; ## 9 1.13e7 0 B M 09/28/2011 21:16 500 E Lexingt… ## 10 1.14e7 0 &lt;NA&gt; &lt;NA&gt; 12/02/2011 19:20 4600 Park Hei… ## # … with 8 more variables: incidentOffense &lt;chr&gt;, incidentLocation &lt;chr&gt;, ## # charge &lt;chr&gt;, chargeDescription &lt;chr&gt;, district &lt;chr&gt;, post &lt;dbl&gt;, ## # neighborhood &lt;chr&gt;, `Location 1` &lt;chr&gt; The arrange operation sorts entities by increasing value of the attributes passed as arguments. The desc helper function is used to indicate sorting by decreasing value. For example, to find the arrests with the 10 oldest subjects we would use: arrest_tab %&gt;% arrange(desc(age)) %&gt;% slice(1:10) ## # A tibble: 10 x 15 ## arrest age race sex arrestDate arrestTime arrestLocation ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;time&gt; &lt;chr&gt; ## 1 1.13e7 87 B M 08/28/2011 15:00 3200 E Baltim… ## 2 1.25e7 87 B M 08/14/2012 07:25 &lt;NA&gt; ## 3 1.12e7 86 B M 02/22/2011 15:15 700 W Saratog… ## 4 1.11e7 85 W M 01/25/2011 08:05 3100 Pulaski … ## 5 1.12e7 85 B M 03/04/2011 09:23 2000 Ellswort… ## 6 1.14e7 84 B M 12/27/2011 14:15 2000 W Pratt … ## 7 NA 84 B M 01/11/2012 11:00 &lt;NA&gt; ## 8 1.26e7 84 B M 11/08/2012 06:40 &lt;NA&gt; ## 9 1.12e7 82 B M 05/17/2011 19:50 1300 Mchenry … ## 10 1.13e7 80 B M 09/26/2011 10:15 3000 Harlem A… ## # … with 8 more variables: incidentOffense &lt;chr&gt;, incidentLocation &lt;chr&gt;, ## # charge &lt;chr&gt;, chargeDescription &lt;chr&gt;, district &lt;chr&gt;, post &lt;dbl&gt;, ## # neighborhood &lt;chr&gt;, `Location 1` &lt;chr&gt; 7.2 Operations that create new attributes We will often see that for many analyses, be it for interpretation or for statistical modeling, we will create new attributes based on existing attributes in a dataset. Suppose I want to represent age in months rather than years in our dataset. To do so I would multiply 12 to the existing age attribute. The function mutate creates new attributes based on the result of a given expression: arrest_tab %&gt;% mutate(age_months = 12 * age) %&gt;% select(arrest, age, age_months) ## # A tibble: 104,528 x 3 ## arrest age age_months ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 11126858 23 276 ## 2 11127013 37 444 ## 3 11126887 46 552 ## 4 11126873 50 600 ## 5 11126968 33 396 ## 6 11127041 41 492 ## 7 11126932 29 348 ## 8 11126940 20 240 ## 9 11127051 24 288 ## 10 11127018 53 636 ## # … with 104,518 more rows 7.3 Operations that summarize attribute values over entities Once we have a set of entities and attributes in a given data frame, we may need to summarize attribute values over the set of entities in the data frame. It collapses the data frame to a single row containing the desired attribute summaries. Continuing with the example we have seen below, we may want to know what the minmum, maximum and average age in the dataset is: summarize(arrest_tab, min_age=min(age), mean_age=mean(age), max_age=max(age)) ## # A tibble: 1 x 3 ## min_age mean_age max_age ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 33.2 87 The summarize functions takes a data frame and calls a summary function over attributes of the data frame. Common summary functions to use include: Operation(s) Result mean, median average and median attribute value, respectively sd standard deviation of attribute values min, max minimum and maximum attribute values, respectively n, n_distinct number of attribute values and number of distinct attribute values any, all for logical attributes (TRUE/FALSE): is any attribute value TRUE, or are all attribute values TRUE Let’s see the number of distinct districts in our dataset: summarize(arrest_tab, n_distinct(district)) ## # A tibble: 1 x 1 ## `n_distinct(district)` ## &lt;int&gt; ## 1 10 We may also refer to these summarization operation as aggregation since we are computing aggregates of attribute values. 7.4 Operations that group entities Summarization (therefore aggregation) goes hand in hand with data grouping, where summaries are computed conditioned on other attributes. The notion of conditioning is fundamental to data analysis and we will see it very frequently through the course. It is the basis of statistical analysis and Machine Learning models and it is essential in understanding the design of effective visualizations. The goal is to group entities with the same value of one or more attributes. The group_by function in essence annotates the rows of a data frame as belonging to a specific group based on the value of some chosen attributes. This call returns a data frame that is grouped by the value of the district attribute. group_by(arrest_tab, district) ## # A tibble: 104,528 x 15 ## # Groups: district [10] ## arrest age race sex arrestDate arrestTime arrestLocation ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;time&gt; &lt;chr&gt; ## 1 1.11e7 23 B M 01/01/2011 00&#39;00&quot; &lt;NA&gt; ## 2 1.11e7 37 B M 01/01/2011 01&#39;00&quot; 2000 Wilkens … ## 3 1.11e7 46 B M 01/01/2011 01&#39;00&quot; 2800 Mayfield… ## 4 1.11e7 50 B M 01/01/2011 04&#39;00&quot; 2100 Ashburto… ## 5 1.11e7 33 B M 01/01/2011 05&#39;00&quot; 4000 Wilsby A… ## 6 1.11e7 41 B M 01/01/2011 05&#39;00&quot; 2900 Spellman… ## 7 1.11e7 29 B M 01/01/2011 05&#39;00&quot; 800 N Monroe … ## 8 1.11e7 20 W M 01/01/2011 05&#39;00&quot; 5200 Moravia … ## 9 1.11e7 24 B M 01/01/2011 07&#39;00&quot; 2400 Gainsdbo… ## 10 1.11e7 53 B M 01/01/2011 15&#39;00&quot; 3300 Woodland… ## # … with 104,518 more rows, and 8 more variables: incidentOffense &lt;chr&gt;, ## # incidentLocation &lt;chr&gt;, charge &lt;chr&gt;, chargeDescription &lt;chr&gt;, ## # district &lt;chr&gt;, post &lt;dbl&gt;, neighborhood &lt;chr&gt;, `Location 1` &lt;chr&gt; Subsequent operations are then performed for each group independently. For example, when summarize is applied to a grouped data frame, summaries are computed for each group of entities, rather than the whole set of entities. For instance, let’s calculate minimum, maximum and average age for each district in our dataset: arrest_tab %&gt;% group_by(district) %&gt;% summarize(min_age=min(age), max_age=max(age), mean_age=mean(age)) ## # A tibble: 10 x 4 ## district min_age max_age mean_age ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 CENTRAL 0 86 33.0 ## 2 EASTERN 0 85 34.1 ## 3 NORTHEASTERN 0 78 30.4 ## 4 NORTHERN 14 80 33.1 ## 5 NORTHWESTERN 0 78 34.6 ## 6 SOUTHEASTERN 0 87 32.5 ## 7 SOUTHERN 0 84 32.3 ## 8 SOUTHWESTERN 0 80 32.4 ## 9 WESTERN 0 73 34.4 ## 10 &lt;NA&gt; 0 87 33.4 Note that after this operation we have effectively changed the entities represented in the result. The entities in our original dataset are arrests while the entities for the result of the last example are the districts. This is a general property of group_by and summarize: it defines a data set where entities are defined by distinct values of the attributes we use for grouping. Let’s look at another example combining some of the operations we have seen so far. Let’s compute the average age for subjects 21 years or older grouped by district and sex: arrest_tab %&gt;% filter(age &gt;= 21) %&gt;% group_by(district, sex) %&gt;% summarize(mean_age=mean(age)) ## # A tibble: 20 x 3 ## # Groups: district [?] ## district sex mean_age ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 CENTRAL F 35.7 ## 2 CENTRAL M 35.3 ## 3 EASTERN F 36.9 ## 4 EASTERN M 37.1 ## 5 NORTHEASTERN F 33.5 ## 6 NORTHEASTERN M 32.8 ## 7 NORTHERN F 35.9 ## 8 NORTHERN M 35.6 ## 9 NORTHWESTERN F 37.5 ## 10 NORTHWESTERN M 37.2 ## 11 SOUTHEASTERN F 33.3 ## 12 SOUTHEASTERN M 34.7 ## 13 SOUTHERN F 33.7 ## 14 SOUTHERN M 34.5 ## 15 SOUTHWESTERN F 35.4 ## 16 SOUTHWESTERN M 35.0 ## 17 WESTERN F 37.1 ## 18 WESTERN M 37.3 ## 19 &lt;NA&gt; F 34.7 ## 20 &lt;NA&gt; M 35.5 7.5 Vectors We briefly saw previously operators to create vectors in R. For instance, we can use seq to create a vector that consists of a sequence of integers: multiples_of_three &lt;- seq(3, 30, by=3) multiples_of_three ## [1] 3 6 9 12 15 18 21 24 27 30 Let’s how this is represented in R (the str is very handy to do this type of digging around): str(multiples_of_three) ## num [1:10] 3 6 9 12 15 18 21 24 27 30 So, this is a numeric vector of length 10. Like many other languages we use square brackets [] to index vectors: multiples_of_three[1] ## [1] 3 We can use ranges as before multiples_of_three[1:4] ## [1] 3 6 9 12 We can use vectors of non-negative integers for indexing: multiples_of_three[c(1,3,5)] ## [1] 3 9 15 Or even logical vectors: multiples_of_three[c(TRUE, FALSE, TRUE, FALSE, TRUE, FALSE, TRUE, FALSE, TRUE, FALSE)] ## [1] 3 9 15 21 27 In R, most operations are designed to work with vectors directly (we call that vectorized). For example, if I want to add two vectors together I would write: (look no for loop!): multiples_of_three + multiples_of_three ## [1] 6 12 18 24 30 36 42 48 54 60 This also works for other arithmetic and logical operations (e.g., -, *, /, &amp;, |). Give them a try! In data analysis the vector is probably the most fundamental data type (other than basic numbers, strings, etc.). Why? Consider getting data about one attribute, say height, for a group of people. What do you get? An array of numbers, all in the same unit (say feet, inches or centimeters). How about their name? Then you get an array of strings. Abstractly, we think of vectors as arrays of values, all of the same class or datatype. 7.6 Attributes as vectors In fact, in the data frames we have been working on, each column, corresponding to an attribute, is a vector. We use the pull function to extract a vector from a data frame. We can then operate index them, or operate on them as vectors age_vec &lt;- arrest_tab %&gt;% pull(age) age_vec[1:10] ## [1] 23 37 46 50 33 41 29 20 24 53 12 * age_vec[1:10] ## [1] 276 444 552 600 396 492 348 240 288 636 We previously saw how the $ operator serves the same function. age_vec &lt;- arrest_tab$age age_vec[1:10] ## [1] 23 37 46 50 33 41 29 20 24 53 The pull function however, can be used as part of a pipeline (using operator %&gt;%): arrest_tab %&gt;% pull(age) %&gt;% mean() ## [1] 33.19639 7.7 Functions Once we have established useful pipelines for a dataset we will want to abstract them into reusable functions that we can apply in other analyses. To do that we would write our own functions that encapsulate the pipelines we have created. As an example, take a function that executes the age by district/sex summarization we created before: summarize_district &lt;- function(df) { df %&gt;% filter(age &gt;= 21) %&gt;% group_by(district, sex) %&gt;% summarize(mean_age=mean(age)) } You can include multiple expressions in the function definition (with the brackts []). Notice there is no return statement in this function. When a function is called, it returns the value of the last expression in the function definition. In this example, it would be the data frame we get from applying the pipeline of operations. You can find more information about vectors, functions and other programming matters we might run into in class in Chapters 17-21 of R for Data Science "],
["basic-plotting-with-ggplot.html", "8 Basic plotting with ggplot 8.1 Plot Construction Details 8.2 Frequently Used Plots", " 8 Basic plotting with ggplot We will spend a good amount of time in the course discussing data visualization. It serves many important roles in data analysis. We use it to gain understanding of dataset characteristics throughout analyses and it is a key element of communicating insights we have derived from data analyses with our target audience. In this section, we will introduce basic functionality of the ggplot package to start our discussion of visualization throughout the course. The ggplot package is designed to work well with the tidyverse set of packages. As such, it is designed around the Entity-Attribute data model. Also, it can be included as part of data frame operation pipelines. Let’s start with a simple example. Let’s create a dot plot of the number of arrests per district in our dataset: arrest_tab %&gt;% group_by(district) %&gt;% summarize(num_arrests=n()) %&gt;% ggplot(mapping=aes(y=district, x=num_arrests)) + geom_point() The ggplot design is very elegant, takes some thinking to get used to, but is extremely powerful. The central premise is to characterize the building pieces behind ggplot plots as follows: The data that goes into a plot, a data frame of entities and attributes The mapping between data attributes and graphical (aesthetic) characteristics The geometric representation of these graphical characteristics So in our example we can fill in these three parts as follows: Data: We pass a data frame to the ggplot function with the %&gt;% operator at the end of the group_by-summarize pipeline. Mapping: Here we map the num_arrests attribute to the x position in the plot and the district attribute to the y position in the plot. Every ggplot will contain one or more aes calls. Geometry: Here we choose points as the geometric representations of our chosen graphical characteristics using the geom_point function. In general, the ggplot call will have the following structure: &lt;data_frame&gt; %&gt;% ggplot(mapping=aes(&lt;graphical_characteristic&gt;=&lt;attribute&gt;)) + geom_&lt;representation&gt;() 8.1 Plot Construction Details 8.1.1 Mappings Some of the graphical characteristics we will commonly map attributes to include: Argument Definition x position along x axis y position along y axis color color shape shape (applicable to e.g., points) size size label string used as label (applicable to text) 8.1.2 Representations Representations we will use frequently are Function Representation geom_point points geom_bar rectangles geom_text strings geom_smooth smoothed line (advanced) geom_hex hexagonal binning We can include multiple geometric representations in a single plot, for example points and text, by adding (+) multiple geom_&lt;representation&gt; functions. Also, we can include mappings inside a geom_ call to map characteristics to attributes strictly for that specific representation. For example geom_point(mapping=aes(color=&lt;attribute&gt;)) maps color to some attribute only for the point representation specified by that call. Mappings given in the ggplot call apply to all representations added to the plot. This cheat sheet is very handy: https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf 8.2 Frequently Used Plots We will look comprehensively at data visualization in more detail later in the course, but for now will list a few common plots we use in data analysis and how they are created using ggplot. Let’s switch data frame to the mpg dataset for our examples: mpg ## # A tibble: 234 x 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 audi a4 1.8 1999 4 auto… f 18 29 p comp… ## 2 audi a4 1.8 1999 4 manu… f 21 29 p comp… ## 3 audi a4 2 2008 4 manu… f 20 31 p comp… ## 4 audi a4 2 2008 4 auto… f 21 30 p comp… ## 5 audi a4 2.8 1999 6 auto… f 16 26 p comp… ## 6 audi a4 2.8 1999 6 manu… f 18 26 p comp… ## 7 audi a4 3.1 2008 6 auto… f 18 27 p comp… ## 8 audi a4 q… 1.8 1999 4 manu… 4 18 26 p comp… ## 9 audi a4 q… 1.8 1999 4 auto… 4 16 25 p comp… ## 10 audi a4 q… 2 2008 4 manu… 4 20 28 p comp… ## # … with 224 more rows 8.2.1 Scatter plot Used to visualize the relationship between two attributes. mpg %&gt;% ggplot(mapping=aes(x=displ, y=hwy)) + geom_point(mapping=aes(color=cyl)) 8.2.2 Bar graph Used to visualize the relationship between a continuous variable to a categorical (or discrete) attribute mpg %&gt;% group_by(cyl) %&gt;% summarize(mean_mpg=mean(hwy)) %&gt;% ggplot(mapping=aes(x=cyl, y=mean_mpg)) + geom_bar(stat=&quot;identity&quot;) 8.2.3 Histogram Used to visualize the distribution of the values of a numeric attribute mpg %&gt;% ggplot(mapping=aes(x=hwy)) + geom_histogram() 8.2.4 Boxplot Used to visualize the distribution of a numeric attribute based on a categorical attribute mpg %&gt;% ggplot(mapping=aes(x=class, y=hwy)) + geom_boxplot() "],
["brief-introduction-to-rmarkdown.html", "9 Brief Introduction to Rmarkdown", " 9 Brief Introduction to Rmarkdown Rstudio has created an impressive eco-system for publishing with R. It has concentrated around two systems: Rmarkdown for creating documents that include both text and data analysis code, publishable in multiple formats, and shiny, a framework for creating interactive html applications that allow users to explore data analyses. Rstudio has provided substantial tutorials and documentation for Rmarkdown: http://rmarkdown.rstudio.com/ The tutorial included in that website is quite good: http://rmarkdown.rstudio.com/lesson-1.html. In class we will go over some of the key points using the same Rmarkdown file as an example: http://rmarkdown.rstudio.com/demos/1-example.Rmd "],
["best-practices-for-data-science-projects.html", "10 Best Practices for Data Science Projects", " 10 Best Practices for Data Science Projects See this slidedeck for discussion of reproducibility, bias, ethics and responsibility along with some practical tips: Best Practices Slidedeck "],
["tidy-data-i-the-er-model.html", "11 Tidy Data I: The ER Model 11.1 Overview 11.2 The Entity-Relationship and Relational Models 11.3 Tidy Data", " 11 Tidy Data I: The ER Model Some of this material is based on Amol Deshpande’s material: https://github.com/umddb/datascience-fall14/blob/master/lecture-notes/models.md 11.1 Overview In this section we will discuss principles of preparing and organizing data in a way that is amenable for analysis, both in modeling and visualization. We think of a data model as a collection of concepts that describes how data is represented and accessed. Thinking abstractly of data structure, beyond a specific implementation, makes it easier to share data across programs and systems, and integrate data from different sources. Once we have thought about structure, we can then think about semantics: what does data represent? Structure: We have assumed that data is organized in rectangular data structures (tables with rows and columns) Semantics: We have discussed the notion of values, attributes, and entities. So far, we have used the following data semantics: a dataset is a collection of values, numeric or categorical, organized into entities (observations) and attributes (variables). Each attribute contains values of a specific measurement across entities, and entities collect all measurements across attributes. In the database literature, we call this exercise of defining structure and semantics as data modeling. In this course we use the term data representational modeling, to distinguish from data statistical modeling. The context should be sufficient to distinguish the two uses of the term data modeling. Data representational modeling is the process of representing/capturing structure in data based on defining: Data model: A collection of concepts that describes how data is represented and accessed Schema: A description of a specific collection of data, using a given data model The purpose of defining abstract data representation models is that it allows us to know the structure of the data/information (to some extent) and thus be able to write general purpose code. Lack of a data model makes it difficult to share data across programs, organizations, systems that need to be able to integrate information from multiple sources. We can also design algorithms and code that can significantly increase efficiency if we can assume general data structure. For instance, we can preprocess data to make access efficient (e.g., building a B-Tree on a field). A data model typically consists of: Modeling Constructs: A collection of concepts used to represent the structure in the data. Typically we need to represent types of entities, their attributes, types of relationships between entities, and relationship attributes Integrity Constraints: Constraints to ensure data integrity (i.e., avoid errors) Manipulation Languages: Constructs for manipulating the data We desire that models are sufficiently expressive so they can capture real-world data well, easy to use, and lend themselves to defining computational methods that have good performance. Some examples of data models are Relational, Entity-relationship model, XML… Object-oriented, Object-relational, RDF… Current favorites in the industry: JSON, Protocol Buffers, Avro, Thrift, Property Graph Why have so many models been defined? There is an inherent tension between descriptive power and ease of use/efficiency. More powerful, expressive, models can be applied to represent more datasets but also tend to be harder to use and query efficiently. Typically there are multiple levels of modeling. Physical modeling concerns itself with how the data is physically stored. Logical or Conceptual modeling concerns itself with type of information stored, the different entities, their attributes, and the relationships among those. There may be several layers of logical/conceptual models to restrict the information flow (for security and/or ease-of-use): Data independence: The idea that you can change the representation of data w/o changing programs that operate on it. Physical data independence: I can change the layout of data on disk and my programs won’t change index the data partition/distribute/replicate the data compress the data sort the data 11.2 The Entity-Relationship and Relational Models The fundamental objects in this formalism are entities and their attributes, as we have seen before, and relationships and relationship attributes. Entities are objects represented in a dataset: people, places, things, etc. Relationships model just that, relationships between entities. Here, rectangles are entitites, diamonds and edges indicate relationships. Circles describe either entity or relationship attributes. Arrows are used indicate multiplicity of relationships (one-to-one, many-to-one, one-to-many, many-to-many): Relationships are defined over pairs of entities. As such, relationship \\(R\\) over sets of entities \\(E_1\\) and \\(E_2\\) is defined over the cartesian product \\(E_1 \\times E_2\\). For example, if \\(e_1 \\in E_1\\) and \\(e_2 \\in E_2\\), then \\((e_1, e_2) \\in R\\). Arrows specify how entities participate in relationships. In particular, an arrow pointing from an entity set \\(E_1\\) (square) into a relationship over \\(E_1\\) and \\(E_2\\) (diamond) specifies that entities in \\(E_1\\) appear in only one relationship pair. That is, there is a single entity \\(e_2 \\in E_2\\) such that \\((e_1,e_2) \\in R\\). Think about what relationships are shown in this diagram? In databases and general datasets we work on, both Entities and Relationships are represented as Relations (tables) such that a unique entity/relationship is represented by a single tuple (the list of attribute values that represent an entity or relationship). This leads to the natural question of how are unique entities determined or defined. Here is where the concept of a key comes in. This is an essential aspect of the Entity-Relationship and Relational models. 11.2.1 Formal introduction to keys Attribute set \\(K\\) is a superkey of relation \\(R\\) if values for \\(K\\) are sufficient to identify a unique tuple of each possible relation \\(r(R)\\) Example: {ID} and {ID,name} are both superkeys of instructor Superkey \\(K\\) is a candidate key if \\(K\\) is minimal Example: {ID} is a candidate key for Instructor One of the candidate keys is selected to be the primary key Typically one that is small and immutable (doesn’t change often) Primary key typically highlighted Foreign key: Primary key of a relation that appears in another relation {ID} from student appears in takes, advisor student called referenced relation takes is the referencing relation Typically shown by an arrow from referencing to referenced Foreign key constraint: the tuple corresponding to that primary key must exist Imagine: Tuple: ('student101', 'CMSC302')in takes But no tuple corresponding to ‘student101’ in student Also called referential integrity constraint 11.2.1.1 Keys: Examples Married(person1-ssn, person2-ssn, date-married, date-divorced) Account(cust-ssn, account-number, cust-name, balance, cust-address) RA(student-id, project-id, superviser-id, appt-time, appt-start-date, appt-end-date) Person(Name, DOB, Born, Education, Religion, …) Information typically found on Wikipedia Pages President(name, start-date, end-date, vice-president, preceded-by, succeeded-by) Info listed on Wikipedia page summary Rider(Name, Born, Team-name, Coach, Sponsor, Year) Tour de France: Historical Rider Participation Information 11.3 Tidy Data Later in the course we will use the term Tidy Data to refer to datasets that are represented in a form that is amenable for manipulation and statistical modeling. It is very closely related to the concept of normal forms in the ER model and the process of normalization in the database literature. Here we assume we are working in the ER data model represented as relations: rectangular data structures where Each attribute (or variable) forms a column Each entity (or observation) forms a row Each type of entity (observational unit) forms a table Here is an example of a tidy dataset: library(nycflights13) head(flights) ## # A tibble: 6 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 544 545 -1 1004 ## 5 2013 1 1 554 600 -6 812 ## 6 2013 1 1 554 558 -4 740 ## # … with 12 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, ## # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, ## # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, ## # time_hour &lt;dttm&gt; it has one entity per row, a single attribute per column. Notice only information about flights are included here (e.g., no airport or airline information other than the name) in these observations. "],
["sql-i-single-table-queries.html", "12 SQL I: Single Table Queries 12.1 Group-by and summarize 12.2 Subqueries", " 12 SQL I: Single Table Queries The Structured-Query-Language (SQL) is the predominant language used in database systems. It is tailored to the Relational data representation model. SQL is a declarative language, we don’t write a procedure to compute a relation, we declare what the relation we want to compute looks like. The actual execution is determined and optimized by the database engine. However, there are clear mappings between parts of SQL queries and the operations we have defined so far as implemented in the tidyverse. The basic construct in SQL is the so-called SFW construct: select-from-where which specifies: select: which attributes you want the answer to have from: which relation (table) you want the answer to be computed from where: what conditions you want to be satisfied by the rows (tuples) of the answer E.g.: movies produced by disney in 1990: note the rename select m.title, m.year from movie m where m.studioname = &#39;disney&#39; and m.year = 1990 The select clause can contain expressions (this is paralleled by the mutate operation we saw previously) select title || ' (' || to_char(year) || ')' as titleyear select 2014 - year The where clause support a large number of different predicates and combinations thereof (this is parallel to the filter operation) year between 1990 and 1995 title like 'star wars%' title like 'star wars _' We can include ordering, e.g., find distinct movies sorted by title select distinct title from movie where studioname = &#39;disney&#39; and year = 1990 order by title; 12.1 Group-by and summarize SQL has an idiom for grouping and summarizing (conditioning as we called it before). Remember this is a very important concept that shows up in many data processing platforms What it does: Partition the tuples by the group attributes (year in this case), and do something (compute avg in this case) for each group Number of resulting tuples == Number of groups E.g., compute the average movie length by year select name, avg(length) from movie group by year 12.2 Subqueries You can nest queries as an expression in an SFW query. We refer to these “subqueries” as “nested subquery”: E.g., find movie with the maximum length select title, year from movie where movie.length = (select max(length) from movie); E.g., find movies with at least 5 stars: an example of a correlated subquery select * from movies m where 5 &gt;= (select count(*) from starsIn si where si.title = m.title and si.year = m.year); The nested subquery counts the number of actors for that movie. E.g., rank movies by their length. select title, year, (select count(*) from movies m2 where m1.length &lt;= m2.length) as rank from movies m1; Key insight: A movie is ranked 5th if there are exactly 4 movies with longer length. Most database systems support some sort of a rank keyword for doing this. Notice that the above query doesn’t work in presence of ties etc. "],
["two-table-operations.html", "13 Two-table operations 13.1 Left Join 13.2 Right Join 13.3 Inner Join 13.4 Full Join 13.5 Join conditions 13.6 Filtering Joins 13.7 SQL Constructs: Multi-table Queries", " 13 Two-table operations So far we have looked at data operations defined over single tables and data frames. In this section we look at efficient methods to combine data from multiple tables. The fundamental operation here is the join, which is a workhorse of database system design and impementation. The join operation combines rows from two tables to create a new single table, based on matching criteria specified over attributes of each of the two tables. Consider the example of joining the flights and airlines table: library(nycflights13) data(flights) data(airlines) Let’s take a look at the flights table again: flights ## # A tibble: 336,776 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 544 545 -1 1004 ## 5 2013 1 1 554 600 -6 812 ## 6 2013 1 1 554 558 -4 740 ## 7 2013 1 1 555 600 -5 913 ## 8 2013 1 1 557 600 -3 709 ## 9 2013 1 1 557 600 -3 838 ## 10 2013 1 1 558 600 -2 753 ## # … with 336,766 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; And add the airlines table: airlines ## # A tibble: 16 x 2 ## carrier name ## &lt;chr&gt; &lt;chr&gt; ## 1 9E Endeavor Air Inc. ## 2 AA American Airlines Inc. ## 3 AS Alaska Airlines Inc. ## 4 B6 JetBlue Airways ## 5 DL Delta Air Lines Inc. ## 6 EV ExpressJet Airlines Inc. ## 7 F9 Frontier Airlines Inc. ## 8 FL AirTran Airways Corporation ## 9 HA Hawaiian Airlines Inc. ## 10 MQ Envoy Air ## 11 OO SkyWest Airlines Inc. ## 12 UA United Air Lines Inc. ## 13 US US Airways Inc. ## 14 VX Virgin America ## 15 WN Southwest Airlines Co. ## 16 YV Mesa Airlines Inc. Here, we want to add airline information to each flight. We can do so by joining the attributes of the respective airline from the airlines table with the flights table based on the values of attributes flights$carrier and airlines$carrier. Specifically, every row of flights with a specific value for flights$carrier, is joined with the the corresponding row in airlines with the same value for airlines$carrier. We will see four different ways of performing this operation that differ on how non-matching observations are handled. 13.1 Left Join In a left join, all observations on left operand (LHS) are retained: flights %&gt;% left_join(airlines, by=&quot;carrier&quot;) ## # A tibble: 336,776 x 20 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 544 545 -1 1004 ## 5 2013 1 1 554 600 -6 812 ## 6 2013 1 1 554 558 -4 740 ## 7 2013 1 1 555 600 -5 913 ## 8 2013 1 1 557 600 -3 709 ## 9 2013 1 1 557 600 -3 838 ## 10 2013 1 1 558 600 -2 753 ## # … with 336,766 more rows, and 13 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, name &lt;chr&gt; RHS variables for LHS observations with no matching RHS observations are coded as NA. 13.2 Right Join All observations on right operand (RHS) are retained: flights %&gt;% right_join(airlines, by=&quot;carrier&quot;) ## # A tibble: 336,776 x 20 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 810 810 0 1048 ## 2 2013 1 1 1451 1500 -9 1634 ## 3 2013 1 1 1452 1455 -3 1637 ## 4 2013 1 1 1454 1500 -6 1635 ## 5 2013 1 1 1507 1515 -8 1651 ## 6 2013 1 1 1530 1530 0 1650 ## 7 2013 1 1 1546 1540 6 1753 ## 8 2013 1 1 1550 1550 0 1844 ## 9 2013 1 1 1552 1600 -8 1749 ## 10 2013 1 1 1554 1600 -6 1701 ## # … with 336,766 more rows, and 13 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, name &lt;chr&gt; LHS variables for RHS observations with no matching LHS observations are coded as NA. 13.3 Inner Join Only observations matching on both tables are retained flights %&gt;% inner_join(airlines, by=&quot;carrier&quot;) ## # A tibble: 336,776 x 20 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 544 545 -1 1004 ## 5 2013 1 1 554 600 -6 812 ## 6 2013 1 1 554 558 -4 740 ## 7 2013 1 1 555 600 -5 913 ## 8 2013 1 1 557 600 -3 709 ## 9 2013 1 1 557 600 -3 838 ## 10 2013 1 1 558 600 -2 753 ## # … with 336,766 more rows, and 13 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, name &lt;chr&gt; 13.4 Full Join All observations are retained, regardless of matching condition flights %&gt;% full_join(airlines, by=&quot;carrier&quot;) ## # A tibble: 336,776 x 20 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 544 545 -1 1004 ## 5 2013 1 1 554 600 -6 812 ## 6 2013 1 1 554 558 -4 740 ## 7 2013 1 1 555 600 -5 913 ## 8 2013 1 1 557 600 -3 709 ## 9 2013 1 1 557 600 -3 838 ## 10 2013 1 1 558 600 -2 753 ## # … with 336,766 more rows, and 13 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, name &lt;chr&gt; All values coded as NA for non-matching observations as appropriate. 13.5 Join conditions All join operations are based on a matching condition: flights %&gt;% left_join(airlines, by=&quot;carrier&quot;) specifies to join observations where flights$carrier equals airlines$carrier. In this case, where no conditions are specified using the by argument: flights %&gt;% left_join(airlines) a natural join is perfomed. In this case all variables with the same name in both tables are used in join condition. You can also specify join conditions on arbitrary attributes using the by argument. flights %&gt;% left_join(airlines, by=c(&quot;carrier&quot; = &quot;name&quot;)) 13.6 Filtering Joins We’ve just seen mutating joins that create new tables. Filtering joins use join conditions to filter a specific table. flights %&gt;% anti_join(airlines, by=&quot;carrier&quot;) ## # A tibble: 0 x 19 ## # … with 19 variables: year &lt;int&gt;, month &lt;int&gt;, day &lt;int&gt;, dep_time &lt;int&gt;, ## # sched_dep_time &lt;int&gt;, dep_delay &lt;dbl&gt;, arr_time &lt;int&gt;, ## # sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, ## # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, ## # distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Filters the flights table to only include flights from airlines that are not included in the airlines table. 13.7 SQL Constructs: Multi-table Queries Key idea: - Do a join to get an appropriate table - Use the constructs for single-table queries You will get used to doing all at once. For the first part, where we use a join to get an appropriate table, the general SQL construct includes: - The name of the first table to join - The type of join to do - The name of the second table to join - The join condition(s) Examples: select title, year, me.name as producerName from movies m join movieexec me where m.producer = me.id; Consider the query: select title, year, producer, count(starName) from movies join starsIn where title = starsIn.movieTitle and year = starsIn.movieYear group by title, year, producer What about movies with no stars ? Need to use outer joins select title, year, producer, count(starName) from movies left outer join starsIn on title = starsIn.movieTitle and year = starsIn.movieYear group by title, year, producer As we saw before, all tuples from ‘movies’ that have no matches in starsIn are included with NULLs (in dplyr this was NA). So, if a tuple (m1, 1990) has no match in starsIn, we get (m1, 1990, NULL) in the result and the count(starName) works correctly then. Note however that count(*) would not work correctly (NULLs can have unintuitive behavior) In most systems JOIN corresponds to an inner join, and include LEFT JOIN and RIGHT JOIN as well. "],
["sql-system-constructs.html", "14 SQL System Constructs 14.1 SQL as a Data Definition Language 14.2 Set Operations and Comparisons 14.3 Views 14.4 NULLs", " 14 SQL System Constructs Database management systems are software applications designed for very efficient manipulation of data targeting a relatively small number of operations. Since they are also defined to operate over a fairly restrictive data model, they are extremely useful in situations where data consistency and safety are required. Here are some examples of capabilities found in DBMS that help in that regard: Transactions A transaction is a sequence of queries and update statements executed as a single unit For example, transferring money from one account to another Both the deduction from one account and credit to the other account should happen, or neither should Triggers A trigger is a statement that is executed automatically by the system as a side effect of a modification to the database Integrity Constraints Predicates on the database that must always hold Key Constraints: Specifiying something is a primary key or unique 14.1 SQL as a Data Definition Language The Structured Query Language (SQL) is both a Data Definition Language and a Data Manipulation Language CREATE TABLE &lt;name&gt; ( &lt;field&gt; &lt;domain&gt;, ... ) INSERT INTO &lt;name&gt; (&lt;field names&gt;) VALUES (&lt;field values&gt;) DELETE FROM &lt;name&gt; WHERE &lt;condition&gt; UPDATE &lt;name&gt; SET &lt;field name&gt; = &lt;value&gt; WHERE &lt;condition&gt; SELECT &lt;fields&gt; FROM &lt;name&gt; WHERE &lt;condition&gt; We can create tables and specify primary key attributes which enforce integrity constraints at the system level CREATE TABLE customer ( ssn CHAR(9) PRIMARY KEY, cname CHAR(15), address CHAR(30), city CHAR(10), UNIQUE (cname, address, city)); Attribute constraints: Constraints on the values of attributes bname char(15) not null balance int not null, check (balance &gt;= 0) Referential integrity: prevent dangling tuples CREATE TABLE branch(bname CHAR(15) PRIMARY KEY, ...); CREATE TABLE loan(..., FOREIGN KEY bname REFERENCES branch); Can tell the system what to do if a referenced tuple is being deleted Global Constraints Single-table CREATE TABLE branch (..., bcity CHAR(15), assets INT, CHECK (NOT(bcity = ‘Bkln’) OR assets &gt; 5M)) Multi-table CREATE ASSERTION loan-constraint CHECK (NOT EXISTS ( SELECT * FROM loan AS L WHERE NOT EXISTS( SELECT * FROM borrower B, depositor D, account A WHERE B.cname = D.cname AND D.acct_no = A.acct_no AND L.lno = B.lno))) 14.2 Set Operations and Comparisons Set operations select name from movieExec union/intersect/minus select name from movieStar Set Comparisons select * from movies where year in [1990, 1995, 2000]; select * from movies where year not in ( select extract(year from birthdate) from MovieStar ); 14.3 Views create view DisneyMovies select * from movie m where m.studioname = &#39;disney&#39;; Can use it in any place where a tablename is used. Views are used quite extensively to: (1) simplify queries, (2) hide data (by giving users access only to specific views). Views may be materialized or not. 14.4 NULLs Value of any attribute can be NULL if value is unknown, or it is not applicable, or hidden, etc. It can lead to counterintuitive behavior. For example, the following query does not return movies where length = NULL select * from movies where length &gt;= 120 or length &lt;= 120` Aggregate operations can be especially tricky when NULLs are present. "],
["ingesting-data.html", "15 Ingesting data 15.1 Structured ingestion 15.2 Scraping", " 15 Ingesting data Now that we have a better understanding of data analysis languages with tidyverse and SQL, we turn to the first significant challenge in data analysis, getting data into R in a shape that we can use to start our analysis. We will look at two types of data ingestion: structured ingestion, where we read data that is already structured, like a comma separated value (CSV) file, and scraping where we obtain data from text, usually in websites. There is an excellent discussion on data import here: http://r4ds.had.co.nz/data-import.html 15.1 Structured ingestion 15.1.1 CSV files (and similar) We saw in a previous chapter how we can use the read_csv file to read data from a CSV file into a data frame. Comma separated value (CSV) files are structured in a somewhat regular way, so reading into a data frame is straightforward. Each line in the CSV file corresponds to an observation (a row in a data frame). Each line contains values separated by a comma (,), corresponding to the variables of each observation. This ideal principle of how a CSV file is constructed is frequently violated by data contained in CSV files. To get a sense of how to deal with these cases look at the documentation of the read_csv function. For instance: the first line of the file may or may not contain the names of variables for the data frame (col_names argument). strings are quoted using ' instead of &quot; (quote argument) missing data is encoded with a non-standard code, e.g., - (na argument) values are separated by a character other than , (read_delim function) file may contain header information before the actual data so we have to skip some lines when loading the data (skip argument) You should read the documentation of the read_csv function to appreciate the complexities it can maneuver when reading data from structured text files. ?read_csv When loading a CSV file, we need to determine how to treat values for each attribute in the dataset. When we call read_csv, it guesses as to the best way to parse each attribute (e.g., is it a number, is it a factor, is it free text, how is missing data encoded). The readr package implements a set of core functions parse_* that parses vectors into different data types (e.g., parse_number, parse_datetime, parse_factor). When we call read_csv it will print it’s data types guesses and any problems it encounters. The problems function let’s you inspect parsing problems. E.g., df &lt;- read_csv(readr_example(&quot;challenge.csv&quot;)) ## Parsed with column specification: ## cols( ## x = col_double(), ## y = col_logical() ## ) ## Warning: 1000 parsing failures. ## row col expected actual file ## 1001 y 1/0/T/F/TRUE/FALSE 2015-01-16 &#39;/Library/Frameworks/R.framework/Versions/3.5/Resources/library/readr/extdata/challenge.csv&#39; ## 1002 y 1/0/T/F/TRUE/FALSE 2018-05-18 &#39;/Library/Frameworks/R.framework/Versions/3.5/Resources/library/readr/extdata/challenge.csv&#39; ## 1003 y 1/0/T/F/TRUE/FALSE 2015-09-05 &#39;/Library/Frameworks/R.framework/Versions/3.5/Resources/library/readr/extdata/challenge.csv&#39; ## 1004 y 1/0/T/F/TRUE/FALSE 2012-11-28 &#39;/Library/Frameworks/R.framework/Versions/3.5/Resources/library/readr/extdata/challenge.csv&#39; ## 1005 y 1/0/T/F/TRUE/FALSE 2020-01-13 &#39;/Library/Frameworks/R.framework/Versions/3.5/Resources/library/readr/extdata/challenge.csv&#39; ## .... ... .................. .......... ............................................................................................ ## See problems(...) for more details. problems(df) ## # A tibble: 1,000 x 5 ## row col expected actual file ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1001 y 1/0/T/F/TRUE… 2015-01… &#39;/Library/Frameworks/R.framework/Ver… ## 2 1002 y 1/0/T/F/TRUE… 2018-05… &#39;/Library/Frameworks/R.framework/Ver… ## 3 1003 y 1/0/T/F/TRUE… 2015-09… &#39;/Library/Frameworks/R.framework/Ver… ## 4 1004 y 1/0/T/F/TRUE… 2012-11… &#39;/Library/Frameworks/R.framework/Ver… ## 5 1005 y 1/0/T/F/TRUE… 2020-01… &#39;/Library/Frameworks/R.framework/Ver… ## 6 1006 y 1/0/T/F/TRUE… 2016-04… &#39;/Library/Frameworks/R.framework/Ver… ## 7 1007 y 1/0/T/F/TRUE… 2011-05… &#39;/Library/Frameworks/R.framework/Ver… ## 8 1008 y 1/0/T/F/TRUE… 2020-07… &#39;/Library/Frameworks/R.framework/Ver… ## 9 1009 y 1/0/T/F/TRUE… 2011-04… &#39;/Library/Frameworks/R.framework/Ver… ## 10 1010 y 1/0/T/F/TRUE… 2010-05… &#39;/Library/Frameworks/R.framework/Ver… ## # … with 990 more rows The argument col_types is used to help the parser handle datatypes correctly. In class discussion: how to parse readr_example(&quot;challenge.csv&quot;) Other hints: You can read every attribute as character using col_types=cols(.default=col_character()). Combine this with type_convert to parse character attributes into other types: df &lt;- read_csv(readr_example(&quot;challenge.csv&quot;), col_types=cols(.default=col_character())) %&gt;% type_convert(cols(x=col_double(), y=col_date())) If nothing else works, you can read file lines using read_lines and then parse lines using string processing operations (which we will see shortly). 15.1.2 Excel spreadsheets Often you will need to ingest data that is stored in an Excel spreadsheet. The readxl package is used to do this. The main function for this package is the read_excel function. It contains similar arguments to the read_csv function we saw above. 15.2 Scraping Often, data we want to use is hosted as part of HTML files in webpages. The markup structure of HTML allows to parse data into tables we can use for analysis. Let’s use the Rotten Tomatoes ratings webpage for Diego Luna as an example: We can scrape ratings for his movies from this page. To do this we need to figure out how the HTML page’s markup can help us write R expressions to find this data in the page. Most web browsers have facilities to show page markup. In Google Chrome, you can use View&gt;Developer&gt;Developer Tools, and inspect the page markdown to find where the data is contained. In this example, we see that the data we want is in a &lt;table&gt; element in the page, with id filmographyTbl. Now that we have that information, we can use the rvest package to scrape this data: library(rvest) url &lt;- &quot;https://www.rottentomatoes.com/celebrity/diego_luna&quot; dl_tab &lt;- url %&gt;% read_html() %&gt;% html_node(&quot;#filmographyTbl&quot;) %&gt;% html_table() head(dl_tab) ## RATING TITLE CREDIT ## 1 No Score Yet Berlin, I Love You Drag Queen ## 2 95% If Beale Street Could Talk Pedrocito ## 3 No Score Yet Crow: The Legend Moth ## 4 4% Flatliners Ray ## 5 84% Rogue One: A Star Wars Story Captain Cassian Andor ## 6 89% Blood Father Jonah ## BOX OFFICE YEAR ## 1 — 2019 ## 2 — 2019 ## 3 — 2018 ## 4 $16.9M 2017 ## 5 $532.2M 2016 ## 6 — 2016 The main two functions we used here are html_node and html_table. html_node finds elements in the HTML page according to some selection criteria. Since we want the element with id=filmographyTbl we use the # selection operation since that corresponds to selection by id. Once the desired element in the page is selected, we can use the html_table function to parse the element’s text into a data frame. The argument to the html_node function uses CSS selector syntax: https://www.w3.org/TR/CSS2/selector.html On your own: If you wanted to extract the TV filmography from the page, how would you change this call? 15.2.1 Scraping from dirty HTML tables We saw above how to extract data from HTML tables. But what if the data we want to extract is not cleanly formatted as a HTML table, or is spread over multiple html pages? Let’s look at an example where we scrape titles and artists from billboard #1 songs: https://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_number-one_singles_of_2017 Let’s start by reading the HTML markup and finding the document node that contains the table we want to scrape library(rvest) url &lt;- &quot;https://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_number-one_singles_of_2017&quot; singles_tab_node &lt;- read_html(url) %&gt;% html_node(&quot;.plainrowheaders&quot;) singles_tab_node ## {xml_node} ## &lt;table class=&quot;wikitable plainrowheaders&quot; style=&quot;text-align: center&quot;&gt; ## [1] &lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;th&gt;Issue date\\n&lt;/th&gt;\\n&lt;th&gt;Song\\n&lt;/th&gt;\\n&lt;th&gt;Artist(s) ... Since the rows of the table are not cleanly aligned, we need to extract each attribute separately. Let’s start with the dates in the first column. Since we noticed that the nodes containing dates have attribute scope we use the attribute CSS selector [scope]. dates &lt;- singles_tab_node %&gt;% html_nodes(&quot;[scope]&quot;) %&gt;% html_text() dates %&gt;% head() ## [1] &quot;January 7\\n&quot; &quot;January 14\\n&quot; &quot;January 21\\n&quot; &quot;January 28\\n&quot; ## [5] &quot;February 4\\n&quot; &quot;February 11\\n&quot; Next, we extract song titles, first we grab the tr (table row) nodes and extract from each the first td node using the td:first-of-type CSS selector. Notice that this gets us the header row which we remove using the magrittr::extract function. The title nodes also tell us how many rows this spans, which we grab from the rowspan attribute. title_nodes &lt;- singles_tab_node %&gt;% html_nodes(&quot;tr&quot;) %&gt;% html_node(&quot;td:first-of-type&quot;) %&gt;% magrittr::extract(-1) song_titles &lt;- title_nodes %&gt;% html_text() title_spans &lt;- title_nodes %&gt;% html_attr(&quot;rowspan&quot;) cbind(song_titles, title_spans) %&gt;% head(10) ## song_titles title_spans ## [1,] &quot;\\&quot;Starboy\\&quot;\\n&quot; NA ## [2,] &quot;\\&quot;Black Beatles\\&quot;\\n&quot; NA ## [3,] &quot;\\&quot;Bad and Boujee\\&quot;\\n&quot; NA ## [4,] &quot;\\&quot;Shape of You\\&quot; &quot; NA ## [5,] &quot;\\&quot;Bad and Boujee\\&quot;\\n&quot; &quot;2&quot; ## [6,] &quot;[7]&quot; NA ## [7,] &quot;\\&quot;Shape of You\\&quot; &quot; &quot;11&quot; ## [8,] &quot;[9]&quot; NA ## [9,] &quot;[10]&quot; NA ## [10,] &quot;[11]&quot; NA To get artist names we get the second data element (td) of each row using the td:nth-of-type(2) CSS selector (again removing the first entry in result coming from the header row) artist_nodes &lt;- singles_tab_node %&gt;% html_nodes(&quot;tr&quot;) %&gt;% html_node(&quot;td:nth-of-type(2)&quot;) %&gt;% magrittr::extract(-1) artists &lt;- artist_nodes %&gt;% html_text() artists %&gt;% head(10) ## [1] &quot;The Weeknd featuring Daft Punk\\n&quot; ## [2] &quot;Rae Sremmurd featuring Gucci Mane\\n&quot; ## [3] &quot;Migos featuring Lil Uzi Vert\\n&quot; ## [4] &quot;Ed Sheeran\\n&quot; ## [5] &quot;Migos featuring Lil Uzi Vert\\n&quot; ## [6] NA ## [7] &quot;Ed Sheeran\\n&quot; ## [8] NA ## [9] NA ## [10] NA Now that we’ve extracted each attribute separately we can combine them into a single data frame billboard_df &lt;- data_frame(month_day=dates, year=&quot;2017&quot;, song_title_raw=song_titles, title_span=title_spans, artist_raw=artists) ## Warning: `data_frame()` is deprecated, use `tibble()`. ## This warning is displayed once per session. billboard_df ## # A tibble: 52 x 5 ## month_day year song_title_raw title_span artist_raw ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 &quot;January 7\\… 2017 &quot;\\&quot;Starboy\\&quot;\\n&quot; &lt;NA&gt; &quot;The Weeknd featuring … ## 2 &quot;January 14… 2017 &quot;\\&quot;Black Beatles\\… &lt;NA&gt; &quot;Rae Sremmurd featurin… ## 3 &quot;January 21… 2017 &quot;\\&quot;Bad and Boujee… &lt;NA&gt; &quot;Migos featuring Lil U… ## 4 &quot;January 28… 2017 &quot;\\&quot;Shape of You\\&quot;… &lt;NA&gt; &quot;Ed Sheeran\\n&quot; ## 5 &quot;February 4… 2017 &quot;\\&quot;Bad and Boujee… 2 &quot;Migos featuring Lil U… ## 6 &quot;February 1… 2017 [7] &lt;NA&gt; &lt;NA&gt; ## 7 &quot;February 1… 2017 &quot;\\&quot;Shape of You\\&quot;… 11 &quot;Ed Sheeran\\n&quot; ## 8 &quot;February 2… 2017 [9] &lt;NA&gt; &lt;NA&gt; ## 9 &quot;March 4\\n&quot; 2017 [10] &lt;NA&gt; &lt;NA&gt; ## 10 &quot;March 11\\n&quot; 2017 [11] &lt;NA&gt; &lt;NA&gt; ## # … with 42 more rows This is by no means a clean data frame yet, but we will discuss how to clean up data like this in later lectures. We can now abstract these operations into a function that scrapes the same data for other years. scrape_billboard &lt;- function(year, baseurl=&quot;https://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_number-one_singles_of_&quot;) { url &lt;- paste0(baseurl, year) # find table node singles_tab_node &lt;- read_html(url) %&gt;% html_node(&quot;.plainrowheaders&quot;) # extract dates dates &lt;- singles_tab_node %&gt;% html_nodes(&quot;[scope]&quot;) %&gt;% html_text() # extract titles and spans title_nodes &lt;- singles_tab_node %&gt;% html_nodes(&quot;tr&quot;) %&gt;% html_node(&quot;td:first-of-type&quot;) %&gt;% magrittr::extract(-1) song_titles &lt;- title_nodes %&gt;% html_text() title_spans &lt;- title_nodes %&gt;% html_attr(&quot;rowspan&quot;) # extract artists artist_nodes &lt;- singles_tab_node %&gt;% html_nodes(&quot;tr&quot;) %&gt;% html_node(&quot;td:nth-of-type(2)&quot;) %&gt;% magrittr::extract(-1) artists &lt;- artist_nodes %&gt;% html_text() # make data frame data_frame(month_day=dates, year=year, song_title_raw=song_titles, title_span=title_spans, artist_raw=artists) } scrape_billboard(&quot;2016&quot;) ## # A tibble: 53 x 5 ## month_day year song_title_raw title_span artist_raw ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 &quot;January 2\\n&quot; 2016 &quot;\\&quot;Hello\\&quot;\\n&quot; 3 &quot;Adele\\n&quot; ## 2 &quot;January 9\\n&quot; 2016 [4] &lt;NA&gt; &lt;NA&gt; ## 3 &quot;January 16\\n&quot; 2016 [5] &lt;NA&gt; &lt;NA&gt; ## 4 &quot;January 23\\n&quot; 2016 &quot;\\&quot;Sorry\\&quot;\\n&quot; 3 &quot;Justin Bieber\\n&quot; ## 5 &quot;January 30\\n&quot; 2016 [7] &lt;NA&gt; &lt;NA&gt; ## 6 &quot;February 6\\n&quot; 2016 [8] &lt;NA&gt; &lt;NA&gt; ## 7 &quot;February 13\\… 2016 &quot;\\&quot;Love Yourself\\… &lt;NA&gt; [9] ## 8 &quot;February 20\\… 2016 &quot;\\&quot;Pillowtalk\\&quot;\\n&quot; &lt;NA&gt; &quot;Zayn\\n&quot; ## 9 &quot;February 27\\… 2016 &quot;\\&quot;Love Yourself\\… &lt;NA&gt; &quot;Justin Bieber\\n&quot; ## 10 &quot;March 5\\n&quot; 2016 &quot;\\&quot;Work\\&quot;\\n&quot; 9 &quot;Rihanna featuring D… ## # … with 43 more rows We can do this for a few years and create a (very dirty) dataset with songs for this current decade: billboard_tab &lt;- as.character(2010:2017) %&gt;% purrr::map_df(scrape_billboard) billboard_tab %&gt;% head(20) %&gt;% knitr::kable(&quot;html&quot;) month_day year song_title_raw title_span artist_raw January 7 2012 “Sexy and I Know It” 2 LMFAO January 14 2012 [13] NA NA January 21 2012 “We Found Love” 2 Rihanna featuring Calvin Harris January 28 2012 [15] NA NA February 4 2012 “Set Fire to the Rain” 2 Adele February 11 2012 [17] NA NA February 18 2012 “Stronger (What Doesn’t Kill You)” 2 Kelly Clarkson February 25 2012 [19] NA NA March 3 2012 “Part of Me” NA Katy Perry March 10 2012 “Stronger (What Doesn’t Kill You)” NA Kelly Clarkson March 17 2012 “We Are Young” 6 Fun featuring Janelle Monáe March 24 2012 [22] NA NA March 31 2012 [23] NA NA April 7 2012 [24] NA NA April 14 2012 [25] NA NA April 21 2012 [7] NA NA April 28 2012 “Somebody That I Used to Know” 8 Gotye featuring Kimbra May 5 2012 [27] NA NA May 12 2012 [28] NA NA May 19 2012 [29] NA NA The function purrr::map_df is an example of a very powerful idiom in functional programming: mapping functions on elements of vectors. Here, we first create a vector of years (as strings) using as.character(2010:2017) we pass that to purrr::map_df which applies the function we create, scrape_billboard on each entry of the year vector. Each of these calls evaluates to a data_frame which are then bound (using bind_rows) to create a single long data frame. The tidyverse package purrr defines a lot of these functional programming idioms. One more thing: here’s a very nice example of rvest at work: https://deanattali.com/blog/user2017/ "],
["tidying-data.html", "16 Tidying data 16.1 Tidy Data 16.2 Common problems in messy data", " 16 Tidying data This section is concerned with common problems in data preparation, namely use cases commonly found in raw datasets that need to be addressed to turn messy data into tidy data. These would be operations that you would perform on data obtained as a csv file from a collaborator or data repository, or as the result of scraping data from webpages or other sources. We derive many of our ideas from the paper Tidy Data by Hadley Wickham. Associated with that paper we will use two very powerful R libraries tidyr and dplyr which are extremely useful in writing scripts for data cleaning, preparation and summarization. A basic design principle behind these libraries is trying to effectively and efficiently capture very common use cases and operations performed in data cleaning. The paper frames these use cases and operations which are them implemented in software. 16.1 Tidy Data Here we assume we are working with a data model based on rectangular data structures where Each attribute (or variable) forms a column Each entity (or observation) forms a row Each type of entity (observational unit) forms a table Here is an example of a tidy dataset: library(nycflights13) head(flights) ## # A tibble: 6 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 544 545 -1 1004 ## 5 2013 1 1 554 600 -6 812 ## 6 2013 1 1 554 558 -4 740 ## # … with 12 more variables: sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, ## # carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, ## # air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, ## # time_hour &lt;dttm&gt; it has one observation per row, a single variable per column. Notice only information about flights are included here (e.g., no airport information other than the name) in these observations. 16.2 Common problems in messy data The set of common operations we will study are based on these common problems found in datasets. We will see each one in detail: Column headers are values, not variable names (gather) Multiple variables stored in one column (split) Variables stored in both rows and column (rotate) Multiple types of observational units are stored in the same table (normalize) Single observational unit stored in multiple tables (join) We are using data from Hadley’s paper found in github. It’s included directory data: data_dir &lt;- &quot;data&quot; 16.2.1 Headers as values The first problem we’ll see is the case where a table header contains values. library(tidyverse) pew &lt;- read_csv(file.path(data_dir, &quot;pew.csv&quot;)) ## Parsed with column specification: ## cols( ## religion = col_character(), ## `&lt;$10k` = col_double(), ## `$10-20k` = col_double(), ## `$20-30k` = col_double(), ## `$30-40k` = col_double(), ## `$40-50k` = col_double(), ## `$50-75k` = col_double(), ## `$75-100k` = col_double(), ## `$100-150k` = col_double(), ## `&gt;150k` = col_double(), ## `Don&#39;t know/refused` = col_double() ## ) pew ## # A tibble: 18 x 11 ## religion `&lt;$10k` `$10-20k` `$20-30k` `$30-40k` `$40-50k` `$50-75k` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Agnostic 27 34 60 81 76 137 ## 2 Atheist 12 27 37 52 35 70 ## 3 Buddhist 27 21 30 34 33 58 ## 4 Catholic 418 617 732 670 638 1116 ## 5 Don’t k… 15 14 15 11 10 35 ## 6 Evangel… 575 869 1064 982 881 1486 ## 7 Hindu 1 9 7 9 11 34 ## 8 Histori… 228 244 236 238 197 223 ## 9 Jehovah… 20 27 24 24 21 30 ## 10 Jewish 19 19 25 25 30 95 ## 11 Mainlin… 289 495 619 655 651 1107 ## 12 Mormon 29 40 48 51 56 112 ## 13 Muslim 6 7 9 10 9 23 ## 14 Orthodox 13 17 23 32 32 47 ## 15 Other C… 9 7 11 13 13 14 ## 16 Other F… 20 33 40 46 49 63 ## 17 Other W… 5 2 3 4 2 7 ## 18 Unaffil… 217 299 374 365 341 528 ## # … with 4 more variables: `$75-100k` &lt;dbl&gt;, `$100-150k` &lt;dbl&gt;, ## # `&gt;150k` &lt;dbl&gt;, `Don&#39;t know/refused` &lt;dbl&gt; This table has the number of survey respondents of a specific religion that report their income within some range. A tidy version of this table would consider the variables of each observation to be religion, income, frequency where frequency has the number of respondents for each religion and income range. The function to use in the tidyr package is gather: tidy_pew &lt;- gather(pew, income, frequency, -religion) tidy_pew ## # A tibble: 180 x 3 ## religion income frequency ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Agnostic &lt;$10k 27 ## 2 Atheist &lt;$10k 12 ## 3 Buddhist &lt;$10k 27 ## 4 Catholic &lt;$10k 418 ## 5 Don’t know/refused &lt;$10k 15 ## 6 Evangelical Prot &lt;$10k 575 ## 7 Hindu &lt;$10k 1 ## 8 Historically Black Prot &lt;$10k 228 ## 9 Jehovah&#39;s Witness &lt;$10k 20 ## 10 Jewish &lt;$10k 19 ## # … with 170 more rows This says: gather all the columns from the pew (except religion) into key-value columns income and frequency. This table is much easier to use in other analyses. Another example: this table has a row for each song appearing in the billboard top 100. It contains track information, and the date it entered the top 100. It then shows the rank in each of the next 76 weeks. billboard &lt;- read_csv(file.path(data_dir, &quot;billboard.csv&quot;)) ## Parsed with column specification: ## cols( ## .default = col_double(), ## artist = col_character(), ## track = col_character(), ## time = col_time(format = &quot;&quot;), ## date.entered = col_date(format = &quot;&quot;), ## wk66 = col_logical(), ## wk67 = col_logical(), ## wk68 = col_logical(), ## wk69 = col_logical(), ## wk70 = col_logical(), ## wk71 = col_logical(), ## wk72 = col_logical(), ## wk73 = col_logical(), ## wk74 = col_logical(), ## wk75 = col_logical(), ## wk76 = col_logical() ## ) ## See spec(...) for full column specifications. billboard ## # A tibble: 317 x 81 ## year artist track time date.entered wk1 wk2 wk3 wk4 wk5 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;tim&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2000 2 Pac Baby… 04:22 2000-02-26 87 82 72 77 87 ## 2 2000 2Ge+h… The … 03:15 2000-09-02 91 87 92 NA NA ## 3 2000 3 Doo… Kryp… 03:53 2000-04-08 81 70 68 67 66 ## 4 2000 3 Doo… Loser 04:24 2000-10-21 76 76 72 69 67 ## 5 2000 504 B… Wobb… 03:35 2000-04-15 57 34 25 17 17 ## 6 2000 98^0 Give… 03:24 2000-08-19 51 39 34 26 26 ## 7 2000 A*Tee… Danc… 03:44 2000-07-08 97 97 96 95 100 ## 8 2000 Aaliy… I Do… 04:15 2000-01-29 84 62 51 41 38 ## 9 2000 Aaliy… Try … 04:03 2000-03-18 59 53 38 28 21 ## 10 2000 Adams… Open… 05:30 2000-08-26 76 76 74 69 68 ## # … with 307 more rows, and 71 more variables: wk6 &lt;dbl&gt;, wk7 &lt;dbl&gt;, ## # wk8 &lt;dbl&gt;, wk9 &lt;dbl&gt;, wk10 &lt;dbl&gt;, wk11 &lt;dbl&gt;, wk12 &lt;dbl&gt;, wk13 &lt;dbl&gt;, ## # wk14 &lt;dbl&gt;, wk15 &lt;dbl&gt;, wk16 &lt;dbl&gt;, wk17 &lt;dbl&gt;, wk18 &lt;dbl&gt;, ## # wk19 &lt;dbl&gt;, wk20 &lt;dbl&gt;, wk21 &lt;dbl&gt;, wk22 &lt;dbl&gt;, wk23 &lt;dbl&gt;, ## # wk24 &lt;dbl&gt;, wk25 &lt;dbl&gt;, wk26 &lt;dbl&gt;, wk27 &lt;dbl&gt;, wk28 &lt;dbl&gt;, ## # wk29 &lt;dbl&gt;, wk30 &lt;dbl&gt;, wk31 &lt;dbl&gt;, wk32 &lt;dbl&gt;, wk33 &lt;dbl&gt;, ## # wk34 &lt;dbl&gt;, wk35 &lt;dbl&gt;, wk36 &lt;dbl&gt;, wk37 &lt;dbl&gt;, wk38 &lt;dbl&gt;, ## # wk39 &lt;dbl&gt;, wk40 &lt;dbl&gt;, wk41 &lt;dbl&gt;, wk42 &lt;dbl&gt;, wk43 &lt;dbl&gt;, ## # wk44 &lt;dbl&gt;, wk45 &lt;dbl&gt;, wk46 &lt;dbl&gt;, wk47 &lt;dbl&gt;, wk48 &lt;dbl&gt;, ## # wk49 &lt;dbl&gt;, wk50 &lt;dbl&gt;, wk51 &lt;dbl&gt;, wk52 &lt;dbl&gt;, wk53 &lt;dbl&gt;, ## # wk54 &lt;dbl&gt;, wk55 &lt;dbl&gt;, wk56 &lt;dbl&gt;, wk57 &lt;dbl&gt;, wk58 &lt;dbl&gt;, ## # wk59 &lt;dbl&gt;, wk60 &lt;dbl&gt;, wk61 &lt;dbl&gt;, wk62 &lt;dbl&gt;, wk63 &lt;dbl&gt;, ## # wk64 &lt;dbl&gt;, wk65 &lt;dbl&gt;, wk66 &lt;lgl&gt;, wk67 &lt;lgl&gt;, wk68 &lt;lgl&gt;, ## # wk69 &lt;lgl&gt;, wk70 &lt;lgl&gt;, wk71 &lt;lgl&gt;, wk72 &lt;lgl&gt;, wk73 &lt;lgl&gt;, ## # wk74 &lt;lgl&gt;, wk75 &lt;lgl&gt;, wk76 &lt;lgl&gt; Challenge: This dataset has values as column names. Which column names are values? How do we tidy this dataset? 16.2.2 Multiple variables in one column The next problem we’ll see is the case when we see multiple variables in a single column. Consider the following dataset of tuberculosis cases: tb &lt;- read_csv(file.path(data_dir, &quot;tb.csv&quot;)) ## Parsed with column specification: ## cols( ## .default = col_double(), ## iso2 = col_character() ## ) ## See spec(...) for full column specifications. tb ## # A tibble: 5,769 x 22 ## iso2 year m04 m514 m014 m1524 m2534 m3544 m4554 m5564 m65 mu ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AD 1989 NA NA NA NA NA NA NA NA NA NA ## 2 AD 1990 NA NA NA NA NA NA NA NA NA NA ## 3 AD 1991 NA NA NA NA NA NA NA NA NA NA ## 4 AD 1992 NA NA NA NA NA NA NA NA NA NA ## 5 AD 1993 NA NA NA NA NA NA NA NA NA NA ## 6 AD 1994 NA NA NA NA NA NA NA NA NA NA ## 7 AD 1996 NA NA 0 0 0 4 1 0 0 NA ## 8 AD 1997 NA NA 0 0 1 2 2 1 6 NA ## 9 AD 1998 NA NA 0 0 0 1 0 0 0 NA ## 10 AD 1999 NA NA 0 0 0 1 1 0 0 NA ## # … with 5,759 more rows, and 10 more variables: f04 &lt;dbl&gt;, f514 &lt;dbl&gt;, ## # f014 &lt;dbl&gt;, f1524 &lt;dbl&gt;, f2534 &lt;dbl&gt;, f3544 &lt;dbl&gt;, f4554 &lt;dbl&gt;, ## # f5564 &lt;dbl&gt;, f65 &lt;dbl&gt;, fu &lt;dbl&gt; This table has a row for each year and strain of tuberculosis (given by the first two columns). The remaining columns state the number of cases for a given demographic. For example, m1524 corresponds to males between 15 and 24 years old, and f1524 are females age 15-24. As you can see each of these columns has two variables: sex and age. Challenge: what else is untidy about this dataset? So, we have to do two operations to tidy this table, first we need to use gather the tabulation columns into a demo and n columns (for demographic and number of cases): tidy_tb &lt;- gather(tb, demo, n, -iso2, -year) tidy_tb ## # A tibble: 115,380 x 4 ## iso2 year demo n ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 AD 1989 m04 NA ## 2 AD 1990 m04 NA ## 3 AD 1991 m04 NA ## 4 AD 1992 m04 NA ## 5 AD 1993 m04 NA ## 6 AD 1994 m04 NA ## 7 AD 1996 m04 NA ## 8 AD 1997 m04 NA ## 9 AD 1998 m04 NA ## 10 AD 1999 m04 NA ## # … with 115,370 more rows Next, we need to separate the values in the demo column into two variables sex and age tidy_tb &lt;- separate(tidy_tb, demo, c(&quot;sex&quot;, &quot;age&quot;), sep=1) tidy_tb ## # A tibble: 115,380 x 5 ## iso2 year sex age n ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 AD 1989 m 04 NA ## 2 AD 1990 m 04 NA ## 3 AD 1991 m 04 NA ## 4 AD 1992 m 04 NA ## 5 AD 1993 m 04 NA ## 6 AD 1994 m 04 NA ## 7 AD 1996 m 04 NA ## 8 AD 1997 m 04 NA ## 9 AD 1998 m 04 NA ## 10 AD 1999 m 04 NA ## # … with 115,370 more rows This calls the separate function on table tidy_db, separating the demo variable into variables sex and age by separating each value after the first character (that’s the sep argument). We can put these two commands together in a pipeline: tidy_tb &lt;- tb %&gt;% gather(demo, n, -iso2, -year) %&gt;% separate(demo, c(&quot;sex&quot;, &quot;age&quot;), sep=1) tidy_tb ## # A tibble: 115,380 x 5 ## iso2 year sex age n ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 AD 1989 m 04 NA ## 2 AD 1990 m 04 NA ## 3 AD 1991 m 04 NA ## 4 AD 1992 m 04 NA ## 5 AD 1993 m 04 NA ## 6 AD 1994 m 04 NA ## 7 AD 1996 m 04 NA ## 8 AD 1997 m 04 NA ## 9 AD 1998 m 04 NA ## 10 AD 1999 m 04 NA ## # … with 115,370 more rows 16.2.3 Variables stored in both rows and columns This is the messiest, commonly found type of data. Let’s take a look at an example, this is daily weather data from for one weather station in Mexico in 2010. weather &lt;- read_csv(file.path(data_dir, &quot;weather.csv&quot;)) ## Parsed with column specification: ## cols( ## .default = col_double(), ## id = col_character(), ## element = col_character(), ## d9 = col_logical(), ## d12 = col_logical(), ## d18 = col_logical(), ## d19 = col_logical(), ## d20 = col_logical(), ## d21 = col_logical(), ## d22 = col_logical(), ## d24 = col_logical() ## ) ## See spec(...) for full column specifications. weather ## # A tibble: 22 x 35 ## id year month element d1 d2 d3 d4 d5 d6 d7 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 MX17… 2010 1 tmax NA NA NA NA NA NA NA ## 2 MX17… 2010 1 tmin NA NA NA NA NA NA NA ## 3 MX17… 2010 2 tmax NA 27.3 24.1 NA NA NA NA ## 4 MX17… 2010 2 tmin NA 14.4 14.4 NA NA NA NA ## 5 MX17… 2010 3 tmax NA NA NA NA 32.1 NA NA ## 6 MX17… 2010 3 tmin NA NA NA NA 14.2 NA NA ## 7 MX17… 2010 4 tmax NA NA NA NA NA NA NA ## 8 MX17… 2010 4 tmin NA NA NA NA NA NA NA ## 9 MX17… 2010 5 tmax NA NA NA NA NA NA NA ## 10 MX17… 2010 5 tmin NA NA NA NA NA NA NA ## # … with 12 more rows, and 24 more variables: d8 &lt;dbl&gt;, d9 &lt;lgl&gt;, ## # d10 &lt;dbl&gt;, d11 &lt;dbl&gt;, d12 &lt;lgl&gt;, d13 &lt;dbl&gt;, d14 &lt;dbl&gt;, d15 &lt;dbl&gt;, ## # d16 &lt;dbl&gt;, d17 &lt;dbl&gt;, d18 &lt;lgl&gt;, d19 &lt;lgl&gt;, d20 &lt;lgl&gt;, d21 &lt;lgl&gt;, ## # d22 &lt;lgl&gt;, d23 &lt;dbl&gt;, d24 &lt;lgl&gt;, d25 &lt;dbl&gt;, d26 &lt;dbl&gt;, d27 &lt;dbl&gt;, ## # d28 &lt;dbl&gt;, d29 &lt;dbl&gt;, d30 &lt;dbl&gt;, d31 &lt;dbl&gt; So, we have two rows for each month, one with maximum daily temperature, one with minimum daily temperature, the columns starting with d correspond to the day in the where the measurements were made. Challenge: How would a tidy version of this data look like? weather %&gt;% gather(day, value, d1:d31, na.rm=TRUE) %&gt;% spread(element, value) ## # A tibble: 33 x 6 ## id year month day tmax tmin ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 MX17004 2010 1 d30 27.8 14.5 ## 2 MX17004 2010 2 d11 29.7 13.4 ## 3 MX17004 2010 2 d2 27.3 14.4 ## 4 MX17004 2010 2 d23 29.9 10.7 ## 5 MX17004 2010 2 d3 24.1 14.4 ## 6 MX17004 2010 3 d10 34.5 16.8 ## 7 MX17004 2010 3 d16 31.1 17.6 ## 8 MX17004 2010 3 d5 32.1 14.2 ## 9 MX17004 2010 4 d27 36.3 16.7 ## 10 MX17004 2010 5 d27 33.2 18.2 ## # … with 23 more rows The new function we’ve used here is spread. It does the inverse of gather it spreads columns element and value into separate columns. 16.2.4 Multiple types in one table Remember that an important aspect of tidy data is that it contains exactly one kind of observation in a single table. Let’s see the billboard example again after the gather operation we did before: tidy_billboard &lt;- billboard %&gt;% gather(week, rank, wk1:wk76, na.rm=TRUE) tidy_billboard ## # A tibble: 5,307 x 7 ## year artist track time date.entered week rank ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;tim&gt; &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2000 2 Pac Baby Don&#39;t Cry (Keep… 04:22 2000-02-26 wk1 87 ## 2 2000 2Ge+her The Hardest Part Of … 03:15 2000-09-02 wk1 91 ## 3 2000 3 Doors Down Kryptonite 03:53 2000-04-08 wk1 81 ## 4 2000 3 Doors Down Loser 04:24 2000-10-21 wk1 76 ## 5 2000 504 Boyz Wobble Wobble 03:35 2000-04-15 wk1 57 ## 6 2000 98^0 Give Me Just One Nig… 03:24 2000-08-19 wk1 51 ## 7 2000 A*Teens Dancing Queen 03:44 2000-07-08 wk1 97 ## 8 2000 Aaliyah I Don&#39;t Wanna 04:15 2000-01-29 wk1 84 ## 9 2000 Aaliyah Try Again 04:03 2000-03-18 wk1 59 ## 10 2000 Adams, Yolan… Open My Heart 05:30 2000-08-26 wk1 76 ## # … with 5,297 more rows Let’s sort this table by track to see a problem with this table: tidy_billboard &lt;- tidy_billboard %&gt;% arrange(track) tidy_billboard ## # A tibble: 5,307 x 7 ## year artist track time date.entered week rank ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;time&gt; &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2000 Nelly (Hot S**t) Country G... 04:17 2000-04-29 wk1 100 ## 2 2000 Nelly (Hot S**t) Country G... 04:17 2000-04-29 wk2 99 ## 3 2000 Nelly (Hot S**t) Country G... 04:17 2000-04-29 wk3 96 ## 4 2000 Nelly (Hot S**t) Country G... 04:17 2000-04-29 wk4 76 ## 5 2000 Nelly (Hot S**t) Country G... 04:17 2000-04-29 wk5 55 ## 6 2000 Nelly (Hot S**t) Country G... 04:17 2000-04-29 wk6 37 ## 7 2000 Nelly (Hot S**t) Country G... 04:17 2000-04-29 wk7 24 ## 8 2000 Nelly (Hot S**t) Country G... 04:17 2000-04-29 wk8 24 ## 9 2000 Nelly (Hot S**t) Country G... 04:17 2000-04-29 wk9 30 ## 10 2000 Nelly (Hot S**t) Country G... 04:17 2000-04-29 wk10 36 ## # … with 5,297 more rows We have a lot of repeated information in many of these rows (the artist, track name, year, title and date entered). The problem is that this table contains information about both tracks and rank in billboard. That’s two different kinds of observations that should belong in two different tables in a tidy dataset. Let’s make a song table that only includes information about songs: song &lt;- tidy_billboard %&gt;% dplyr::select(artist, track, year, time, date.entered) %&gt;% unique() song ## # A tibble: 317 x 5 ## artist track year time date.entered ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;time&gt; &lt;date&gt; ## 1 Nelly (Hot S**t) Country G... 2000 04:17 2000-04-29 ## 2 Nu Flavor 3 Little Words 2000 03:54 2000-06-03 ## 3 Jean, Wyclef 911 2000 04:00 2000-10-07 ## 4 Brock, Chad A Country Boy Can Su... 2000 03:54 2000-01-01 ## 5 Clark, Terri A Little Gasoline 2000 03:07 2000-12-16 ## 6 Son By Four A Puro Dolor (Purest... 2000 03:30 2000-04-08 ## 7 Carter, Aaron Aaron&#39;s Party (Come ... 2000 03:23 2000-08-26 ## 8 Nine Days Absolutely (Story Of... 2000 03:09 2000-05-06 ## 9 De La Soul All Good? 2000 05:02 2000-12-23 ## 10 Blink-182 All The Small Things 2000 02:52 1999-12-04 ## # … with 307 more rows The unique function removes any duplicate rows in a table. That’s how we have a single row for each song. Next, we would like to remove all the song information from the rank table. But we need to do it in a way that still remembers which song each ranking observation corresponds to. To do that, let’s first give each song an identifier that we can use to link songs and rankings. So, we can produce the final version of our song table like this: song &lt;- tidy_billboard %&gt;% dplyr::select(artist, track, year, time, date.entered) %&gt;% unique() %&gt;% mutate(song_id = row_number()) song ## # A tibble: 317 x 6 ## artist track year time date.entered song_id ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;time&gt; &lt;date&gt; &lt;int&gt; ## 1 Nelly (Hot S**t) Country G... 2000 04:17 2000-04-29 1 ## 2 Nu Flavor 3 Little Words 2000 03:54 2000-06-03 2 ## 3 Jean, Wyclef 911 2000 04:00 2000-10-07 3 ## 4 Brock, Chad A Country Boy Can Su... 2000 03:54 2000-01-01 4 ## 5 Clark, Terri A Little Gasoline 2000 03:07 2000-12-16 5 ## 6 Son By Four A Puro Dolor (Purest... 2000 03:30 2000-04-08 6 ## 7 Carter, Aaron Aaron&#39;s Party (Come ... 2000 03:23 2000-08-26 7 ## 8 Nine Days Absolutely (Story Of... 2000 03:09 2000-05-06 8 ## 9 De La Soul All Good? 2000 05:02 2000-12-23 9 ## 10 Blink-182 All The Small Things 2000 02:52 1999-12-04 10 ## # … with 307 more rows The mutate function adds a new column to the table, in this case with column name song_id and value the row number the song appears in the table (from the row_number column). Now we can make a rank table, we combine the tidy billboard table with our new song table using a join (we’ll learn all about joins later). It checks the values on each row of the billboard table and looks for rows in the song table that have the exact same values, and makes a new row that combines the information from both tables. tidy_billboard %&gt;% left_join(song, c(&quot;artist&quot;, &quot;year&quot;, &quot;track&quot;, &quot;time&quot;, &quot;date.entered&quot;)) ## # A tibble: 5,307 x 8 ## year artist track time date.entered week rank song_id ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;tim&gt; &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2000 Nelly (Hot S**t) Country … 04:17 2000-04-29 wk1 100 1 ## 2 2000 Nelly (Hot S**t) Country … 04:17 2000-04-29 wk2 99 1 ## 3 2000 Nelly (Hot S**t) Country … 04:17 2000-04-29 wk3 96 1 ## 4 2000 Nelly (Hot S**t) Country … 04:17 2000-04-29 wk4 76 1 ## 5 2000 Nelly (Hot S**t) Country … 04:17 2000-04-29 wk5 55 1 ## 6 2000 Nelly (Hot S**t) Country … 04:17 2000-04-29 wk6 37 1 ## 7 2000 Nelly (Hot S**t) Country … 04:17 2000-04-29 wk7 24 1 ## 8 2000 Nelly (Hot S**t) Country … 04:17 2000-04-29 wk8 24 1 ## 9 2000 Nelly (Hot S**t) Country … 04:17 2000-04-29 wk9 30 1 ## 10 2000 Nelly (Hot S**t) Country … 04:17 2000-04-29 wk10 36 1 ## # … with 5,297 more rows That adds the song_id variable to the tidy_billboard table. So now we can remove the song information and only keep ranking information and the song_id. rank &lt;- tidy_billboard %&gt;% left_join(song, c(&quot;artist&quot;, &quot;year&quot;, &quot;track&quot;, &quot;time&quot;, &quot;date.entered&quot;)) %&gt;% dplyr::select(song_id, week, rank) rank ## # A tibble: 5,307 x 3 ## song_id week rank ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 wk1 100 ## 2 1 wk2 99 ## 3 1 wk3 96 ## 4 1 wk4 76 ## 5 1 wk5 55 ## 6 1 wk6 37 ## 7 1 wk7 24 ## 8 1 wk8 24 ## 9 1 wk9 30 ## 10 1 wk10 36 ## # … with 5,297 more rows Challenge: Let’s do a little better job at tidying the billboard dataset: When using gather to make the week and rank columns, remove any weeks where the song does not appear in the top 100. This is coded as missing (NA). See the na.rm argument to gather. Make week a numeric variable (i.e., remove wk). See what the extract_numeric function does. Instead of date.entered add a date column that states the actual date of each ranking. See how R deals with dates ?Date and how you can turn a string into a Date using as.Date. Sort the resulting table by date and rank. Make new song and rank tables. song will now not have the date.entered column, and rank will have the new date column you have just created. "],
["text-and-dates.html", "17 Text and Dates 17.1 Text 17.2 Handling dates", " 17 Text and Dates In this chapter we briefly discuss common patterns to handle text and date data and point to useful resources. 17.1 Text Frequently, data scraped or ingested will contain text that will need to be processed to either extract data, correct errors or resolve duplicate records. In this section we will look at a few common patterns: 1) tools for string operations, 2) tools using regular expressions, and 3) deriving attributes from text. For further reading consult: http://r4ds.had.co.nz/strings.html 17.1.1 String operations The stringr package contains a number of useful, commonly used string manipulation operations. library(tidyverse) library(stringr) short_string &lt;- &quot;I love Spring&quot; long_string &lt;- &quot;There&#39;s is nothing I love more than 320 in the Spring&quot; Here are a few common ones: string length: str_len str_length(c(short_string, long_string)) ## [1] 13 53 combining strings: str_c str_c(short_string, long_string, sep=&quot;. &quot;) ## [1] &quot;I love Spring. There&#39;s is nothing I love more than 320 in the Spring&quot; subsetting strings: str_sub str_sub(c(short_string, long_string), 2, 5) ## [1] &quot; lov&quot; &quot;here&quot; trim strings: str_trim str_trim(&quot; I am padded &quot;, side=&quot;both&quot;) ## [1] &quot;I am padded&quot; 17.1.2 Regular expressions By far, the most powerful tools for extracting and cleaning text data are regular expressions. The stringr package provides a great number of tools based on regular expression matching. First, some basics strs &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;) str_view(strs, &quot;an&quot;) Match any character: . Match the ‘dot’ character: \\\\. str_view(strs, &quot;.a.&quot;) str_view(c(strs, &quot;a.c&quot;), &quot;a\\\\.c&quot;) Anchor start (^), end ($) str_view(strs, &quot;^a&quot;) str_view(strs, &quot;a$&quot;) str_view(c(&quot;apple pie&quot;, &quot;apple&quot;, &quot;apple cake&quot;), &quot;apple&quot;) str_view(c(&quot;apple pie&quot;, &quot;apple&quot;, &quot;apple cake&quot;), &quot;^apple$&quot;) Character classes and alternatives \\d: match any digit \\s: match any whitespace (e.g., space, tab, newline) [abc]: match set of characters (e.g, a, b, or c) [^abc]: match anything except this set of characters |: match any of one or more patterns Match vowels or digits str_view(c(&quot;t867nine&quot;, &quot;gray9&quot;), &quot;[aeiou]|[0-9]&quot;) Repetition ?: zero or one +: one or more *: zero or more str_view(c(&quot;color&quot;, &quot;colour&quot;), &quot;colou?r&quot;) Grouping and backreferences Parentheses define groups, which can be referenced using \\1, \\2, etc. fruit &lt;- c(&quot;banana&quot;, &quot;coconut&quot;, &quot;cucumber&quot;, &quot;jujube&quot;, &quot;papaya&quot;, &quot;salal berry&quot;) str_view(fruit, &quot;(..)\\\\1&quot;) 17.1.3 Tools using regular expressions Determine which strings match a pattern: str_detect: given a vector of strings, return TRUE for those that match a regular expression, FALSE otherwise data(words) print(head(words)) ## [1] &quot;a&quot; &quot;able&quot; &quot;about&quot; &quot;absolute&quot; &quot;accept&quot; &quot;account&quot; data_frame(word=words, result=str_detect(words, &quot;^[aeiou]&quot;)) %&gt;% sample_n(30) ## Warning: `data_frame()` is deprecated, use `tibble()`. ## This warning is displayed once per session. ## # A tibble: 30 x 2 ## word result ## &lt;chr&gt; &lt;lgl&gt; ## 1 minus FALSE ## 2 sell FALSE ## 3 wee FALSE ## 4 across TRUE ## 5 chairman FALSE ## 6 file FALSE ## 7 sense FALSE ## 8 future FALSE ## 9 kid FALSE ## 10 grow FALSE ## # … with 20 more rows Similarly, str_count returns the number of matches in a string instead of just TRUE or FALSE Filter string vectors to include only those that match a regular expression data(sentences) print(head(sentences)) ## [1] &quot;The birch canoe slid on the smooth planks.&quot; ## [2] &quot;Glue the sheet to the dark blue background.&quot; ## [3] &quot;It&#39;s easy to tell the depth of a well.&quot; ## [4] &quot;These days a chicken leg is a rare dish.&quot; ## [5] &quot;Rice is often served in round bowls.&quot; ## [6] &quot;The juice of lemons makes fine punch.&quot; colors &lt;- c(&quot;red&quot;, &quot;orange&quot;, &quot;yellow&quot;, &quot;green&quot;, &quot;blue&quot;, &quot;purple&quot;) colors_re &lt;- str_c(colors, collapse=&quot;|&quot;) print(colors_re) ## [1] &quot;red|orange|yellow|green|blue|purple&quot; sentences_with_color &lt;- str_subset(sentences, colors_re) %&gt;% head(10) str_view_all(sentences_with_color, colors_re) Extracting matches: str_extract, str_extract_all str_extract(sentences_with_color, colors_re) ## [1] &quot;blue&quot; &quot;blue&quot; &quot;red&quot; &quot;red&quot; &quot;red&quot; &quot;blue&quot; &quot;yellow&quot; ## [8] &quot;red&quot; &quot;red&quot; &quot;green&quot; Grouped matches: str_match noun_re &lt;- &quot;(a|the) ([^ ]+)&quot; noun_matches &lt;- sentences %&gt;% str_subset(noun_re) %&gt;% str_match(noun_re) %&gt;% head(10) noun_matches ## [,1] [,2] [,3] ## [1,] &quot;the smooth&quot; &quot;the&quot; &quot;smooth&quot; ## [2,] &quot;the sheet&quot; &quot;the&quot; &quot;sheet&quot; ## [3,] &quot;the depth&quot; &quot;the&quot; &quot;depth&quot; ## [4,] &quot;a chicken&quot; &quot;a&quot; &quot;chicken&quot; ## [5,] &quot;the parked&quot; &quot;the&quot; &quot;parked&quot; ## [6,] &quot;the sun&quot; &quot;the&quot; &quot;sun&quot; ## [7,] &quot;the huge&quot; &quot;the&quot; &quot;huge&quot; ## [8,] &quot;the ball&quot; &quot;the&quot; &quot;ball&quot; ## [9,] &quot;the woman&quot; &quot;the&quot; &quot;woman&quot; ## [10,] &quot;a helps&quot; &quot;a&quot; &quot;helps&quot; The result is a string matrix, with one row for each string in the input vector. The first column includes the complete match to the regular expression (just like str_extract), the remaining columns has the matches for the groups defined in the pattern. To extract the first group matches one would index one of the columns. For example, the matches for the second group are noun_matches[,3] ## [1] &quot;smooth&quot; &quot;sheet&quot; &quot;depth&quot; &quot;chicken&quot; &quot;parked&quot; &quot;sun&quot; &quot;huge&quot; ## [8] &quot;ball&quot; &quot;woman&quot; &quot;helps&quot; Splitting strings: str_split split strings in a vector based on a match. For instance, to split sentences into words: sentences %&gt;% head(5) %&gt;% str_split(&quot; &quot;) ## [[1]] ## [1] &quot;The&quot; &quot;birch&quot; &quot;canoe&quot; &quot;slid&quot; &quot;on&quot; &quot;the&quot; &quot;smooth&quot; ## [8] &quot;planks.&quot; ## ## [[2]] ## [1] &quot;Glue&quot; &quot;the&quot; &quot;sheet&quot; &quot;to&quot; &quot;the&quot; ## [6] &quot;dark&quot; &quot;blue&quot; &quot;background.&quot; ## ## [[3]] ## [1] &quot;It&#39;s&quot; &quot;easy&quot; &quot;to&quot; &quot;tell&quot; &quot;the&quot; &quot;depth&quot; &quot;of&quot; &quot;a&quot; &quot;well.&quot; ## ## [[4]] ## [1] &quot;These&quot; &quot;days&quot; &quot;a&quot; &quot;chicken&quot; &quot;leg&quot; &quot;is&quot; &quot;a&quot; ## [8] &quot;rare&quot; &quot;dish.&quot; ## ## [[5]] ## [1] &quot;Rice&quot; &quot;is&quot; &quot;often&quot; &quot;served&quot; &quot;in&quot; &quot;round&quot; &quot;bowls.&quot; 17.1.4 Extracting attributes from text Handling free text in data pipelines and or statistical models is tricky. Frequently we extract attributes from text in order to perform analysis. We draw from https://www.tidytextmining.com/tidytext.html for this discussion. We usually think of text datasets (called a text corpus) in terms of documents: the instances of free text in our dataset, and terms the specific, e.g., words, they contain. In terms of the representation models we have used so far, we can think of documents as entities, described by attributes based on words, or words as entitites, described by attributes based on documents. To tidy text data, we tend to create one-token-per-row data frames that list the instances of terms in documents in a dataset Here’s a simple example using Jane Austen text library(janeaustenr) library(tidyverse) original_books &lt;- austen_books() %&gt;% group_by(book) %&gt;% mutate(linenumber = row_number(), chapter = cumsum(str_detect(text, regex(&quot;^chapter [\\\\divxlc]&quot;, ignore_case=TRUE)))) %&gt;% ungroup() original_books ## # A tibble: 73,422 x 4 ## text book linenumber chapter ## &lt;chr&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; ## 1 SENSE AND SENSIBILITY Sense &amp; Sensibility 1 0 ## 2 &quot;&quot; Sense &amp; Sensibility 2 0 ## 3 by Jane Austen Sense &amp; Sensibility 3 0 ## 4 &quot;&quot; Sense &amp; Sensibility 4 0 ## 5 (1811) Sense &amp; Sensibility 5 0 ## 6 &quot;&quot; Sense &amp; Sensibility 6 0 ## 7 &quot;&quot; Sense &amp; Sensibility 7 0 ## 8 &quot;&quot; Sense &amp; Sensibility 8 0 ## 9 &quot;&quot; Sense &amp; Sensibility 9 0 ## 10 CHAPTER 1 Sense &amp; Sensibility 10 1 ## # … with 73,412 more rows Let’s re-structure it as a one-token-per-row column using the unnest_tokens function in the tidytext package library(tidytext) tidy_books &lt;- original_books %&gt;% unnest_tokens(word, text) tidy_books ## # A tibble: 725,055 x 4 ## book linenumber chapter word ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 Sense &amp; Sensibility 1 0 sense ## 2 Sense &amp; Sensibility 1 0 and ## 3 Sense &amp; Sensibility 1 0 sensibility ## 4 Sense &amp; Sensibility 3 0 by ## 5 Sense &amp; Sensibility 3 0 jane ## 6 Sense &amp; Sensibility 3 0 austen ## 7 Sense &amp; Sensibility 5 0 1811 ## 8 Sense &amp; Sensibility 10 1 chapter ## 9 Sense &amp; Sensibility 10 1 1 ## 10 Sense &amp; Sensibility 13 1 the ## # … with 725,045 more rows Let’s remove stop words from the data frame data(stop_words) tidy_books &lt;- tidy_books %&gt;% anti_join(stop_words, by=&quot;word&quot;) tidy_books ## # A tibble: 217,609 x 4 ## book linenumber chapter word ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 Sense &amp; Sensibility 1 0 sense ## 2 Sense &amp; Sensibility 1 0 sensibility ## 3 Sense &amp; Sensibility 3 0 jane ## 4 Sense &amp; Sensibility 3 0 austen ## 5 Sense &amp; Sensibility 5 0 1811 ## 6 Sense &amp; Sensibility 10 1 chapter ## 7 Sense &amp; Sensibility 10 1 1 ## 8 Sense &amp; Sensibility 13 1 family ## 9 Sense &amp; Sensibility 13 1 dashwood ## 10 Sense &amp; Sensibility 13 1 settled ## # … with 217,599 more rows Now, we can use this dataset to compute attributes for entities of interest. For instance, let’s create a data frame with words as entities, with an attribute containing the number of times the word appears in this corpus frequent_words &lt;- tidy_books %&gt;% count(word, sort=TRUE) %&gt;% filter(n &gt; 600) Which can then use like other data frames as we have used previously. For example to plot most frequent words: frequent_words %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot(aes(x=word, y=n)) + geom_col() + theme_bw() + labs(x=NULL, y=&quot;frequency&quot;) + coord_flip() 17.2 Handling dates The lubridate package provides common operations for parsing and operating on dates and times. See http://r4ds.had.co.nz/dates-and-times.html for more information. A number of functions for parsing dates in a variety of formats are provided, along with functions to extract specific components from parsed date objects library(lubridate) datetime &lt;- ymd_hms(&quot;2016-07-08 12:34:56&quot;) year(datetime) ## [1] 2016 month(datetime) ## [1] 7 day(datetime) ## [1] 8 mday(datetime) ## [1] 8 yday(datetime) ## [1] 190 wday(datetime) ## [1] 6 They can also return month and day of the week names, abbreviated, as ordered factors month(datetime, label=TRUE) ## [1] Jul ## 12 Levels: Jan &lt; Feb &lt; Mar &lt; Apr &lt; May &lt; Jun &lt; Jul &lt; Aug &lt; Sep &lt; ... &lt; Dec We can also create attributes of type datetime from string attributes. Here’s an example using the flights dataset flights_with_dt &lt;- flights %&gt;% mutate(dep_dt=make_datetime(year, month, day, dep_time %/% 100, dep_time %% 100)) %&gt;% dplyr::select(year, month, day, dep_time, dep_dt) flights_with_dt ## # A tibble: 336,776 x 5 ## year month day dep_time dep_dt ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dttm&gt; ## 1 2013 1 1 517 2013-01-01 05:17:00 ## 2 2013 1 1 533 2013-01-01 05:33:00 ## 3 2013 1 1 542 2013-01-01 05:42:00 ## 4 2013 1 1 544 2013-01-01 05:44:00 ## 5 2013 1 1 554 2013-01-01 05:54:00 ## 6 2013 1 1 554 2013-01-01 05:54:00 ## 7 2013 1 1 555 2013-01-01 05:55:00 ## 8 2013 1 1 557 2013-01-01 05:57:00 ## 9 2013 1 1 557 2013-01-01 05:57:00 ## 10 2013 1 1 558 2013-01-01 05:58:00 ## # … with 336,766 more rows With this attribute in place we can extract day of the week and plot the number of flights per day of the week flights_with_dt %&gt;% mutate(wday=wday(dep_dt, label=TRUE)) %&gt;% ggplot(aes(x=wday)) + geom_bar() "],
["part-exploratory-data-analysis.html", "(Part) Exploratory Data Analysis", " (Part) Exploratory Data Analysis "],
["exploratory-data-analysis-visualization.html", "18 Exploratory Data Analysis: Visualization 18.1 Visualization of single variables 18.2 EDA with the grammar of graphics", " 18 Exploratory Data Analysis: Visualization We are now entering the last step of what we would want to do with a dataset before starting modeling using statistics or Machine Learning. We have seen manipulations and operations that prepare datasets into tidy (or normal form), compute summaries, and join tables to obtain organized, clean data tables that contain the observational units, or entities, we want to model statistically. At this point, we want to perform Exploratory Data Analysis to better understand the data at hand, and help us make decisions about appropriate statistical or Machine Learning methods, or data transformations that may be helpful to do. Moreover, there are many instances where statistical data modeling is not required to tell a clear and convincing story with data. Many times an effective visualization can lead to convincing conclusions. 18.0.1 EDA (Exploratory Data Analysis) The goal of EDA is to perform an initial exploration of attributes/variables across entities/observations. In this section, we will concentrate on exploration of single or pairs of variables. Later on in the course we will see dimensionality reduction methods that are useful in exploration of more than two variables at a time. We will concentrate on computing summary statistics, and how to interpret them in order to understand properties of variables. To go along with that, we will consider some data transformations we can apply to change properties of variables to help in visualization or modeling. But first, we will discuss how to use visualization for exploratory data analysis. Ultimately, the purpose of EDA is to spot problems in data (as part of data wrangling) and understand variable properties like: central trends (mean) spread (variance) skew outliers This will help us think of possible modeling strategies (e.g., probability distributions) We also want to use EDA to understand relationships between pairs of variables, e.g. their correlation or covariance. 18.1 Visualization of single variables Let’s begin by using R’s basic graphics capabilities which are great for creating quick plots especially for EDA. We will then see ggplot2 that requires you to be a bit more thoughtful on data exploration that can lead to good ideas about analysis and modeling. Let’s start with a very simple visualization of values of the dep_delay attribute in the flights dataset. library(nycflights13) flights %&gt;% sample_frac(.1) %&gt;% rowid_to_column() %&gt;% ggplot(aes(x=rowid, y=dep_delay)) + geom_point() That’s not particularly informative since there is no structure to this plot. Let’s sort the values in the plot and change the graphical representation to make easier to see. flights %&gt;% sample_frac(.1) %&gt;% arrange(dep_delay) %&gt;% rowid_to_column() %&gt;% ggplot(aes(x=rowid, y=dep_delay)) + geom_point() What can we make of that plot now? Start thinking of central tendency, spread and skew as you look at that plot. Let’s now create a graphical summary of that variable to incorporate observations made from this initial plot. Let’s start with a histogram: it divides the range of the dep_delay attribute into equal-sized bins, then plots the number of observations within each bin. What additional information does this new visualization give us about this variable? flights %&gt;% ggplot(aes(x=dep_delay)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 8255 rows containing non-finite values (stat_bin). The nclass parameter controls the number of bins into which the dep_delay range is divided. Try changing that parameter and see what happens. Now, we can (conceptually) make the bins as small as possible and get a smooth curve that describes the distribution of values of the dep_delay variable. We call this a density plot: flights %&gt;% ggplot(aes(x=dep_delay)) + geom_density() ## Warning: Removed 8255 rows containing non-finite values (stat_density). Now, one more very useful way of succintly graphically summarizing the distribution of a variable is using a boxplot. flights %&gt;% ggplot(aes(x=&#39;&#39;,y=dep_delay)) + geom_boxplot() ## Warning: Removed 8255 rows containing non-finite values (stat_boxplot). That’s not very clear to see, so let’s do a transformation of this data to see this better: flights %&gt;% mutate(min_delay=min(dep_delay, na.rm=TRUE)) %&gt;% mutate(log_dep_delay = log(dep_delay - min_delay)) %&gt;% ggplot(aes(x=&#39;&#39;, y=log_dep_delay)) + geom_boxplot() ## Warning: Removed 8256 rows containing non-finite values (stat_boxplot). So what does this represent: (a) central tendency (using the median) is represented by the black line within the box, (b) spread (using inter-quartile range) is represented by the box and whiskers. (c) outliers (data that is unusually outside the spread of the data) We will see more formal descriptions of these summary statistics in the next section, but you can see what we are trying to capture with them graphically. 18.1.1 Visualization of pairs of variables Now we can start looking at the relationship between pairs of attributes. That is, how are each of the distributional properties we care about (central trend, spread and skew) of the values of an attribute changing based on the value of a different attribute. Suppose we want to see the relationship between dep_delay, a numeric variable, and origin, a categorical variable. Previously, we saw use the group_by-summarize operations to compute attribute summaries based on the value of another attribute. We also called this conditioning. In visualization we can start thinking about conditioning as we saw before. Here is how we can see a plot of the distribution of departure delays conditioned on origin airport. flights %&gt;% mutate(min_delay = min(dep_delay, na.rm=TRUE)) %&gt;% mutate(log_dep_delay = log(dep_delay - min_delay)) %&gt;% ggplot(aes(x=origin, y=log_dep_delay)) + geom_boxplot() ## Warning: Removed 8256 rows containing non-finite values (stat_boxplot). For pairs of continuous variables, the most useful visualization is the scatter plot. This gives an idea of how one variable varies (in terms of central trend, variance and skew) conditioned on another variable. flights %&gt;% sample_frac(.1) %&gt;% ggplot(aes(x=dep_delay, y=arr_delay)) + geom_point() ## Warning: Removed 904 rows containing missing values (geom_point). 18.2 EDA with the grammar of graphics While we have seen a basic repertoire of graphics it’s easier to proceed if we have a bit more formal way of thinking about graphics and plots. Here is where we will use the grammar of graphics implemented in R by the package ggplot2. The central premise is to characterize the building pieces behind plots: The data that goes into a plot, works best when data is tidy The mapping between data and aesthetic attributes The geometric representation of these attributes Let’s start with a simple example: library(dplyr) library(ggplot2) library(Lahman) batting &lt;- tbl_df(Batting) # scatter plot of at bats vs. runs for 2010 batting %&gt;% filter(yearID == &quot;2010&quot;) %&gt;% ggplot(aes(x=AB, y=R)) + geom_point() Data: Batting table filtering for year Aesthetic attributes: - x-axis mapped to variables AB - y-axis mapped to variable R Geometric Representation: points! Now, you can cleanly distinguish the constituent parts of the plot. E.g., change the geometric representation # scatter plot of at bats vs. runs for 2010 batting %&gt;% filter(yearID == &quot;2010&quot;) %&gt;% ggplot(aes(x=AB, y=R, label=teamID)) + geom_text() + geom_point() E.g., change the data. # scatter plot of at bats vs. runs for 1995 batting %&gt;% filter(yearID == &quot;1995&quot;) %&gt;% ggplot(aes(x=AB, y=R)) + geom_point() E.g., change the aesthetic. # scatter plot of at bats vs. hits for 2010 batting %&gt;% filter(yearID == &quot;2010&quot;) %&gt;% ggplot(aes(x=AB, y=H)) + geom_point() Let’s make a line plot What do we change? (data, aesthetic or geometry?) batting %&gt;% filter(yearID == &quot;2010&quot;) %&gt;% sample_n(100) %&gt;% ggplot(aes(x=AB, y=H)) + geom_line() Let’s add a regression line What do we add? (data, aesthetic or geometry?) batting %&gt;% filter(yearID == &quot;2010&quot;) %&gt;% ggplot(aes(x=AB, y=H)) + geom_point() + geom_smooth(method=lm) What can we see about central trend, variation and skew with this plot? 18.2.1 Other aesthetics Using other aesthetics we can incorporate information from other variables. Color: color by categorical variable batting %&gt;% filter(yearID == &quot;2010&quot;) %&gt;% ggplot(aes(x=AB, y=H, color=lgID)) + geom_point() + geom_smooth(method=lm) Size: size by (discrete) numeric variable batting %&gt;% filter(yearID == &quot;2010&quot;) %&gt;% ggplot(aes(x=AB, y=R, size=HR)) + geom_point() + geom_smooth(method=lm) 18.2.2 Faceting The last major component of exploratory analysis called faceting in visualization, corresponds to conditioning in statistical modeling, we’ve seen it as the motivation of grouping when wrangling data. batting %&gt;% filter(yearID %in% c(&quot;1995&quot;, &quot;2000&quot;, &quot;2010&quot;)) %&gt;% ggplot(aes(x=AB, y=R, size=HR)) + facet_grid(lgID~yearID) + geom_point() + geom_smooth(method=lm) "],
["exploratory-data-analysis-summary-statistics.html", "19 Exploratory Data Analysis: Summary Statistics 19.1 Range 19.2 Central Tendency 19.3 Spread 19.4 Outliers 19.5 Skew 19.6 Covariance and correlation 19.7 Postscript: Finding Maxima/Minima using Derivatives", " 19 Exploratory Data Analysis: Summary Statistics Let’s continue our discussion of Exploratory Data Analysis. In the previous section we saw ways of visualizing attributes (variables) using plots to start understanding properties of how data is distributed, an essential and preliminary step in data analysis. In this section, we start discussing statistical, or numerical, summaries of data to quantify properties that we observed using visual summaries and representations. Remember that one purpose of EDA is to spot problems in data (as part of data wrangling) and understand variable properties like: central trends (mean) spread (variance) skew suggest possible modeling strategies (e.g., probability distributions) We also want to use EDA to understand relationship between pairs of variables, e.g. their correlation or covariance. One last note on EDA. John W. Tukey was an exceptional scientist/mathematician, who had profound impact on statistics and Computer Science. A lot of what we cover in EDA is based on his groundbreaking work. I highly recommend you read more about him: https://www.stat.berkeley.edu/~brill/Papers/life.pdf. 19.1 Range Part of our goal is to understand how variables are distributed in a given dataset. Note, again, that we are not using distributed in a formal mathematical (or probabilistic) sense. All statements we are making here are based on data at hand, so we could refer to this as the empirical distribution of data. Here, empirical is used in the sense that this is data resulting from an experiment. Let’s use a dataset on diamond characteristics as an example. data(diamonds) diamonds %&gt;% ggplot(aes(x=depth)) + geom_histogram(bins=100) (Here’s some help interpreting these variables: https://en.wikipedia.org/wiki/Diamond_(gemstone)#Gemological_characteristics). Let’s start using some notation to make talking about this a bit more efficient. We assume that we have data across \\(n\\) entitites (or observational units) for \\(p\\) attributes. In this dataset \\(n=53940\\) and \\(p=10\\). However, let’s consider a single attribute, and denote the data for that attribute (or variable) as \\(x_1, x_2, \\ldots, x_n\\). Ok, so what’s the first question we want to ask about how data is distributed? Since we want to understand how data is distributed across a range, we should first define the range. diamonds %&gt;% summarize(min_depth = min(depth), max_depth = max(depth)) ## # A tibble: 1 x 2 ## min_depth max_depth ## &lt;dbl&gt; &lt;dbl&gt; ## 1 43 79 We use notation \\(x_{(1)}\\) and \\(x_{(n)}\\) to denote the minimum and maximum statistics. In general, we use notation \\(x_{(q)}\\) for the rank statistics, e.g., the \\(q\\)th largest value in the data. 19.2 Central Tendency Now that we know the range over which data is distributed, we can figure out a first summary of data is distributed across this range. Let’s start with the center of the data: the median is a statistic defined such that half of the data has a smaller value. We can use notation \\(x_{(n/2)}\\) (a rank statistic) to represent the median. Note that we can use an algorithm based on the quicksort partition scheme to compute the median in linear time (on average). diamonds %&gt;% ggplot(aes(x=depth)) + geom_histogram(bins=100) + geom_vline(aes(xintercept=median(depth)), color=&quot;red&quot;) 19.2.1 Derivation of the mean as central tendency statistic Of course, the best known statistic for central tendency is the mean, or average of the data: \\(\\overline{x} = \\frac{1}{n} \\sum_{i=1}^n x_i\\). It turns out that in this case, we can be a bit more formal about “center” means in this case. Let’s say that the center of a dataset is a point in the range of the data that is close to the data. To say that something is close we need a measure of distance. So for two points \\(x_1\\) and \\(x_2\\) what should we use for distance? We could base it on \\((x_1 - x_2)\\) but that’s not enough since its sign depends on the order in which we write it. Using the absolute value solves that problem \\(|x_1 - x_2|\\) since now the sign doesn’t matter, but this has some issues that we will see later. So, next best thing we can do is use the square of the difference. So, in this case, the distance between data point \\(x_1\\) and \\(x_2\\) is \\((x_1 - x_2)^2\\). Here is a fun question: what’s the largest distance between two points in our dataset? So, to define the center, let’s build a criterion based on this distance by adding this distance across all points in our dataset: \\[ RSS(\\mu) = \\frac{1}{2} \\sum_{i=1}^n (x_i - \\mu)^2 \\] Here RSS means residual sum of squares, and we \\(\\mu\\) to stand for candidate values of center. We can plot RSS for different values of \\(\\mu\\): Now, what should our “center” estimate be? We want a value that is close to the data based on RSS! So we need to find the value in the range that minimizes RSS. From calculus, we know that a necessary condition for the minimizer \\(\\hat{\\mu}\\) of RSS is that the derivative of RSS is zero at that point. So, the strategy to minimize RSS is to compute its derivative, and find the value of \\(\\mu\\) where it equals zero. So, let’s find the derivative of RSS: \\[ \\begin{eqnarray} \\frac{\\partial}{\\partial \\mu} \\frac{1}{2} \\sum_{i=1}^n (x_i - \\mu)^2 &amp; = &amp; \\frac{1}{2} \\sum_{i=1}^n \\frac{\\partial}{\\partial \\mu} (x_i - \\mu)^2 \\; \\textrm{(sum rule)}\\\\ {} &amp; = &amp; \\frac{1}{2} \\sum_{i=1}^n 2(x_i - \\mu) \\times \\frac{\\partial}{\\partial \\mu} (x_i - \\mu) \\; \\textrm{(power rule and chain rule)}\\\\ {} &amp; = &amp; \\frac{1}{2} \\sum_{i=1}^n 2(x_i - \\mu) \\times (-1) \\; \\textrm{(sum rule and power rule)}\\\\ {} &amp; = &amp; \\frac{1}{2} 2 \\sum_{i=1}^n (\\mu - x_i) \\textrm{(rearranging)}\\\\ {} &amp; = &amp; \\sum_{i=1}^n \\mu - \\sum_{i=1}^n x_i \\\\ {} &amp; = &amp; n\\mu - \\sum_{i=1}^n x_i \\end{eqnarray} \\] Next, we set that equal to zero and find the value of \\(\\mu\\) that solves that equation: \\[ \\begin{eqnarray} \\frac{\\partial}{\\partial \\mu} &amp; = &amp; 0 &amp; \\Rightarrow \\\\ n\\mu - \\sum_{i=1}^n x_i &amp; = &amp; 0 &amp; \\Rightarrow \\\\ n\\mu &amp; = &amp; \\sum_{i=1}^n x_i &amp; \\Rightarrow \\\\ \\mu &amp; = &amp; \\frac{1}{n} \\sum_{i=1}^n x_i &amp; {} \\end{eqnarray} \\] That’s the average we know and love! So the fact you should remember: The mean is the value that minimizes RSS for a vector of attribute values It equals the value where the derivative of RSS is 0: It is the value that minimizes RSS: And it serves as an estimate of central tendency of the dataset: Note that in this dataset the mean and median are not exactly equal, but are very close: diamonds %&gt;% summarize(mean_depth = mean(depth), median_depth = median(depth)) ## # A tibble: 1 x 2 ## mean_depth median_depth ## &lt;dbl&gt; &lt;dbl&gt; ## 1 61.7 61.8 One last note, there is a similar argument to define the median as a measure of center. In this case, instead of using RSS we use a different criterion: the sum of absolute deviations \\[ SAD(m) = \\sum_{i=1}^n |x_i - m|. \\] The median is the minimizer of this criterion. 19.3 Spread Now that we have a measure of center, we can now discuss how data is spread around that center. 19.3.1 Variance For the mean, we have a convenient way of describing this: the average distance (using squared difference) from the mean. We call this the variance of the data: \\[ \\mathrm{var}(x) = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\overline{x})^2 \\] You will also see it with a slightly different constant in the front for technical reasons that we may discuss later on: \\[ \\mathrm{var}(x) = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\overline{x})^2 \\] Variance is a commonly used statistic for spread but it has the disadvantage that its units are not easy to conceptualize (e.g., squared diamond depth). A spread statistic that is in the same units as the data is the standard deviation, which is just the squared root of variance: \\[ \\mathrm{sd}(x) = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n (x_i - \\overline{x})^2} \\] We can also use standard deviations as an interpretable unit of how far a given data point is from the mean: # create a df with standard deviation values to plot sds_to_plot &lt;- seq(-6,6) sd_df &lt;- diamonds %&gt;% summarize(mean_depth = mean(depth), sd_depth = sd(depth)) %&gt;% slice(rep_along(sds_to_plot, 1)) %&gt;% mutate(sd_to_plot=sds_to_plot) %&gt;% mutate(sd_val = mean_depth + sd_to_plot * sd_depth) diamonds %&gt;% ggplot(aes(x=depth)) + geom_histogram(bins=100) + geom_vline(aes(xintercept=mean(depth)), col=&quot;blue&quot;, size=1.5) + geom_vline(aes(xintercept = sd_val), data=sd_df, linetype=2, size=1.2 - abs(seq(-1,1, len=13))) As a rough guide, we can use “standard deviations away from the mean” as a measure of spread as follows: SDs proportion Interpretation 1 0.68 68% of the data is within \\(\\pm\\) 1 sds 2 0.95 95% of the data is within \\(\\pm\\) 2 sds 3 0.9973 99.73% of the data is within \\(\\pm\\) 3 sds 4 0.999937 99.9937% of the data is within \\(\\pm\\) 4 sds 5 0.9999994 99.999943% of the data is within \\(\\pm\\) 5 sds 6 1 99.9999998% of the data is within \\(\\pm\\) 6 sds We will see later how these rough approximations are derived from a mathematical assumption about how data is distributed beyond the data we have at hand. 19.3.2 Spread estimates using rank statistics Just like we saw how the median is a rank statistic used to describe central tendency, we can also use rank statistics to describe spread. For this we use two more rank statistics: the first and third quartiles, \\(x_{(n/4)}\\) and \\(x_{(3n/4)}\\) respectively: quartile_df &lt;- diamonds %&gt;% summarize(first=quantile(diamonds$depth, p=1/4), third=quantile(diamonds$depth, p=3/4)) %&gt;% tidyr::gather(quartile, value) diamonds %&gt;% ggplot(aes(x=depth)) + geom_histogram(bins=100) + geom_vline(aes(xintercept=median(depth)), size=1.3, color=&quot;red&quot;) + geom_vline(aes(xintercept=value), data=quartile_df, size=1,color=&quot;red&quot;, linetype=2) Note, the five order statistics we have seen so far: minimum, maximum, median and first and third quartiles are so frequently used that this is exactly what R uses by default as a summary of a numeric vector of data (along with the mean): summary(diamonds$depth) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 43.00 61.00 61.80 61.75 62.50 79.00 This five-number summary are also all of the statistics used to construct a boxplot to summarize data distribution. In particular, the inter-quartile range, which is defined as the difference between the third and first quartile: \\(\\mathrm{IQR}(x) = x_{(3n/4)} - x_{(1n/4)}\\) gives a measure of spread. The interpretation here is that half the data is within the IQR around the median. diamonds %&gt;% summarize(sd_depth = sd(depth), iqr_depth = IQR(depth)) ## # A tibble: 1 x 2 ## sd_depth iqr_depth ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1.43 1.5 19.4 Outliers We can use estimates of spread to identify outlier values in a dataset. Given an estimate of spread based on the techniques we’ve just seen, we can identify values that are unusually far away from the center of the distribution. One often cited rule of thumb is based on using standard deviation estimates. We can identify outliers as the set \\[ \\mathrm{outliers_{sd}}(x) = \\{x_j \\, | \\, |x_j| &gt; \\overline{x} + k \\times \\mathrm{sd}(x) \\} \\] where \\(\\overline{x}\\) is the sample mean of the data and \\(\\mathrm{sd}(x)\\) it’s standard deviation. Multiplier \\(k\\) determines if we are identifying (in Tukey’s nomenclature) outliers or points that are far out. Here is an example usage: outlier_df &lt;- diamonds %&gt;% summarize(mean_depth=mean(depth), sd_depth=sd(depth)) %&gt;% slice(rep(1, 4)) %&gt;% mutate(multiplier = c(-3, -1.5, 1.5, 3)) %&gt;% mutate(outlier_limit = mean_depth + multiplier * sd_depth) diamonds %&gt;% ggplot(aes(x=depth)) + geom_histogram(bins=100) + geom_vline(aes(xintercept=outlier_limit), data=outlier_df, color=&quot;blue&quot;) While this method works relatively well in practice, it presents a fundamental problem. Severe outliers can significantly affect spread estimates based on standard deviation. Specifically, spread estimates will be inflated in the presence of severe outliers. To circumvent this problem, we use rank-based estimates of spread to identify outliers as: \\[ \\mathrm{outliers_{IQR}}(x) = \\{x_j \\, | \\, x_j &lt; x_{(1/4)} - k \\times \\mathrm{IQR}(x) \\; \\mathrm{ or } \\; x_j &gt; x_{(3/4)} + k \\times \\mathrm{IQR}(x)\\} \\] This is usually referred to as the Tukey outlier rule, with multiplier \\(k\\) serving the same role as before. We use the IQR here because it is less susceptible to be inflated by severe outliers in the dataset. It also works better for skewed data than the method based on standard deviation. Here we demonstrate its use again: outlier_df &lt;- diamonds %&gt;% summarize(q1=quantile(depth, 1/4), q3=quantile(depth, 3/4), iqr=IQR(depth)) %&gt;% slice(rep(1, 2)) %&gt;% mutate(multiplier = c(1.5, 3)) %&gt;% mutate(lower_outlier_limit = q1 - multiplier * iqr) %&gt;% mutate(upper_outlier_limit = q3 + multiplier * iqr) diamonds %&gt;% ggplot(aes(x=depth)) + geom_histogram(bins=100) + geom_vline(aes(xintercept=lower_outlier_limit), data=outlier_df, color=&quot;red&quot;) + geom_vline(aes(xintercept=upper_outlier_limit), data=outlier_df, color=&quot;red&quot;) 19.5 Skew One last thought. Although there are formal ways of defining this precisely, the five-number summary can be used to understand if data is skewed. How? Consider the differences between the first and third quartiles to the median: diamonds %&gt;% summarize(med_depth = median(depth), q1_depth = quantile(depth, 1/4), q3_depth = quantile(depth, 3/4)) %&gt;% mutate(d1_depth = med_depth - q1_depth, d2_depth = q3_depth - med_depth) %&gt;% select(d1_depth, d2_depth) ## # A tibble: 1 x 2 ## d1_depth d2_depth ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.800 0.7 If one of these differences is larger than the other, then that indicates that this dataset might be skewed, that is, that the range of data on one side of the median is longer (or shorter) than the range of data on the other side of the median. Do you think our diamond depth dataset is skewed? 19.6 Covariance and correlation The scatter plot is a visual way of observing relationships between pairs of variables. Like descriptions of distributions of single variables, we would like to construct statistics that summarize the relationship between two variables quantitatively. To do this we will extend our notion of spread (or variation of data around the mean) to the notion of co-variation: do pairs of variables vary around the mean in the same way. Consider now data for two variables over the same \\(n\\) entities: \\((x_1,y_1), (x_2,y_2), \\ldots, (x_n,y_n)\\). For example, for each diamond, we have carat and price as two variables: diamonds %&gt;% ggplot(aes(x=carat, y=price)) + geom_point() + geom_hline(aes(yintercept = mean(price)), color=&quot;blue&quot;, lty=2) + geom_vline(aes(xintercept = mean(carat)), color=&quot;blue&quot;, lty=2) We want to capture the relationship: does \\(x_i\\) vary in the same direction and scale away from its mean as \\(y_i\\)? This leads to covariance \\[ cov(x,y) = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\overline{x})(y_i - \\overline{y}) \\] Think of what would the covariance for \\(x\\) and \\(y\\) be if \\(x_i\\) varies in the opposite direction as \\(y_i\\)? Just like variance, we have an issue with units and interpretation for covariance, so we introduce correlation (formally, Pearson’s correlation coefficient) to summarize this relationship in a unit-less way: \\[ cor(x,y) = \\frac{cov(x,y)}{sd(x) sd(y)} \\] As before, we can also use rank statistics to define a measure of how two variables are associated. One of these, Spearman correlation is commonly used. It is defined as the Pearson correlation coefficient of the ranks (rather than actual values) of pairs of variables. 19.7 Postscript: Finding Maxima/Minima using Derivatives The values at which a function attains its maximum value are called maxima ( maximum if unique) of the function. Similarly, the values at which a function attains its minimum value are called minima ( minimum if unique) of the function. In a smoothly changing function maxima or minima are found where the function flattens (slope becomes \\(0\\)). The first derivative of the function tells us where the slope is \\(0\\). This is the first derivate test. The derivate of the slope (the second derivative of the original function) can be useful to know if the value we found from first derivate test is a maxima or minima. When a function’s slope is zero at \\(x\\), and the second derivative at \\(x\\) is: This is called the second derivate test. 19.7.1 Steps to find Maxima/Minima of function \\(f(x)\\) Find the value(s) at which \\(f&#39;(x)=0\\) (First derivative test). Find the value of the second derivative for each of the x’s found in step 1 (Second derivative test). If the value of the second derivative at \\(x\\) is: - less than 0, it is a local maximum - greater than 0, it is a local minimum - equal to 0, then the test fails (no minima or maxima) 19.7.2 Notes on Finding Derivatives Sum Rule The derivative of the sum of two functions is the sum of the derivatives of the two functions: \\[\\begin{eqnarray*} \\frac{d}{dx}(f(x)+g(x)) = \\frac{d}{dx}(f(x)) + \\frac{d}{dx}(g(x)) \\end{eqnarray*}\\] Similarly, the derivative of the difference of two functions is the difference of the derivatives of the two functions. Power Rule If we have a function f(x) of the form \\(f(x)=x^{n}\\) for any integer n, \\[\\begin{eqnarray*} \\frac{d}{dx}(f(x)) = \\frac{d}{dx}(x^{n}) = nx^{n-1} \\end{eqnarray*}\\] Chain Rule If we have two functions of the form \\(f(x)\\) and \\(g(x)\\), the chain rule can be stated as follows: \\[\\begin{eqnarray*} \\frac{d}{dx}(f(g(x)) = f^{&#39;}(g(x)) g^{&#39;}(x) \\end{eqnarray*}\\] Differentiate \\(y=(3x+1)^{2}\\) with respect to x.\\ Applying the above equation, we have the following: \\[\\begin{eqnarray*} \\frac{d}{dx}((3x+1)^{2}) = 2(3x+1)^{2-1} \\frac{d}{dx}((3x+1)) = 2(3x+1)(3) = 6(3x+1) \\end{eqnarray*}\\] Product Rule If we have two functions f(x) and g(x), \\[\\begin{eqnarray*} \\frac{d}{dx}(f(x)g(x)) = f(x)\\frac{d}{dx}(g(x)) + g(x)\\frac{d}{dx}(f(x)) = f(x)g&#39;(x) + g(x)f&#39;(x) \\end{eqnarray*}\\] Quotient Rule If we have two functions f(x) and g(x) (\\(g(x)\\neq 0\\)), \\[\\begin{eqnarray*} \\frac{d}{dx}\\frac{f(x)}{g(x)} = \\left(\\frac{g(x)\\frac{d}{dx} (f(x)) - f(x)\\frac{d}{dx} (g(x)) }{g(x)^{2}}\\right) \\end{eqnarray*}\\] 19.7.3 Resources: A useful calculus cheat sheet: http://tutorial.math.lamar.edu/pdf/Calculus_Cheat_Sheet_Derivatives.pdf Discussion on finding maxima/minima: https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;ved=0ahUKEwi32ZGPvbbPAhUCdj4KHcdyDZAQFggnMAI&amp;url=http%3A%2F%2Fwww.math.psu.edu%2Ftseng%2Fclass%2FMath140A%2FNotes-First_and_Second_Derivative_Tests.doc&amp;usg=AFQjCNEUih6RsfXq933pFwmoPk0yOvc1Mg&amp;sig2=zyxh1-zWe7TY7zYwnhpH8g&amp;cad=rja "],
["eda-data-transformations.html", "20 EDA: Data Transformations 20.1 Centering and scaling 20.2 Treating categorical variables as numeric 20.3 Skewed Data", " 20 EDA: Data Transformations Having a sense of how data is distributed, both from using visual or quantitative summaries, we can consider transformations of variables to ease both interpretation of data analyses and the application statistical and machine learning models to a dataset. 20.1 Centering and scaling A very common and important transformation is to scale data to a common unit-less scale. Informally, you can think of this as transforming variables from whatever units they are measured (e.g., diamond depth percentage) into “standard deviations away from the mean” units (actually called standard units, or \\(z\\)-score). Given data \\(x = x_1, x_2, \\ldots, x_n\\), the transformation applied to obtain centered and scaled variable \\(z\\) is: \\[ z_i = \\frac{(x_i - \\overline{x})}{\\mathrm{sd}(x)} \\] where \\(\\overline{x}\\) is the mean of data \\(x\\), and \\(\\mathrm{sd}(x)\\) is its standard deviation. library(ggplot2) data(diamonds) diamonds %&gt;% mutate(scaled_depth = (depth - mean(depth)) / sd(depth)) %&gt;% ggplot(aes(x=scaled_depth)) + geom_histogram(binwidth=.5) Question: what is the mean of \\(z\\)? What is it’s standard deviation? Another name for this transformation is to standardize a variable. One useful result of applying this transformation to variables in a dataset is that all variables are in the same, and thus comparable units. On occasion, you will have use to apply transformations that only center (but not scale) data: \\[ z_i = (x_i - \\overline{x}) \\] Question: what is the mean of \\(z\\) in this case? What is it’s standard deviation? Or, apply transformations that only scale (but not center) data: \\[ z_i = \\frac{x_i}{\\mathrm{sd}(x)} \\] Question: what is the mean of \\(z\\) in this case? What is it’s standard deviation? 20.2 Treating categorical variables as numeric Many modeling algorithms work strictly on numeric measurements. For example, we will see methods to predict some variable given values for other variables such as linear regression or support vector machines, that are strictly defined for numeric measurements. In this case, we would need to transform categorical variables into something that we can treat as numeric. We will see more of this in later sections of the course but let’s see a couple of important guidelines for binary variables (categorical variables that only take two values, e.g., health_insurance). One option is to encode one value of the variable as 1 and the other as 0. For instance: library(ISLR) library(tidyverse) data(Wage) Wage %&gt;% mutate(numeric_insurace = ifelse(health_ins == &quot;1. Yes&quot;, 1, 0)) %&gt;% head() ## year age maritl race education region ## 1 2006 18 1. Never Married 1. White 1. &lt; HS Grad 2. Middle Atlantic ## 2 2004 24 1. Never Married 1. White 4. College Grad 2. Middle Atlantic ## 3 2003 45 2. Married 1. White 3. Some College 2. Middle Atlantic ## 4 2003 43 2. Married 3. Asian 4. College Grad 2. Middle Atlantic ## 5 2005 50 4. Divorced 1. White 2. HS Grad 2. Middle Atlantic ## 6 2008 54 2. Married 1. White 4. College Grad 2. Middle Atlantic ## jobclass health health_ins logwage wage ## 1 1. Industrial 1. &lt;=Good 2. No 4.318063 75.04315 ## 2 2. Information 2. &gt;=Very Good 2. No 4.255273 70.47602 ## 3 1. Industrial 1. &lt;=Good 1. Yes 4.875061 130.98218 ## 4 2. Information 2. &gt;=Very Good 1. Yes 5.041393 154.68529 ## 5 2. Information 1. &lt;=Good 1. Yes 4.318063 75.04315 ## 6 2. Information 2. &gt;=Very Good 1. Yes 4.845098 127.11574 ## numeric_insurace ## 1 0 ## 2 0 ## 3 1 ## 4 1 ## 5 1 ## 6 1 Another option is to encode one value as 1 and the other as -1: Wage %&gt;% mutate(numeric_insurance = ifelse(health_ins == &quot;1. Yes&quot;, 1, -1)) %&gt;% head() ## year age maritl race education region ## 1 2006 18 1. Never Married 1. White 1. &lt; HS Grad 2. Middle Atlantic ## 2 2004 24 1. Never Married 1. White 4. College Grad 2. Middle Atlantic ## 3 2003 45 2. Married 1. White 3. Some College 2. Middle Atlantic ## 4 2003 43 2. Married 3. Asian 4. College Grad 2. Middle Atlantic ## 5 2005 50 4. Divorced 1. White 2. HS Grad 2. Middle Atlantic ## 6 2008 54 2. Married 1. White 4. College Grad 2. Middle Atlantic ## jobclass health health_ins logwage wage ## 1 1. Industrial 1. &lt;=Good 2. No 4.318063 75.04315 ## 2 2. Information 2. &gt;=Very Good 2. No 4.255273 70.47602 ## 3 1. Industrial 1. &lt;=Good 1. Yes 4.875061 130.98218 ## 4 2. Information 2. &gt;=Very Good 1. Yes 5.041393 154.68529 ## 5 2. Information 1. &lt;=Good 1. Yes 4.318063 75.04315 ## 6 2. Information 2. &gt;=Very Good 1. Yes 4.845098 127.11574 ## numeric_insurance ## 1 -1 ## 2 -1 ## 3 1 ## 4 1 ## 5 1 ## 6 1 The decision of which of these two transformations to use is based on the method to use or the goal of your analysis. For instance, when predicting someone’s wage based on their health insurance status, the 0/1 encoding let’s us make statements like: “on average, wage increases by $XX if a person has health insurance”. On the other hand, a prediction algorithm called a Support Vector Machine is strictly defined on data coded as 1/-1. For categorical attributes with more than two values, we extend this idea and encode each value of the categorical variable as a 0/1 column. You will see this referred to as one-hot-encoding. Wage %&gt;% mutate(race_white = ifelse(race == &quot;1. White&quot;, 1, 0), race_black = ifelse(race == &quot;2. Black&quot;, 1, 0), race_asian = ifelse(race == &quot;3. Asian&quot;, 1, 0), race_other = ifelse(race == &quot;4. Other&quot;, 1, 0)) %&gt;% select(starts_with(&quot;race&quot;)) %&gt;% head() ## race race_white race_black race_asian race_other ## 1 1. White 1 0 0 0 ## 2 1. White 1 0 0 0 ## 3 1. White 1 0 0 0 ## 4 3. Asian 0 0 1 0 ## 5 1. White 1 0 0 0 ## 6 1. White 1 0 0 0 The builtin function model.matrix does this general transformation. We will see it when we look at statistical and Machine Learning models. 20.2.1 Discretizing continuous values. How about transforming data in the other direction, from continuous to discrete values. This can make it easier to compare differences related to continuous measurements: Do doctors prescribe a certain medication to older kids more often? Is there a difference in wage based on age? It is also a useful way of capturing non-linear relationships in data: we will see this in our regression and prediction unit. Two standard methods used for discretization are to use equal-length bins, where variable range is divided into bins regardless of the data distribution: flights %&gt;% mutate(dep_delay_discrete = cut(dep_delay, breaks=100)) %&gt;% ggplot(aes(x=dep_delay_discrete)) + geom_bar() The second approach uses equal-sized bins, where the range is divided into bins based on data distribution flights %&gt;% mutate(dep_delay_discrete = cut(dep_delay, breaks=quantile(dep_delay, probs=seq(0,1,len=11), na.rm=TRUE))) %&gt;% ggplot(aes(x=dep_delay_discrete)) + geom_bar() In both cases, the cut function is used to apply discretization, with the breaks argument determining which method is applied. In the first example, breaks=100 specifies that 100 bins of equal-length are to be used. In the second example, the quantile function is used to define 10 equal-sized bins. 20.3 Skewed Data In many data analysis, variables will have a skewed distribution over their range. In the last section we saw one way of defining skew using quartiles and median. Variables with skewed distributions can be hard to incorporate into some modeling procedures, especially in the presence of other variables that are not skewed. In this case, applying a transformation to reduce skew will improve performance of models. Also, skewed data may arise when measuring multiplicative processes. This is very common in physical or biochemical processes. In this case, interpretation of data may be more intiuitive after a transformation. We have seen an example of skewed data previously when we looked at departure delays in our flights dataset. flights %&gt;% ggplot(aes(x=dep_delay)) + geom_histogram(binwidth=30) ## Warning: Removed 8255 rows containing non-finite values (stat_bin). Previously, we looked at a way of determining skew for a dataset. Let’s see what that looks like for the dep_delay variable: (see dplyr vignette for info on ‘enquo’ and ‘!!’) compute_skew_stat &lt;- function(df, attribute) { attribute &lt;- enquo(attribute) df %&gt;% summarize(med_attr=median(!!attribute, na.rm=TRUE), q1_attr=quantile(!!attribute, 1/4, na.rm=TRUE), q3_attr=quantile(!!attribute, 3/4, na.rm=TRUE)) %&gt;% mutate(d1 = med_attr - q1_attr, d2 = q3_attr - med_attr, skew_stat = d1 - d2) %&gt;% select(d1, d2, skew_stat) } flights %&gt;% compute_skew_stat(dep_delay) ## # A tibble: 1 x 3 ## d1 d2 skew_stat ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3 13 -10 In many cases a logarithmic transform is an appropriate transformation to reduce data skew: If values are all positive: apply log2 transform If some values are negative, two options Started Log: shift all values so they are positive, apply log2 Signed Log: \\(sign(x) \\times log2(abs(x) + 1)\\). Here is a signed log transformation of departure delay data: transformed_flights &lt;- flights %&gt;% mutate(transformed_dep_delay = sign(dep_delay) * log2(abs(dep_delay) + 1)) transformed_flights %&gt;% ggplot(aes(x=transformed_dep_delay)) + geom_histogram(binwidth=1) ## Warning: Removed 8255 rows containing non-finite values (stat_bin). Let’s see if that reduced the skew of the dataset: transformed_flights %&gt;% compute_skew_stat(transformed_dep_delay) ## # A tibble: 1 x 3 ## d1 d2 skew_stat ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 5.17 -4.17 "],
["eda-handling-missing-data.html", "21 EDA: Handling Missing Data", " 21 EDA: Handling Missing Data We can now move on to a very important aspect of data preparation and transformation: how to deal with missing data? By missing data we mean values that are unrecorded, unknown or unspecified in a dataset. We saw an example of this when we looked at the tidy unit. Here is the tidy weather dataset again: ## # A tibble: 22 x 35 ## id year month element d1 d2 d3 d4 d5 d6 d7 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 MX17… 2010 1 tmax NA NA NA NA NA NA NA ## 2 MX17… 2010 1 tmin NA NA NA NA NA NA NA ## 3 MX17… 2010 2 tmax NA 27.3 24.1 NA NA NA NA ## 4 MX17… 2010 2 tmin NA 14.4 14.4 NA NA NA NA ## 5 MX17… 2010 3 tmax NA NA NA NA 32.1 NA NA ## 6 MX17… 2010 3 tmin NA NA NA NA 14.2 NA NA ## 7 MX17… 2010 4 tmax NA NA NA NA NA NA NA ## 8 MX17… 2010 4 tmin NA NA NA NA NA NA NA ## 9 MX17… 2010 5 tmax NA NA NA NA NA NA NA ## 10 MX17… 2010 5 tmin NA NA NA NA NA NA NA ## # … with 12 more rows, and 24 more variables: d8 &lt;dbl&gt;, d9 &lt;lgl&gt;, ## # d10 &lt;dbl&gt;, d11 &lt;dbl&gt;, d12 &lt;lgl&gt;, d13 &lt;dbl&gt;, d14 &lt;dbl&gt;, d15 &lt;dbl&gt;, ## # d16 &lt;dbl&gt;, d17 &lt;dbl&gt;, d18 &lt;lgl&gt;, d19 &lt;lgl&gt;, d20 &lt;lgl&gt;, d21 &lt;lgl&gt;, ## # d22 &lt;lgl&gt;, d23 &lt;dbl&gt;, d24 &lt;lgl&gt;, d25 &lt;dbl&gt;, d26 &lt;dbl&gt;, d27 &lt;dbl&gt;, ## # d28 &lt;dbl&gt;, d29 &lt;dbl&gt;, d30 &lt;dbl&gt;, d31 &lt;dbl&gt; And the result of tidying this dataset: tidy_weather &lt;- weather %&gt;% gather(day, temp, d1:d31) %&gt;% spread(element, temp) tidy_weather ## # A tibble: 341 x 6 ## id year month day tmax tmin ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 MX17004 2010 1 d1 NA NA ## 2 MX17004 2010 1 d10 NA NA ## 3 MX17004 2010 1 d11 NA NA ## 4 MX17004 2010 1 d12 NA NA ## 5 MX17004 2010 1 d13 NA NA ## 6 MX17004 2010 1 d14 NA NA ## 7 MX17004 2010 1 d15 NA NA ## 8 MX17004 2010 1 d16 NA NA ## 9 MX17004 2010 1 d17 NA NA ## 10 MX17004 2010 1 d18 NA NA ## # … with 331 more rows In this dataset, temperature observations coded as NA are considered missing. Now, we can imagine that either the measurement failed in a specific day for a specific weather station, or that certain stations only measure temperatures on certain days of the month. Knowing which of these applies can change how we approach this missing data. As you can see, how to treat missing data depends highly on how the data was obtained, and the more you know about a dataset, the better decision you can make. In general, the central question with missing data is: Should we remove observations with missing values, or should we impute missing values? This also relates to the difference between values that are missing at random vs. values that are missing systematically. In the weather example above, the first case (of failed measurements) could be thought of as missing at random, and the second case as missing systematically. Data that is missing systematically can significantly bias an analysis. For example: Suppose we want to predict how sick someone is from test result. If doctors do not carry out the test because a patient is too sick, then the fact test is missing is a great predictor of how sick the patient is. So in general, the first step when dealing with missing data is to understand why and how data may be missing. I.e., talk to collaborator, or person who created the dataset. Once you know that, if a relatively small fraction of observations contain have missing values, then it may be safe to remove observations. tidy_weather_nomissing &lt;- tidy_weather %&gt;% tidyr::drop_na(tmax, tmin) tidy_weather_nomissing ## # A tibble: 33 x 6 ## id year month day tmax tmin ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 MX17004 2010 1 d30 27.8 14.5 ## 2 MX17004 2010 2 d11 29.7 13.4 ## 3 MX17004 2010 2 d2 27.3 14.4 ## 4 MX17004 2010 2 d23 29.9 10.7 ## 5 MX17004 2010 2 d3 24.1 14.4 ## 6 MX17004 2010 3 d10 34.5 16.8 ## 7 MX17004 2010 3 d16 31.1 17.6 ## 8 MX17004 2010 3 d5 32.1 14.2 ## 9 MX17004 2010 4 d27 36.3 16.7 ## 10 MX17004 2010 5 d27 33.2 18.2 ## # … with 23 more rows 21.0.1 Dealing with data missing at random In the case of categorical variables, a useful approach is to encode missing as a new category and include that in subsequent modeling. tb &lt;- read_csv(file.path(&quot;data&quot;, &quot;tb.csv&quot;)) tidy_tb &lt;- tb %&gt;% gather(demo, n, -iso2, -year) %&gt;% separate(demo, c(&quot;sex&quot;, &quot;age&quot;), sep=1) tidy_tb %&gt;% tidyr::replace_na(list(iso2=&quot;missing&quot;)) ## # A tibble: 115,380 x 5 ## iso2 year sex age n ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 AD 1989 m 04 NA ## 2 AD 1990 m 04 NA ## 3 AD 1991 m 04 NA ## 4 AD 1992 m 04 NA ## 5 AD 1993 m 04 NA ## 6 AD 1994 m 04 NA ## 7 AD 1996 m 04 NA ## 8 AD 1997 m 04 NA ## 9 AD 1998 m 04 NA ## 10 AD 1999 m 04 NA ## # … with 115,370 more rows In the case of numeric values, we can use a simple method for imputation where we replace missing values for a variable with, for instance, the mean of non-missing values flights %&gt;% tidyr::replace_na(list(dep_delay=mean(.$dep_delay, na.rm=TRUE))) ## # A tibble: 336,776 x 19 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 544 545 -1 1004 ## 5 2013 1 1 554 600 -6 812 ## 6 2013 1 1 554 558 -4 740 ## 7 2013 1 1 555 600 -5 913 ## 8 2013 1 1 557 600 -3 709 ## 9 2013 1 1 557 600 -3 838 ## 10 2013 1 1 558 600 -2 753 ## # … with 336,766 more rows, and 12 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt; A more complex method is to replace missing values for a variable predicting from other variables when variables are related (we will see linear regression using the lm and predict functions later on) dep_delay_fit &lt;- flights %&gt;% lm(dep_delay~origin, data=.) # use average delay conditioned on origin airport flights %&gt;% modelr::add_predictions(dep_delay_fit, var=&quot;pred_delay&quot;) %&gt;% mutate(dep_delay_fixed = ifelse(!is.na(dep_delay), dep_delay, pred_delay)) %&gt;% select(origin, dest, dep_delay, dep_delay_fixed) %&gt;% filter(is.na(dep_delay)) ## # A tibble: 8,255 x 4 ## origin dest dep_delay dep_delay_fixed ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 EWR RDU NA 15.1 ## 2 LGA DFW NA 10.3 ## 3 LGA MIA NA 10.3 ## 4 JFK FLL NA 12.1 ## 5 EWR CVG NA 15.1 ## 6 EWR PIT NA 15.1 ## 7 EWR MHT NA 15.1 ## 8 EWR ATL NA 15.1 ## 9 EWR IND NA 15.1 ## 10 JFK LAX NA 12.1 ## # … with 8,245 more rows In either case, a common approach is to add an additional indicator variable stating if numeric missing value was imputed flights %&gt;% mutate(dep_delay_missing = is.na(dep_delay)) ## # A tibble: 336,776 x 20 ## year month day dep_time sched_dep_time dep_delay arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 ## 2 2013 1 1 533 529 4 850 ## 3 2013 1 1 542 540 2 923 ## 4 2013 1 1 544 545 -1 1004 ## 5 2013 1 1 554 600 -6 812 ## 6 2013 1 1 554 558 -4 740 ## 7 2013 1 1 555 600 -5 913 ## 8 2013 1 1 557 600 -3 709 ## 9 2013 1 1 557 600 -3 838 ## 10 2013 1 1 558 600 -2 753 ## # … with 336,766 more rows, and 13 more variables: sched_arr_time &lt;int&gt;, ## # arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, dep_delay_missing &lt;lgl&gt; In both of these cases note that imputing missing values has two effects. First, the central tendency of data is retained, for example, if we impute missing data using the mean of a numeric variable, the mean after imputation will not change. This is a good reason to impute based on estimates of central tendency. However, the spread of the data will change. After imputation, the spread of the data will be smaller relative to spread if we ignore missing values. This could be problematic as underestimating the spread of data can yield over-confident inferences in downstream analysis. We will not address these issues directly in later chapters, but you should be aware of this. "],
["part-statistical-learning.html", "(Part) Statistical Learning", " (Part) Statistical Learning "],
["univariate-distributions-and-statistics.html", "22 Univariate distributions and statistics 22.1 Variation, randomness and stochasticity 22.2 (Discrete) Probability distributions 22.3 Expectation 22.4 Estimation 22.5 The normal distribution", " 22 Univariate distributions and statistics One of the purposes of this class is for you to learn Statistical and Machine Learning techniques commonly used in data analysis. By the end of the term, you should be able to read papers that use these methods critically and analyze data using these methods. When using any of these tools we will be we will be asking ourselves if our findings are “statistically significant”. For example, if we make use of a classification algorithm to distinguish between to groups of entities and find that we can correctly predict a class in 70 out of our 100 cases, how can we determine if this could have happened by chance alone? To be able to answer these questions, we need to understand some basic probabilistic and statistical principles. In this section we will review some of these principles. 22.1 Variation, randomness and stochasticity In the preceeding sections of the class we have not spoken too much about randomness and stochasticity. We have, however, spoken about variation. When we discussed the notion of spread in a given dataset, measured by the sample standard deviation, for example, we are referring to the fact that in a population of entities (e.g., a set of tweets) there is naturally occuring variation in measurements (different frequency of word usage, for example). Notice that we can discuss the notation of variation without referring to any randomness, stochasticity or noise. Why do we study probability then? Because, we do want to distinguish, when possible, between natural occuring variation and randomness or stochasticity. For instance, suppose we want to learn something about education loan debt for 19-30 year olds in Maryland. We could find loan debt for all 19-30 year old Maryland residents, and calculate mean and standard deviation. But that’s difficult to do for all residents. So, instead we sample (say by randomly sending Twitter surveys), and estimate the average and standard deviation of debt in this population from the sample. Now, this presents an issue since we could do the same from a different random sample and get a different set of estimates. Why? Because there is naturally-occuring variation in this population. So, a simple question to ask is, how good are our estimates of debt mean and standard deviation from sample of 19-30 year old Marylanders? In another example, suppose we build a predictive model of loan debt for 19-30 year old Marylanders based on other variables (e.g., sex, income, education, wages, etc.) from our sample. How good will this model perform when predicting debt in general? We use probability and statistics to answer these questions. We use probability to capture stochasticity in the sampling process and model naturally occuring variation in measurements in a population of interest. One final word, the term population which we use extensively here means the entire collection of entities we want to model. This could include people, but also images, text, GO positions, etc. 22.1.1 Random variables The basic concept in our discussion of probability is the random variable. Consider a situation where you are tasked with determining if a given tweet was generated by a bot. You sample a tweet at random from the set of all tweets ever written and have a human expert decide if it was generated by a bot or not. You can denote this as a binary random variable \\(X \\in \\{0,1\\}\\), with value \\(1\\) if the tweet is bot-gerneated and 0 otherwise. Why is this a random value? Because it depends on the tweet that was randomly sampled. 22.2 (Discrete) Probability distributions Now we can start talking about the distribution of values of a random variable. In our example, random variable \\(X\\) can take values 0 or 1. We would like to specify how these values are distributed over the set of all possible tweets one can randomly sample. We use a probability distribution to do this. A probability distribution is a function \\(P:\\mathcal{D} \\to [0,1]\\) over set \\(\\mathcal{D}\\) of all values random variable \\(X\\) can take to the interval \\([0,1]\\). The function \\(P\\) describes how values of \\(X\\) are distributed over domain \\(\\mathcal{D}\\). We start with a probability mass function \\(p\\) which must satisfy two properties: \\(p(X=x) \\geq 0\\) for all values \\(x \\in mathcal{D}\\), and \\(\\sum_{x\\in \\mathcal{D}} p(X=x) = 1\\) Now, how do we interpret quantity \\(p(X=1)\\)? \\(p(X=1)\\) is the probability that a uniformly random sampled tweet is bot-generated, which implies the proportion of bot-generated tweets in the set of “all” tweets is \\(p(X=1)\\). I say “all” because it’s really the set of tweets one could possibly sample. Armed with a probability mass function we can talk about a cumulative probability distribution that describes the sum of probability up to a given value. We saw a similar concept for the empirical distribution of data when we discussed quartiles. 22.2.1 Example The oracle of TWEET Suppose we have a magical oracle and know for a fact that 70% of “all” tweets are bot-generated. In that case \\(p(X=1) = .7\\) and \\(p(X=0)=1-.7=.3\\). 22.3 Expectation What if I randomly sampled \\(n=100\\) tweets? How many of those do I expect to be bot-generated? Expectation is a formal concept in probability: \\[ \\mathbb{E} X = \\sum_{x\\in \\mathcal{D}} x p(X=x) \\] What is the expectation of \\(X\\) (a single sample) in our tweet example? \\[ 0 \\times p(X=0) + 1 \\times p(X=1) = \\ 0 \\times .3 + 1 \\times .7 = .7 \\] Now, consider random variable \\(Y=X_1 + X_2 + \\cdots + X_{100}\\). What is \\(Y\\)? Remember we want to know the expected number of bot-generated tweets in a sample of \\(n=100\\) tweets. We have \\(X_i=\\{0,1\\}\\) for each of the \\(n=100\\) tweets, each a random variable, which we obtained by uniformly and independently sampling for the set of all tweets. With that, now random variable \\(Y\\) equals the number of bot-generated tweets in my sample of \\(n=100\\) tweets. In this case: \\[ \\begin{aligned} \\mathbb{E} Y &amp; = \\mathbb{E} (X_1 + X_2 + \\cdots + X_{100}) \\\\ {} &amp; = \\mathbb{E} X_1 + \\mathbb{E} X_2 + \\cdots + \\mathbb{E} X_{100} \\\\ {} &amp; = .7 + .7 + \\cdots + .7 \\\\ {} &amp; = 100 \\times .7 \\\\ {} &amp; = 70 \\end{aligned} \\] This uses some facts about expectation you can show in general. For any pair of random variables \\(X_1\\) and \\(X_2\\), \\(\\mathbb{E} (X_1 + X_2) = \\mathbb{E} X_1 + \\mathbb{E} X_2\\). For any random variable \\(X\\) and constant a, \\(\\mathbb{E} aX = a \\mathbb{E} X\\). 22.4 Estimation Our discussion so far has assumed that we have access to an oracle that told us \\(p(X=1)=.7\\), but we don’t. For our tweet analysis task, we need to estimate the proportion of “all” tweets that are bot-generated. This is where our probability model and the expectation we derive from it comes in. Given data \\(x_1, x_2, x_3, \\ldots, x_{100}\\), with 67 of those tweets labeled as bot-generated (i.e., \\(x_i=1\\) for 67 of them), we can say \\(y=\\sum_i x_i=67\\). Now from our discussion above, we expect \\(y=np\\) where \\(p=p(X=1)\\), so let’s use that observation to estimate \\(p\\)! \\[ \\begin{aligned} np = 67 &amp; \\Rightarrow \\\\ 100p = 67 &amp; \\Rightarrow \\\\ \\hat{p} = \\frac{67}{100} &amp; \\Rightarrow \\\\ \\hat{p} = .67 \\end{aligned} \\] Our estimate is wrong, but close (remember we had an oracle of TWEET), but can we ever get it right? Can I say how wrong I should expect my estimates to be? Notice that our estimate of \\(p\\), \\(\\hat{p}\\) is the sample mean of \\(x_1,x_2,\\ldots,x_n\\). Let’s go back to our oracle of tweet to do a thought experiment and replicate how we derived our estimate from 100 tweets a few thousand times. # proportion of bot-tweets in the the tweet population # as given by the oracle of TWEET p &lt;- 0.7 # let&#39;s sample 100 tweets # this function chooses between values in a vector (0 and 1) # with probability given by vector prob # we need 100 samples from this vector with replacement # since there are fewer items in the vector than the size # of the sample we are making x &lt;- sample(c(0,1), size=100, replace=TRUE, prob=c(1-p,p)) # compute the estimated proportion that are bot-generated (using the sample mean) phat &lt;- mean(x) # if we had an oracle that let&#39;s us do this cheaply, # we could replicate our experiment 1000 times # (you don&#39;t in real life) # first let&#39;s write a function that gets an estimate # of proportion from a random sample get_estimate &lt;- function(n, p=0.7) mean(sample(c(0,1), size=n, replace=TRUE, prob=c(1-p,p))) # let&#39;s make a vector with 1000 _estimates_ phats_100 &lt;- replicate(1000, get_estimate(100)) # now let&#39;s plot a histogram of the hist(phats_100, xlab=expression(hat(p)), xlim=c(0.5,1), main=&quot;Distribution of p estimates from 100 tweets&quot;) What does this say about our estimates of the proportion of bot-generated tweets if we use 100 tweets in our sample? Now what if instead of sampling \\(n=100\\) tweets we used other sample sizes? par(mfrow=c(2,3)) # what if we sample 10 tweets phats_10 &lt;- replicate(1000, get_estimate(10)) hist(phats_10, main=&quot;10 tweets&quot;, xlab=&quot;p hat&quot;, xlim=c(.5,1), probability=TRUE) # what if we sample 100 tweets phats_100 &lt;- replicate(1000, get_estimate(100)) hist(phats_100, main=&quot;100 tweets&quot;, xlab=&quot;p hat&quot;, xlim=c(.5,1), probability=TRUE) # what if we sample 500 tweets phats_500 &lt;- replicate(1000, get_estimate(500)) hist(phats_500, main=&quot;500 tweets&quot;, xlab=&quot;p hat&quot;, xlim=c(.5,1), probability=TRUE) # what about 1000 tweets phats_1000 &lt;- replicate(1000, get_estimate(1000)) hist(phats_1000, main=&quot;1000 tweets&quot;, xlab=&quot;p hat&quot;, xlim=c(.5,1), probability=TRUE) # what about 5000 tweets phats_5000 &lt;- replicate(1000, get_estimate(5000)) hist(phats_5000, main=&quot;5000 tweets&quot;, xlab=&quot;p hat&quot;, xlim=c(.5,1), probability=TRUE) # what about 10000 tweets phats_10000 &lt;- replicate(1000, get_estimate(10000)) hist(phats_10000, main=&quot;10000 tweets&quot;, xlab=&quot;p hat&quot;, xlim=c(.5,1), probability=TRUE) We can make a couple of observations: The distribution of estimate \\(\\hat{p}\\) is centered at \\(p=.7\\), our unknown population proportion, and The spread of the distribution depends on the number of samples \\(n\\). This is an illustration of two central tenets of statistics that serves as the foundation of much of what we will do later in the course to interpret the models we build from data. 22.4.1 Law of large numbers (LLN) Given independently sampled random variables \\(X_1,X_2,\\cdots,X_n\\) with \\(\\mathbb{E} X_i=\\mu\\) for all \\(i\\), the LLN states that the sample mean \\[ \\frac{1}{n} \\sum_i X_i \\to \\mu \\] tends to the population mean (under some assumptions beyond the scope of this class). regardless of the distribution of the \\(X_i\\). An implication of this is that using the sample mean is the right thing to estimate parameters by matching their expected value! 22.4.2 Central Limit Theorem (CLT) The LLN says that estimates built using the sample mean will tend to the correct answer, the CLT describes how these estimates are spread around the correct answer. Here we will use the concept of variance which is expected spread, measured in squared distance, from the expected value of a random variable: \\[ \\mathrm{var(X)} = \\mathbb{E} (X - \\mathbb{E} X)^2 \\] Example: consider the variance of our random tweet example: \\[ \\begin{aligned} \\mathrm{var(X)} &amp; = \\sum_{\\mathcal{D}} (x-\\mathbb{E} X)^2 p(X=x) \\\\ {} &amp; = (0 - p)^2 \\times (1-p) + (1 - p)^2 \\times p \\\\ {} &amp; = p^2(1-p) + (1-p)^2p \\\\ {} &amp; = p(1-p) (p + (1-p)) \\\\ {} &amp; = p(1-p) (p - p + 1) \\\\ {} &amp; = p(1-p) \\end{aligned} \\] Now, we can state the CLT: \\[ \\frac{1}{n} \\sum_{i=1} X_i \\] tends towards a normal distribution as \\(n \\rightarrow \\infty\\). This says, that as sample size increases the distribution of sample means is well approximated by a normal distribution. This means we can approximate the expected error of our estimates well. 22.5 The normal distribution The normal distribution describes the distribution of continuous random variables over the range \\((-\\infty,\\infty)\\) using two parameters: mean \\(\\mu\\) and standard deviation \\(\\sigma\\). We write “\\(Y\\) is normally distributed with mean \\(\\mu\\) and standard deviation \\(\\sigma\\)” as \\(Y\\sim N(\\mu,\\sigma)\\). We write its probability density function as: \\[ p(Y=y) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\mathrm{exp} \\left\\{ -\\frac{1}{2} \\left( \\frac{y-\\mu}{\\sigma} \\right)^2 \\right\\} \\] Here are three examples of probability density functions of normal distributions with mean \\(\\mu=60,50,60\\) and standard deviation \\(\\sigma=2,2,6\\): # 100 equally spaced values between 40 and 80 yrange &lt;- seq(40, 80, len=100) # values of the normal density function density_values_1 &lt;- dnorm(yrange, mean=60, sd=2) density_values_2 &lt;- dnorm(yrange, mean=50, sd=2) density_values_3 &lt;- dnorm(yrange, mean=60, sd=6) # now plot the function plot(yrange, density_values_1, type=&quot;l&quot;, col=&quot;red&quot;, lwd=2, xlab=&quot;y&quot;, ylab=&quot;density&quot;) lines(yrange, density_values_2, col=&quot;blue&quot;, lwd=2) lines(yrange, density_values_3, col=&quot;orange&quot;, lwd=2) legend(&quot;topright&quot;, legend=c(&quot;mean 60, sd 2&quot;, &quot;mean 50, sd 2&quot;, &quot;mean 60, sd 6&quot;), col=c(&quot;red&quot;,&quot;blue&quot;,&quot;orange&quot;), lwd=2) Like the discrete case, probability density functions for continuous random variables need to satisfy certain conditions: \\(p(Y=y) \\geq 0\\) for all values \\(Y \\in (-\\infty,\\infty)\\), and \\(\\int_{-\\infty}^{\\infty} p(Y=y) dy = 1\\) One way of remembering the density function of the normal distribution is that probability decays exponentially with rate \\(\\sigma\\) based on squared distance to the mean \\(\\mu\\). (Here is squared distance again!) Also, notice the term inside the squared? \\[ z = \\left( \\frac{y - \\mu}{\\sigma} \\right) \\] this is the standardization transformation we saw in previous lectures. In fact the name standardization comes from the standard normal distribution \\(N(0,1)\\) (mean 0 and standard deviation 1), which is very convenient to work with because it’s density function is much simpler: \\[ p(Z=z) = \\frac{1}{\\sqrt{2\\pi}} \\mathrm{exp} \\left\\{ -\\frac{1}{2} z^2 \\right\\} \\] In fact, if random variable \\(Y \\sim N(\\mu,\\sigma)\\) then random variable \\(Z=\\frac{Y-\\mu}{\\sigma} \\sim N(0,1)\\). 22.5.1 CLT continued We need one last bit of terminology to finish the statement of the CLT. Consider data \\(X_1,X_2,\\cdots,X_n\\) with \\(\\mathbb{E}X_i= \\mu\\) for all \\(i\\), and \\(\\mathrm{sd}(X_i)=\\sigma\\) for all \\(i\\), and their sample mean \\(Y=\\frac{1}{n} \\sum_i X_i\\). The standard deviation of \\(Y\\) is called the standard error: \\[ \\mathrm{se}(Y) = \\frac{\\sigma}{\\sqrt{n}} \\] Ok, now we can make the CLT statement precise: the distribution of \\(Y\\) tends towards \\(N(\\mu,\\frac{\\sigma}{\\sqrt{n}})\\) as \\(n \\rightarrow \\infty\\). This says, that as sample size increases the distribution of sample means is well approximated by a normal distribution, and that the spread of the distribution goes to zero at the rate \\(\\sqrt{n}\\). Disclaimer: there a few mathematical subtleties. Two important ones are that \\(X_1,\\ldots,X_n\\) are iid (independent, identically distributed) random variables, and \\(\\mathrm{var}X &lt; \\infty\\) Let’s redo our simulated replications of our tweet samples to illustrate the CLT at work: # we can calculate standard error for each of the # settings we saw previously and compare these replications # to the normal distribution given by the CLT # let&#39;s write a function that adds a normal density # plot for a given sample size draw_normal_density &lt;- function(n) { se &lt;- sqrt(p*(1-p))/sqrt(n) f &lt;- dnorm(seq(0.5,1,len=1000), mean=p, sd=se) lines(seq(0.5,1,len=1000), f, col=&quot;red&quot;, lwd=1.6) } par(mfrow=c(2,3)) # what if we sample 10 tweets phats_10 &lt;- replicate(1000, get_estimate(10)) hist(phats_10, main=&quot;10 tweets&quot;, xlab=&quot;p hat&quot;, xlim=c(.5,1), probability=TRUE) draw_normal_density(10) # what if we sample 100 tweets phats_100 &lt;- replicate(1000, get_estimate(100)) hist(phats_100, main=&quot;100 tweets&quot;, xlab=&quot;p hat&quot;, xlim=c(.5,1), probability=TRUE) draw_normal_density(100) # what if we sample 500 tweets phats_500 &lt;- replicate(1000, get_estimate(500)) hist(phats_500, main=&quot;500 tweets&quot;, xlab=&quot;p hat&quot;, xlim=c(.5,1), probability=TRUE) draw_normal_density(500) # what about 1000 tweets phats_1000 &lt;- replicate(1000, get_estimate(1000)) hist(phats_1000, main=&quot;1000 tweets&quot;, xlab=&quot;p hat&quot;, xlim=c(.5,1), probability=TRUE) draw_normal_density(1000) # what about 5000 tweets phats_5000 &lt;- replicate(1000, get_estimate(5000)) hist(phats_5000, main=&quot;5000 tweets&quot;, xlab=&quot;p hat&quot;, xlim=c(.5,1), probability=TRUE) draw_normal_density(5000) # what about 10000 tweets phats_10000 &lt;- replicate(1000, get_estimate(10000)) hist(phats_10000, main=&quot;10000 tweets&quot;, xlab=&quot;p hat&quot;, xlim=c(.5,1), probability=TRUE) draw_normal_density(10000) Here we see the three main points of the LLN and CLT: the normal density is centered around \\(\\mu=.7\\), the normal approximation gets better as \\(n\\) increases, and the standard error goes to 0 as \\(n\\) increases. "],
["experiment-design-and-hypothesis-testing.html", "23 Experiment design and hypothesis testing 23.1 Inference 23.2 A/B Testing 23.3 Summary 23.4 Probability Distributions", " 23 Experiment design and hypothesis testing In this section we see one instance in which we can apply the CLT in data analysis. 23.1 Inference One way to think about how we use probability in data analysis (statistical and machine learning) is like this: The LLN tells us that our parameter \\(\\hat{p}\\) will be close to \\(p\\) on average, the CLT lets us answer how confident are we that we found \\(p\\). We do this by constructing a confidence interval as follows. Since \\(\\hat{p} \\sim N(p,\\frac{\\sqrt{p(1-p)}}{\\sqrt{n}})\\), we want to find an interval \\([\\hat{p}_{-}, \\hat{p}_{+}]\\), with \\(\\hat{p}\\) at its center, with 95% of the probability specified by the CLT. Why? In that case, there is 95% that the value of parameter \\(p\\) will be within that interval. Now, how do we calculate this interval, since we want the interval to contain 95% of the probability? The probability for the tails (values outside this interval) will be \\((1-.95)/2\\) (since there are two tails). So, the lower value of the interval will be one where the normal probability distribution (with mean \\(\\hat{p}\\) and standard deviation \\(\\frac{\\sqrt{\\hat{p}(1-\\hat{p})}}{\\sqrt{n}}\\)) is such that \\(P(Y \\leq \\hat{p}_{-}) = .05/2\\), which we can calculate using the function qnorm function in R: \\[ \\begin{align} \\hat{p}_{-} &amp; = \\mathtt{qnorm}(.05/2, \\hat{p}, \\frac{\\sqrt{\\hat{p}(1-\\hat{p})}}{\\sqrt{n}}) \\\\ {} &amp; = \\hat{p} + \\mathtt{qnorm}(.05/2,0, \\frac{\\sqrt{\\hat{p}(1-\\hat{p})}}{\\sqrt{n}}) \\end{align} \\] The upper value of the interval is computed with probability \\(1-(.05/2)\\), which by the symmetry of the normal distribution is given by \\(\\hat{p}_{+} = \\hat{p} + -\\mathtt{qnorm}(.05/2,0, \\frac{\\sqrt{\\hat{p}(1-\\hat{p})}}{\\sqrt{n}})\\). Let’s see how these intervals look for our twitter bot example: library(dplyr) get_estimate &lt;- function(n, p=0.7) mean(sample(c(0,1), size=n, replace=TRUE, prob=c(1-p,p))) set.seed(1) # let&#39;s construct confidence intervals for samples of size n=10,100,500,1000,10000 tab &lt;- data.frame(sample_size=c(10,100,500,1000,10000)) %&gt;% mutate(phat = sapply(sample_size,get_estimate)) %&gt;% mutate(se = sqrt(phat*(1-phat)) / sqrt(sample_size)) %&gt;% mutate(lower = phat + qnorm(.05/2, sd=se)) %&gt;% mutate(upper = phat + -qnorm(.05/2, sd=se)) knitr::kable(tab) sample_size phat se lower upper 10 0.7000 0.1449138 0.4159742 0.9840258 100 0.6900 0.0462493 0.5993530 0.7806470 500 0.7020 0.0204546 0.6619097 0.7420903 1000 0.6940 0.0145727 0.6654380 0.7225620 10000 0.6982 0.0045904 0.6892030 0.7071970 For our sample of \\(n=500\\), we would say that our estimate of \\(p\\) is \\(0.7 \\pm -0.04\\). A compact way of writing that is that our estimate of \\(p\\) is \\({}_{0.66}0.7_{0.74}\\). 23.1.1 Hypothesis testing How else is this framework useful? Suppose that before I sampled tweets I thought (hypothesized) that more than 50% of tweets are bot-generated. One way very popular way of thinking about this problem is to reject the hypothesis that this is not the case. In this, case we have a null hypothesis that 50% or less of tweets are bot-generated), against an alternative hypothesis that more than 50% of tweets are bot-generated. You will see this written in statistics textbooks as: \\[ \\begin{align} H_0: \\, &amp; p &lt;= .5 &amp; \\textrm{(null)} \\\\ H_1: \\, &amp; p &gt; .5 &amp; \\textrm{(alternative)} \\end{align} \\] Note: this is a one-sided test vs. a two-sided test where the null hypothesis is that \\(p=.5\\) and the alternative is \\(p \\neq .5\\). According to the CLT, estimates \\(\\hat{p}\\) of \\(p\\) from \\(n\\) samples would be distributed as \\(N(.5, \\frac{\\sqrt{.5(1-.5)}}{\\sqrt{n}})\\) (we use \\(p=.5\\) as this is the worst case for the hypothesis we want to test). Once we do have our sample of \\(n\\) tweets we can get an estimate \\(\\hat{p}\\) as we did before. If we see that \\(\\hat{p}\\) (sample mean from our sample of tweets) is too far from \\(p=.5\\) then we could reject the null hypothesis since the estimate we derived from the data we have is not statistically consistent with the null hypothesis. Now, how do we say our estimate \\(\\hat{p}\\) is too far? Here, we use the probability model given by the CLT. If \\(P(Y \\geq \\hat{p}) \\geq .95\\) under the null model (of \\(p=.5\\)), we say it is too far and we reject. This 95% rejection threshold is conservative, but somewhat arbitrary. So we use one more metric, \\(P(|Y| \\geq \\hat{p})\\) (the infamous p-value) to say: we could reject this hypothesis for all thresholds greater than this p-value. Let’s see how testing the hypothesis \\(p &gt; .5\\) would look like for our tweet example tab &lt;- tab %&gt;% mutate(p_value = 1-pnorm(phat, mean=.5, sd=se)) knitr::kable(tab) sample_size phat se lower upper p_value 10 0.7000 0.1449138 0.4159742 0.9840258 0.0837731 100 0.6900 0.0462493 0.5993530 0.7806470 0.0000199 500 0.7020 0.0204546 0.6619097 0.7420903 0.0000000 1000 0.6940 0.0145727 0.6654380 0.7225620 0.0000000 10000 0.6982 0.0045904 0.6892030 0.7071970 0.0000000 Notice that rejection occurs when the parameter value for the null hypothesis \\(p=.5\\) is outside the 95% confidence interval. Another note, these results hold for \\(n\\) sufficiently large that the normal distribution in the CLT provides a good approximation of the distribution of estimates \\(\\hat{p}\\). In cases where \\(n\\) is smaller, the \\(t\\)-distribution, as opposed to the normal distribution, provides a better approximation of the distribution of estimates \\(\\hat{p}\\). In that case, instead of using pnorm in the calculations above, we would use pt (for \\(t\\)-distribution) and the testing procedure above is referred to as a \\(t\\)-test (one-sided or two-sided as above). Now, as \\(n\\) grows, the \\(t\\)-distribution approaches a normal distribution which is why analysts use the \\(t\\)-test regularly. 23.2 A/B Testing A classic experimental design where hypothesis testing is commonly used in A/B testing. Here we are interested in seeing if proposed changes to a webpage has a desired effect. For example, we would like to know if page visitors follow a link more often after a page redesign. Here we have two estimates \\(\\hat{p}_A\\) and \\(\\hat{p}_B\\), the proportion of clicks for design A and B respectively. The null hypothesis we would test is that there is no difference in proportions between the two designs. Mathematically, we would like to know “What is the probability that we observe a difference in proportions this large under the null hypothesis”. We will work this out as a homework exercise. 23.3 Summary Inference: estimate parameter from data based on assumed probability model (for example, matching expectation. We’ll see later another method called maximum likelihood). For averages the LLN and CLT tells us how to compute probabilities from a single parameter estimate, that is, derived from one dataset of samples. With these probabilities we can construct confidence intervals for our estimate. Testing: Having a hypothesis about our parameter of interest, we can use probability under this hypothesis to see how statistically consistent our data is with that hypothesis, and reject the hypothesis if data is not statistically consistent enough (again using probability from CLT when dealing with averages). 23.4 Probability Distributions In this example we saw three distributions: 23.4.1 Bernoulli Notation: \\(X \\sim \\mathrm{Bernoulli}(p)\\). Values: \\(X \\in \\{0,1\\}\\) Parameter: \\(p\\), \\(p(X=1)=p\\) (probability of success). Expected Value: \\(\\mathbb{E} X = p\\) Variance: \\(\\mathrm{var}(X) = p(1-p)\\). We can write the probability mass function as \\[ p(X=x)=p^x(1-p)^{(1-x)} \\] 23.4.2 Binomial This corresponds to the number of \\(1\\)’s in a draw of \\(n\\) independent \\(\\mathrm{Bernoulli}(p)\\) random variables. Notation: \\(X \\sim \\mathrm{Bin(n,p)}\\). Values: \\(X \\in 0,1,2,\\ldots,n\\) Parameters: \\(p\\) (probability of success), \\(n\\) number of Bernoulli draws Expected Value: \\(\\mathbb{E} X=np\\) Variance: \\(\\mathrm{var}(X) = np(1-p)\\) Here the probability mass function is a little more complicated since we have many different ways in which \\(n\\) draws of independent Bernoulli random variables result in the same number of successess \\[ p(X=k) = \\binom{n}{k} p^k(1-p)^{n-k} \\] 23.4.3 Normal (Gaussian) distribution Notation: \\(X \\sim N(\\mu,\\sigma)\\) Values: \\(X \\in \\mathbb{R}\\) Parameters: mean \\(\\mu\\), standard deviation \\(\\sigma\\) Expected Value: \\(\\mathbb{E} X = \\mu\\) Variance: \\(\\mathrm{var}(X) = \\sigma^2\\) The probability density function was given above. A useful reference for probability distributions can be found here: https://blog.cloudera.com/blog/2015/12/common-probability-distributions-the-data-scientists-crib-sheet/ 23.4.4 Distributions in R For a majority of common distributions, R has the so-called d,p,q,r family of functions: function use d probability density (or mass) function p cumulative probability function q quantile function r random value generator For example, to use these for the Binomial distribution: # using n=10, p=.3 # compute probability mass function value for k=4 successess dbinom(4, n=10, p=.3) # compute cumulative probability function for k=4 successess pbinom(4, n=10, p=.3) # compute the number of success corresponding to the .80th quantile qbinom(.8, n=10, p=.3) # generate a random value k rbinom(1, n=10, p=.3) "],
["multivariate-probability.html", "24 Multivariate probability 24.1 Joint and conditional probability 24.2 Bayes’ Rule 24.3 Conditional expectation 24.4 Maximum likelihood", " 24 Multivariate probability 24.1 Joint and conditional probability Suppose that for each tweet I sample I can also say if it has a lot of retweets or not. So, I have another binary random variable \\(Y \\in \\{0,1\\}\\) where \\(Y=1\\) indicates the sampled tweet has a lot of retweets. (Note, we could say \\(Y\\sim \\mathrm{Bernoulli}(p_Y))\\). So we could illustrate the population of “all” tweets as We can talk of the joint probability distribution of \\(X\\) and \\(Y\\): \\(p(X=x, Y=y)\\), where random variables \\(X\\) and \\(Y\\) can take values from domains \\(\\mathcal{D}_X\\) and \\(\\mathcal{D}_Y\\) respectively. Here we have the same conditions as we had for univariate distributions: \\(p(X=x,Y=y)\\geq 0\\) for all combination of values \\(x\\) and \\(y\\), and \\(\\sum_{(x,y) \\in \\mathcal{D}_X \\times \\mathcal{D}_Y} p(X=x,Y=y) = 1\\) We can also talk about conditional probability where we look at the probability of a tweet being bot-generated or not, conditioned on wheter it has lots of retweets or not: \\[ p(X=x | Y=y) \\] which also needs to satisfy the properties of a probability distribution. So to make sure \\[ \\sum_{x \\in \\mathcal{D}_X} p(X=x|Y=y) = 1 \\] we define \\[ p(X=x | Y=y) = \\frac{p(X=x,Y=y)}{p(Y=y)} \\] Here we use the important concept of marginalization, which follows from the properties of joint probability distribution we saw above: \\(\\sum_{x \\in \\mathcal{D}_X} p(X=x, Y=y) = p(Y=y)\\). This also lets us talk about conditional independence: if the probabilty of a tweet being bot-generated does not depend on a tweet having lots of retweets, that is \\(p(X=x) = p(X=x|Y=y)\\) for all \\(y\\), then we say \\(X\\) is conditionally independent of \\(Y\\). Consider the tweet diagram above, is \\(X\\) conditionally independent of \\(Y\\)? What would the diagram look like if \\(X\\) was conditionally independent of \\(Y\\)? One more note, you can also see that for conditionally independent variables, the joint probability has an easy form \\(p(X=x,Y=y)=p(X=x)p(Y=y)\\), which generalizes to more than two independent random variables. 24.2 Bayes’ Rule One extremely useful and important rule of probability follows from our definitions of conditional and joint probability above. Bayes’ rule is pervasive in Statistics, Machine Learning and Artificial Intelligence. It is a very powerful tool to talk about uncertainty, beliefs, evidence, and many other technical and philosophical matters. It is however, of extreme simplicity. All Bayes’ Rule states is that \\[ p(X=x|Y=y) = \\frac{p(Y=y|X=x)p(X=x)}{p(Y=y)} \\] which follow directly from our definitions above. One very common usage of Bayes’ Rule is that it let’s us define one conditional probability distribution based on another probability distribution. This is useful is the latter is easier to reason about, or estimate. For example, it may be hard to reason about \\(p(X=x|Y=y)\\) in our tweet example. If you know a tweet has a lot retweets \\((Y=1)\\), what can you say about the probability that it is bot-generated, i.e., \\(p(X=1|Y=1)\\)? Maybe not much, tweets have lots of retweets for many reasons. However, it may be easier to reason about the reverse: if I tell you a tweet is bot-generated \\((X=1)\\), what can you say about the probability that it has a lot of retweets, i.e., \\(p(Y=1|X=1)\\). That may be easier to reason about, at least bot-generated tweets are designed to get lots of retweets. At minimum, it’s easier to estimate because we can get a training set of bot-generated tweets and estimate this conditional probability. Bayes’ Rule tells us how to get the hard to reason about (or estimate) conditional probability \\(p(X=x|Y=y)\\) in terms of the conditional probability that is easier to reason about (or estimate) \\(p(Y=y|X=x)\\). This is the basis of the Naive Bayes prediction method, which we’ll revisit briefly later on. 24.3 Conditional expectation With conditional probabilty we can start talking about conditional expectation, which generalizes the concept of expectation we saw before. For example, the conditional expected value (conditional mean) of \\(X\\) given \\(Y=y\\) is \\[ \\mathbb{E} [ X|Y=y ] = \\sum_{x \\in \\mathcal{D}_X} x p(X=x|Y=y) \\] This notion of conditional expectation, which follows from conditional probability, will serve as the basis for our Machine Learning method studies in the next few lectures! 24.4 Maximum likelihood One last note. We saw before how we estimated a parameter from matching expectation from a probability model with what we observed in data. The most popular method of estimation uses a similar idea: given data \\(x_1,x_2,\\ldots,x_n\\) and an assumed model of their distribution, e.g., \\(X_i\\sim \\mathrm{Bernoulli}(p)\\) for all \\(i\\), and they are iid, let’s find the value of parameter \\(p\\) that maximizes the likelihood (or probability) of the data we observe under this assumed probability model. We call the resulting estimate the maximum likelihood estimate. Here are some fun exercises to try: Given a sample \\(x_1\\) with \\(X_1 \\sim N(\\mu,1)\\), show that the maximum likelihood estimate of \\(\\mu\\), \\(\\hat{\\mu}=x_1\\). It is most often convinient to minimize negative log-likelihood instead of maximizing likelihood. So in this case: \\[ \\begin{align} -\\mathscr{L}(\\mu) &amp; = - \\log p(X_1=x_1) \\\\ {} &amp; = \\log{\\sqrt{2\\pi}} + \\frac{1}{2}(x_1 - \\mu)^2 \\end{align} \\] To minimize this function of \\(\\mu\\) we can ignore all terms that are independent of \\(\\mu\\), and concentrate only on minimizing the last term. Now, this term is always positive, so the smallest value it can have is 0. So, we minimize it by setting \\(\\hat{\\mu}=x_1\\). Given a sample \\(x_1,x_2,\\ldots,x_n\\) of \\(n\\) iid random variables with \\(X_i \\sim N(\\mu,1)\\) for all \\(i\\), show that the maximum likelihood estimate of \\(\\mu\\), \\(\\hat{\\mu}=\\overline{x}\\) the sample mean! Here we would follow a similar approach, write out the negative log likelihood as a function \\(f(\\mu;x_i)\\) of \\(\\mu\\) that depends on data \\(x_i\\). Two useful properties here are: \\(p(X_1=x_1,X_2=x_2,\\ldots,X_n=x_n)=p(X_1=x_1)p(X_2=x_2)\\cdots p(X_n=x_n)\\), and \\(\\log \\prod_i f(\\mu;x_i) = \\sum_i \\log f(\\mu;x_i)\\) Then find a value of \\(\\mu\\) that minimizes this function. Hint: we saw this when we showed that the sample mean is the minimizer of total squared distance in our exploratory analysis unit! "],
["part-machine-learning.html", "(Part) Machine Learning", " (Part) Machine Learning "],
["data-analysis-with-geometry.html", "25 Data Analysis with Geometry 25.1 Motivating Example: Credit Analysis 25.2 From data to feature vectors 25.3 Technical notation 25.4 Geometry and Distances 25.5 Quick vector algebra review 25.6 The curse of dimensionality 25.7 Summary", " 25 Data Analysis with Geometry A common situation in data analysis is that one has an outcome attribute (variable) \\(Y\\) and one or more independent covariate or predictor attributes \\(X_1,\\ldots,X_p\\). One usually observes these variables for multiple “instances” (or entities). (Note) As before, we use upper case to denote a random variable. To denote actual numbers we use lower case. One way to think about it: \\(Y\\) has not happened yet, and when it does, we see \\(Y=y\\). One may be interested in various things: What effects do the covariates have on the outcome? How well can we describe these effects? Can we predict the outcome using the covariates?, etc… 25.1 Motivating Example: Credit Analysis default student balance income No No 729.5265 44361.625 No Yes 817.1804 12106.135 No No 1073.5492 31767.139 No No 529.2506 35704.494 No No 785.6559 38463.496 No Yes 919.5885 7491.559 Task: predict account default What is the outcome \\(Y\\)? What are the predictors \\(X_j\\)? We will sometimes call attributes \\(Y\\) and \\(X\\) the outcome/predictors, sometimes observed/covariates, and even input/output. We may call each entity an observation or example. We will denote predictors with \\(X\\) and outcomes with \\(Y\\) (quantitative) and \\(G\\) (qualitative). Notice \\(G\\) are not numbers, so we cannot add or multiply them. We will use \\(G\\) to denote the set of possible values. For gender it would be \\(G=\\{Male,Female\\}\\). 25.2 From data to feature vectors The vast majority of ML algorithms we see in class treat instances as “feature vectors”. We can represent each instance as a vector in Euclidean space \\(\\langle x_1,\\ldots,x_p,y \\rangle\\). This means: every measurement is represented as a continuous value in particular, categorical variables become numeric (e.g., one-hot encoding) Here is the same credit data represented as a matrix of feature vectors ## Warning: `as_data_frame()` is deprecated, use `as_tibble()` (but mind the new semantics). ## This warning is displayed once per session. default student balance income -1 0 1057.6332 37229.81 1 0 1717.0716 38408.89 1 1 2145.6077 23516.19 1 1 1704.5785 16887.41 -1 1 703.8388 19318.06 -1 0 853.4381 58757.68 25.3 Technical notation Observed values will be denoted in lower case. So \\(x_i\\) means the \\(i\\)th observation of the random variable \\(X\\). Matrices are represented with bold face upper case. For example \\(\\mathbf{X}\\) will represent all observed predictors. \\(N\\) (or \\(n\\)) will usually mean the number of observations, or length of \\(Y\\). \\(i\\) will be used to denote which observation and \\(j\\) to denote which covariate or predictor. Vectors will not be bold, for example \\(x_i\\) may mean all predictors for subject \\(i\\), unless it is the vector of a particular predictor \\(\\mathbf{x}_j\\). All vectors are assumed to be column vectors, so the \\(i\\)-th row of \\(\\mathbf{X}\\) will be \\(x_i&#39;\\), i.e., the transpose of \\(x_i\\). 25.4 Geometry and Distances Now that we think of instances as vectors we can do some interesting operations. Let’s try a first one: define a distance between two instances using Euclidean distance \\[d(x_1,x_2) = \\sqrt{\\sum_{j=1}^p(x_{1j}-x_{2j})^2}\\] 25.4.1 K-nearest neighbor classification Now that we have a distance between instances we can create a classifier. Suppose we want to predict the class for an instance \\(x\\). K-nearest neighbors uses the closest points in predictor space predict \\(Y\\). \\[ \\hat{Y} = \\frac{1}{k} \\sum_{x_k \\in N_k(x)} y_k. \\] \\(N_k(x)\\) represents the \\(k\\)-nearest points to \\(x\\). How would you use \\(\\hat{Y}\\) to make a prediction? An important notion in ML and prediction is inductive bias. What assumptions we make about our data that allow us to make predictions. In KNN, our inductive bias is that points that are nearby will be of the same class. Parameter \\(K\\) is a hyper-parameter, it’s value may affect prediction accuracy significantly. Question: which situation may lead to overfitting, high or low values of \\(K\\)? Why? 25.4.2 The importance of transformations Feature scaling is an important issue in distance-based methods. In the example below, which of these two features will affect distance the most? 25.5 Quick vector algebra review A (real-valued) vector is just an array of real values, for instance \\(x = \\langle 1, 2.5, −6 \\rangle\\) is a three-dimensional vector. Vector sums are computed pointwise, and are only defined when dimensions match, so \\[\\langle 1, 2.5, −6 \\rangle + \\langle 2, −2.5, 3 \\rangle = \\langle 3, 0, −3 \\rangle\\]. In general, if \\(c = a + b\\) then \\(cd = ad + bd\\) for all vectors \\(d\\). Vector addition can be viewed geometrically as taking a vector \\(a\\), then tacking on \\(b\\) to the end of it; the new end point is exactly \\(c\\). Scalar Multiplication: vectors can be scaled by real values; \\[2\\langle 1, 2.5, −6 \\rangle = \\langle 2, 5, −12\\rangle\\] In general, \\(ax = \\langle ax_1, ax_2, \\ldots, ax_p\\rangle\\) The norm of a vector \\(x\\), written \\(\\|x\\|\\) is its length. Unless otherwise specified, this is its Euclidean length, namely: \\[\\|x\\| = \\sqrt{\\sum_{j=1}^p x_j^2}\\] 25.5.1 Quiz Write Euclidean distance of vectors \\(u\\) and \\(v\\) as a vector norm The dot product, or inner product of two vectors \\(u\\) and \\(v\\) is defined as \\[u&#39;v = \\sum_{j=1}^p u_i v_i\\] A useful geometric interpretation of the inner product \\(v&#39;u\\) is that it gives the projection of \\(v\\) onto \\(u\\) (when \\(\\|u\\|=1\\)). 25.6 The curse of dimensionality Distance-based methods like KNN can be problematic in high-dimensional problems Consider the case where we have many covariates. We want to use \\(k\\)-nearest neighbor methods. Basically, we need to define distance and look for small multi-dimensional “balls” around the target points. With many covariates this becomes difficult. Imagine we have equally spaced data and that each covariate is in \\([0,1]\\). We want to something like kNN with a local focus that uses 10% of the data in the local fitting. If we have \\(p\\) covariates and we are forming \\(p\\)-dimensional cubes, then each side of the cube must have size \\(l\\) determined by \\(l \\times l \\times \\dots \\times l = l^p = .10\\). If the number of covariates is p=10, then \\(l = .1^{1/10} = .8\\). So it really isn’t local! If we reduce the percent of data we consider to 1%, \\(l=0.63\\). Still not very local. If we keep reducing the size of the neighborhoods we will end up with very small number of data points in each average and thus predictions with very large variance. This is known as the curse of dimensionality. Because of this so-called curse, it is not always possible to use KNN. But other methods, like Decision Trees, thrive on multidimensional data. 25.7 Summary We will represent many ML algorithms geometrically as vectors Vector math review K-nearest neighbors The curse of dimensionality "],
["linear-regression.html", "26 Linear Regression 26.1 Simple Regression 26.2 Inference 26.3 Some important technicalities 26.4 Issues with linear regression 26.5 Multiple linear regression 26.6 Interactions in linear models", " 26 Linear Regression Linear regression is a very elegant, simple, powerful and commonly used technique for data analysis. We use it extensively in exploratory data analysis (we used in project 2, for example) and in statistical analyses since it fits into the statistical framework we saw in the last unit, and thus lets us do things like construct confidence intervals and hypothesis testing for relationships between variables. It also provides predictions for continuous outcomes of interest. Note: Much of this development is based on “Introduction to Statistical Learning” by James, Witten, Hastie and Tibshirani. http://www-bcf.usc.edu/~gareth/ISL/ 26.1 Simple Regression Let’s start with the simplest linear model. The goal here is to analyze the relationship between a continuous numerical variable \\(Y\\) and another (numerical or categorical) variable \\(X\\). We assume that in our population of interest the relationship between the two is given by a linear function: \\[ Y = \\beta_0 + \\beta_1 X \\] Here is (simulated) data from an advertising campaign measuring sales and the amount spent in advertising. We think that sales are related to the amount of money spent on TV advertising: \\[ \\mathtt{sales} \\approx \\beta_0 + \\beta_1 \\times \\mathtt{TV} \\] Given this data, we would say that we regress sales on TV when we perform this regression analysis. As before, given data we would like to estimate what this relationship is in the population (what is the population in this case?). What do we need to estimate in this case? Values for \\(\\beta_0\\) and \\(\\beta_1\\). What is the criteria that we use to estimate them? Just like the previous unit we need to setup an inverse problem. What we are stating mathematically in the linear regression problem is that the conditional expectation (or conditional mean, conditional average) of \\(Y\\) given \\(X=x\\) is defined by this linear relationship: \\[ \\mathbb{E}[Y|X=x] = \\beta_0 + \\beta_1 x \\] Given a dataset, the inverse problem is then to find the values of \\(\\beta_0\\) and \\(\\beta_1\\) that minimize deviation between data and expectation, and again use squared devation to do this. The linear regression problem Given data \\((x_1, y_1), (x_2, y_2), \\ldots, (x_n, y_n)\\), find values \\(\\beta_0\\) and \\(\\beta_1\\) that minimize objective or loss function RSS (residual sum of squares): \\[ \\arg \\min_{\\beta_0,\\beta_1} RSS = \\frac{1}{2} \\sum_i (y_i - (\\beta_0 + \\beta_1 x_i))^2 \\] Similar to what we did with the derivation of the mean as a measure of central tendency we can derive the values of minimizers\\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\). We use the same principle, compute derivatives (partial this time) of the objective function RSS, set to zero and solve to obtain: \\[ \\begin{aligned} \\hat{\\beta}_1 &amp; = \\frac{\\sum_{i=1}^n (y_i - \\overline{y})(x_i - \\overline{x})}{\\sum_{i=1}^n (x_i - \\overline{x})^2} \\\\ {} &amp; = \\frac{\\mathrm{cov}(y,x)}{\\mathrm{var}(x)} \\\\ \\hat{\\beta}_0 &amp; = \\overline{y} - \\hat{\\beta}_1 \\overline{x} \\end{aligned} \\] Let’s take a look at some data. Here is data measuring characteristics of cars, including horsepower, weight, displacement, miles per gallon. Let’s see how well a linear model captures the relationship between miles per gallon and weight library(ISLR) library(tidyverse) data(Auto) Auto %&gt;% ggplot(aes(x=weight, y=mpg)) + geom_point() + geom_smooth(method=lm) + theme_minimal() In R, linear models are built using the lm function auto_fit &lt;- lm(mpg~weight, data=Auto) auto_fit ## ## Call: ## lm(formula = mpg ~ weight, data = Auto) ## ## Coefficients: ## (Intercept) weight ## 46.216525 -0.007647 This states that for this dataset \\(\\hat{\\beta}_0 = 46.2165245\\) and \\(\\hat{\\beta}_1 = -0.0076473\\). What’s the interpretation? According to this model, a weightless car weight=0 would run \\(\\approx 46.22\\) miles per gallon on average, and, on average, a car would run \\(\\approx 0.01\\) miles per gallon fewer for every extra pound of weight. Note, that the units of the outcome \\(Y\\) and the predictor \\(X\\) matter for the interpretation of these values. 26.2 Inference As we saw in the last unit, now that we have an estimate, we want to know its precision. We will see that similar arguments based on the CLT hold again. The main point is to understand that like the sample mean, the regression line we learn from a specific dataset is an estimate. A different sample from the same population would give us a different estimate (regression line). But, the CLT tells us that, on average, we are close to population regression line (I.e., close to \\(\\beta_0\\) and \\(\\beta_1\\)), that the spread around \\(\\beta_0\\) and \\(\\beta_1\\) is well approximated by a normal distribution and that the spread goes to zero as the sample size increases. 26.2.1 Confidence Interval Using the same framework as before, we can construct a confidence interval to say how precise we think our estimates of the population regression line is. In particular, we want to see how precise our estimate of \\(\\beta_1\\) is, since that captures the relationship between the two variables. We again, use a similar framework. First, we calculate a standard error estimate for \\(\\beta_1\\): \\[ \\mathrm{se}(\\hat{beta}_1)^2 = \\frac{\\sum_i (y_i - \\hat{y}_i)^2}{\\sum_i (x_i - \\overline{x})^2} \\] and construct a 95% confidence interval \\[ \\beta_1 = \\hat{\\beta}_1 \\pm 1.95 \\times \\mathrm{se}(\\hat{beta}_1) \\] Note, \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\\). Going back to our example: auto_fit_stats &lt;- auto_fit %&gt;% tidy() %&gt;% select(term, estimate, std.error) auto_fit_stats ## # A tibble: 2 x 3 ## term estimate std.error ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 46.2 0.799 ## 2 weight -0.00765 0.000258 This tidy function is defined by the broom package, which is very handy to manipulate the result of learning models in a consistent manner. The select call removes some extra information that we will discuss shortly. confidence_interval_offset &lt;- 1.95 * auto_fit_stats$std.error[2] confidence_interval &lt;- round(c(auto_fit_stats$estimate[2] - confidence_interval_offset, auto_fit_stats$estimate[2], auto_fit_stats$estimate[2] + confidence_interval_offset), 4) Given the confidence interval, we would say, “on average, a car runs \\(_{-0.0082} -0.0076_{-0.0071}\\) miles per gallon fewer per pound of weight. 26.2.2 The \\(t\\)-statistic and the \\(t\\)-distribution As in the previous unit, we can also test a null hypothesis about this relationship: “there is no relationship between weight and miles per gallon”, which translates to \\(\\beta_1=0\\). Again, using the same argument based on the CLT, if this hypothesis is true then the distribution of \\(\\hat{\\beta}_1\\) is well approximated by \\(N(0,\\mathrm{se}(\\hat{\\beta}_1))\\), and if we observe the learned \\(\\hat{\\beta}_1\\) is too far from 0 according to this distribution then we reject the hypothesis. Now, there is a technicality here that we did not discuss in the previous unit that is worth paying attention to. We saw before that the CLT states that the normal approximation is good as sample size increases, but what about moderate sample sizes (say, less than 100)? The \\(t\\) distribution provides a better approximation of the sampling distribution of these estimates for moderate sample sizes, and it tends to the normal distribution as sample size increases. The \\(t\\) distribution is commonly used in this testing situation to obtain the probability of rejecting the null hypothesis. It is based on the \\(t\\)-statistic \\[ \\frac{\\hat{\\beta}_1}{\\mathrm{se}(\\hat{\\beta}_1)} \\] You can think of this as a signal-to-noise ratio, or a standardizing transformation on the estimated parameter. Under the null hypothesis, the \\(t\\)-statistic is well approximated by a \\(t\\)-distribution with \\(n-2\\) degrees of freedom (we will get back to degrees of freedom shortly). Like other distributions, you can compute with the \\(t\\)-distribution using the p,d,q,r-family of functions, e.g., pt is the cumulative probability distribution function. In our example, we get a \\(t\\) statistic and P-value as follows: auto_fit_stats &lt;- auto_fit %&gt;% tidy() auto_fit_stats ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 46.2 0.799 57.9 1.62e-193 ## 2 weight -0.00765 0.000258 -29.6 6.02e-102 We would say: “We found a statistically significant relationship between weight and miles per gallon. On average, a car runs \\(_{-0.0082} -0.0076_{-0.0071}\\) miles per gallon fewer per pound of weight (\\(t\\)=-29.65, \\(p\\)-value&lt;6.015296110^{-102}$).” 26.2.3 Global Fit Now, notice that we can make predictions based on our conditional expectation, and that prediction should be better than a prediction with a simple average. We can use this comparison as a measure of how good of a job we are doing using our model to fit this data: how much of the variance of \\(Y\\) can we explain with our model. To do this we can calculate total sum of squares: \\[ TSS = \\sum_i (y_i - \\overline{y})^2 \\] (this is the squared error of a prediction using the sample mean of \\(Y\\)) and the residual sum of squares: \\[ RSS = \\sum_i (y_i - \\hat{y}_i)^2 \\] (which is the squared error of a prediction using the linear model we learned) The commonly used \\(R^2\\) measure comparse these two quantities: \\[ R^2 = \\frac{\\mathrm{TSS}-\\mathrm{RSS}}{\\mathrm{TSS}} = 1 - \\frac{\\mathrm{RSS}}{\\mathrm{TSS}} \\] These types of global statistics for the linear model can be obtained using the glance function in the broom package. In our example auto_fit %&gt;% glance() %&gt;% select(r.squared, sigma, statistic, df, p.value) ## # A tibble: 1 x 5 ## r.squared sigma statistic df p.value ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0.693 4.33 879. 2 6.02e-102 We will explain the the columns statistic, df and p.value when we discuss regression using more than a single predictor \\(X\\). 26.3 Some important technicalities We mentioned above that predictor \\(X\\) could be numeric or categorical. However, this is not precisely true. We can use a transformation to represent categorical variables. Here is a simple example: Suppose we have a categorical variable sex with values female and male, and we want to show the relationship between, say credit card balance and sex. We can create a dummy variable \\(x\\) as follows: \\[ x_i = \\left\\{ \\begin{aligned} 1 &amp; \\textrm{ if female} \\\\ 0 &amp; \\textrm{o.w.} \\end{aligned} \\right. \\] and fit a model \\(y = \\beta_0 + \\beta_1 x\\). What is the conditional expectation given by this model? If the person is male, then \\(y=\\beta_0\\), if the person is female, then \\(y=\\beta_0 + \\beta_1\\). So, what is the interpretation of \\(\\beta_1\\)? The average difference in credit card balance between females and males. We could do a different encoding: \\[ x_i = \\left\\{ \\begin{aligned} +1 &amp; \\textrm{ if female} \\\\ -1 &amp; \\textrm{o.w.} \\end{aligned} \\right. \\] Then what is the interpretation of \\(\\beta_1\\) in this case? Note, that when we call the lm(y~x) function and x is a factor with two levels, the first transformation is used by default. What if there are more than 2 levels? We need multiple regression, which we will see shortly. 26.4 Issues with linear regression There are some assumptions underlying the inferences and predictions we make using linear regression that we should verify are met when we use this framework. Let’s start with four important ones that apply to simple regression 26.4.1 Non-linearity of outcome-predictor relationship What if the underlying relationship is not linear? We will see later that we can capture non-linear relationships between variables, but for now, let’s concentrate on detecting if a linear relationship is a good approximation. We can use exploratory visual analysis to do this for now by plotting residuals \\((y_i - \\hat{y}_i)^2\\) as a function of the fitted values \\(\\hat{y}_i\\). The broom package uses the augment function to help with this task. It augments the input data used to learn the linear model with information of the fitted model for each observation augmented_auto &lt;- auto_fit %&gt;% augment() augmented_auto %&gt;% head() ## # A tibble: 6 x 10 ## .rownames mpg weight .fitted .se.fit .resid .hat .sigma .cooksd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 18 3504 19.4 0.258 -1.42 0.00353 4.34 1.91e-4 ## 2 2 15 3693 18.0 0.286 -2.97 0.00437 4.34 1.04e-3 ## 3 3 18 3436 19.9 0.249 -1.94 0.00330 4.34 3.33e-4 ## 4 4 16 3433 20.0 0.248 -3.96 0.00329 4.33 1.38e-3 ## 5 5 17 3449 19.8 0.250 -2.84 0.00334 4.34 7.23e-4 ## 6 6 15 4341 13.0 0.414 1.98 0.00914 4.34 9.73e-4 ## # … with 1 more variable: .std.resid &lt;dbl&gt; With that we can make the plot we need to check for possible non-linearity augmented_auto %&gt;% ggplot(aes(x=.fitted,y=.resid)) + geom_point() + geom_smooth() + labs(x=&quot;fitted&quot;, y=&quot;residual&quot;) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; 26.4.2 Correlated Error For our inferences to be valid, we need residuals to be independent and identically distributed. We can spot non independence if we observe a trend in residuals as a function of the predictor \\(X\\). Here is a simulation to demonstrate this: In this case, our standard error estimates would be underestimated and our confidence intervals and hypothesis testing results would be biased. 26.4.3 Non-constant variance Another violation of the iid assumption would be observed if the spread of residuals is not independent of the fitted values. Here is an illustration, and a possible fix using a log transformation on the outcome \\(Y\\). 26.5 Multiple linear regression Now that we’ve seen regression using a single predictor we’ll move on to regression using multiple predictors. In this case, we use models of conditional expectation represented as linear functions of multiple variables: \\[ \\mathbb{E}[Y|X_1=x_1,X_2=x_2,\\ldots,X_p=x_p] = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots \\beta_3 x_3 \\] In the case of our advertising example, this would be a model: \\[ \\mathtt{sales} = \\beta_0 + \\beta_1 \\times \\mathtt{TV} + \\beta_2 \\times \\mathtt{newspaper} + \\beta_3 \\times \\mathtt{facebook} \\] These models let us make statements of the type: “holding everything else constant, sales increased on average by 1000 per dollar spent on Facebook advertising” (this would be given by parameter \\(\\beta_3\\) in the example model). 26.5.1 Estimation in multivariate regression Generalizing simple regression, we estimate \\(\\beta\\)’s by minimizing an objective function that represents the difference between observed data and our expectation based on the linear model: \\[ \\begin{aligned} RSS &amp; = \\frac{1}{2} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\\\ {} &amp; = \\frac{1}{2} \\sum_{i=1}^n (y_i - (\\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p))^2 \\end{aligned} \\] The minimizer is found using numerical algorithms to solve this type of least squares problems. These are covered in Linear Algebra courses, and include the QR decomposition, Gauss-Seidel method, and many others. Later in the course we will look at stochastic gradient descent, a simple algorithm that scales to very large datasets. 26.5.2 Example (cont’d) Continuing with our Auto example, we can build a model for miles per gallon using multiple predictors: auto_fit &lt;- lm(mpg~1+weight+cylinders+horsepower+displacement+year, data=Auto) auto_fit ## ## Call: ## lm(formula = mpg ~ 1 + weight + cylinders + horsepower + displacement + ## year, data = Auto) ## ## Coefficients: ## (Intercept) weight cylinders horsepower displacement ## -12.779493 -0.006524 -0.343690 -0.007715 0.006996 ## year ## 0.749924 From this model we can make the statement: “Holding everything else constant, cars run 0.76 miles per gallon more each year on average”. 26.5.3 Statistical statements (cont’d) Like simple linear regression, we can construct confidence intervals, and test a null hypothesis of no relationship (\\(\\beta_j=0\\)) for the parameter corresponding to each predictor. This is again nicely managed by the broom package: auto_fit_stats &lt;- auto_fit %&gt;% tidy() auto_fit_stats %&gt;% knitr::kable() term estimate std.error statistic p.value (Intercept) -12.7794934 4.2739387 -2.9900975 0.0029676 weight -0.0065245 0.0005866 -11.1215621 0.0000000 cylinders -0.3436900 0.3315619 -1.0365786 0.3005812 horsepower -0.0077149 0.0107036 -0.7207702 0.4714872 displacement 0.0069964 0.0073095 0.9571736 0.3390787 year 0.7499243 0.0524361 14.3016700 0.0000000 In this case we would reject the null hypothesis of no relationship only for predictors weight and year. We would write the statement for year as follows: “Holding everything else constant, cars run \\({}_{0.65} 0.75_{0.85}\\) miles per gallon more each year on average (P-value=P-value&lt;1e-16)”. 26.5.4 The F-test We can make additional statements for multivariate regression: “is there a relationship between any of the predictors and the response?”. Mathematically, we write this as \\(\\beta_1 = \\beta_2 = \\cdots = \\beta_p = 0\\). Under the null, our model for \\(y\\) would be estimated by the sample mean \\(\\overline{y}\\), and the error for that estimate is by total sum of squared error \\(TSS\\). As before, we can compare this to the residual sum of squared error \\(RSS\\) using the \\(F\\) statistic: \\[ \\frac{(\\mathrm{TSS}-\\mathrm{RSS})/p}{\\mathrm{RSS}/(n-p-1)} \\] If this statistic is greater (enough) than 1, then we reject hypothesis that there is no relationship between response and predictors. Back to our example, we use the glance function to compute this type of summary: auto_fit %&gt;% glance() %&gt;% select(r.squared, sigma, statistic, df, p.value) %&gt;% knitr::kable(&quot;html&quot;) r.squared sigma statistic df p.value 0.8089093 3.433902 326.7965 6 0 In comparison with the linear model only using weight, this multivariate model explains more of the variance of mpg, but using more predictors. This is where the notion of degrees of freedom comes in: we now have a model with expanded representational ability. However, the bigger the model, we are conditioning more and more, and intuitively, given a fixed dataset, have fewer data points to estimate conditional expectation for each value of the predictors. That means, that are estimated conditional expectation is less precise. To capture this phenomenon, we want statistics that tradeoff how well the model fits the data, and the “complexity” of the model. Now, we can look at the full output of the glance function: auto_fit %&gt;% glance() %&gt;% knitr::kable(&quot;html&quot;) r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual 0.8089093 0.806434 3.433902 326.7965 0 6 -1036.81 2087.62 2115.419 4551.589 386 Columns AIC and BIC display statistics that penalize model fit with model size. The smaller this value, the better. Let’s now compare a model only using weight, a model only using weight and year and the full multiple regression model we saw before. lm(mpg~weight, data=Auto) %&gt;% glance() %&gt;% knitr::kable(&quot;html&quot;) r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual 0.6926304 0.6918423 4.332712 878.8309 0 2 -1129.969 2265.939 2277.852 7321.234 390 lm(mpg~weight+year, data=Auto) %&gt;% glance() %&gt;% knitr::kable(&quot;html&quot;) r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual 0.8081803 0.8071941 3.427153 819.473 0 3 -1037.556 2083.113 2098.998 4568.952 389 In this case, using more predictors beyond weight and year doesn’t help. 26.5.5 Categorical predictors (cont’d) We saw transformations for categorical predictors with only two values, and deferred our discussion of categorical predictors with more than two values. In our example we have the origin predictor, corresponding to where the car was manufactured, which has multiple values Auto &lt;- Auto %&gt;% mutate(origin=factor(origin)) levels(Auto$origin) ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; As before, we can only use numerical predictors in linear regression models. The most common way of doing this is to create new dummy predictors to encode the value of the categorical predictor. Let’s take a categorical variable major that can take values CS, MATH, BUS. We can encode these values using variables \\(x_1\\) and \\(x_2\\) \\[ x_1 = \\left\\{ \\begin{aligned} 1 &amp; \\textrm{ if MATH} \\\\ 0 &amp; \\textrm{ o.w.} \\end{aligned} \\right. \\] \\[ x_2 = \\left\\{ \\begin{aligned} 1 &amp; \\textrm{ if BUS} \\\\ 0 &amp; \\textrm{ o.w.} \\end{aligned} \\right. \\] Now let’s build a model to capture the relationship between salary and major: \\[ \\mathtt{salary} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\] What is the expected salary for a CS major? \\(\\beta_0\\). For a MATH major? \\(\\beta_0 + \\beta_1\\). For a BUS major? \\(\\beta_0 + \\beta_2\\). So, \\(\\beta_1\\) is the average difference in salary between MATH and CS majors. How can we calculate the average difference in salary between MATH and BUS majors? \\(\\beta_1 - \\beta_2\\). The lm function in R does this transformation by default when a variable has class factor. We can see what the underlying numerical predictors look like by using the model_matrix function and passing it the model formula we build: extended_df &lt;- model.matrix(~origin, data=Auto) %&gt;% as.data.frame() %&gt;% mutate(origin = Auto$origin) extended_df %&gt;% filter(origin == &quot;1&quot;) %&gt;% head() ## (Intercept) origin2 origin3 origin ## 1 1 0 0 1 ## 2 1 0 0 1 ## 3 1 0 0 1 ## 4 1 0 0 1 ## 5 1 0 0 1 ## 6 1 0 0 1 extended_df %&gt;% filter(origin == &quot;2&quot;) %&gt;% head() ## (Intercept) origin2 origin3 origin ## 1 1 1 0 2 ## 2 1 1 0 2 ## 3 1 1 0 2 ## 4 1 1 0 2 ## 5 1 1 0 2 ## 6 1 1 0 2 extended_df %&gt;% filter(origin == &quot;3&quot;) %&gt;% head() ## (Intercept) origin2 origin3 origin ## 1 1 0 1 3 ## 2 1 0 1 3 ## 3 1 0 1 3 ## 4 1 0 1 3 ## 5 1 0 1 3 ## 6 1 0 1 3 26.6 Interactions in linear models The linear models so far include additive terms for a single predictor. That let us made statemnts of the type “holding everything else constant…”. But what if we think that a pair of predictors together have a relationship with the outcome. We can add these interaction terms to our linear models as products: \\[ \\mathbb{E} Y|X_1=x_1,X_2=x2 = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{12} x_1 x_2 \\] Consider the advertising example: \\[ \\mathtt{sales} = \\beta_0 + \\beta_1 \\times \\mathtt{TV} + \\beta_2 \\times \\mathtt{facebook} + \\beta_3 \\times (\\mathtt{TV} \\times \\mathtt{facebook}) \\] If \\(\\beta_3\\) is positive, then the effect of increasing TV advertising money is increased if facebook advertising is also increased. When using categorical variables, interactions have an elegant interpretation. Consider our car example, and suppose we build a model with an interaction between weight and origin. Let’s look at what the numerical predictors look like: extended_df &lt;- model.matrix(~weight+origin+weight:origin, data=Auto) %&gt;% as.data.frame() %&gt;% mutate(origin = Auto$origin) extended_df %&gt;% filter(origin == &quot;1&quot;) %&gt;% head() ## (Intercept) weight origin2 origin3 weight:origin2 weight:origin3 origin ## 1 1 3504 0 0 0 0 1 ## 2 1 3693 0 0 0 0 1 ## 3 1 3436 0 0 0 0 1 ## 4 1 3433 0 0 0 0 1 ## 5 1 3449 0 0 0 0 1 ## 6 1 4341 0 0 0 0 1 extended_df %&gt;% filter(origin == &quot;2&quot;) %&gt;% head() ## (Intercept) weight origin2 origin3 weight:origin2 weight:origin3 origin ## 1 1 1835 1 0 1835 0 2 ## 2 1 2672 1 0 2672 0 2 ## 3 1 2430 1 0 2430 0 2 ## 4 1 2375 1 0 2375 0 2 ## 5 1 2234 1 0 2234 0 2 ## 6 1 2123 1 0 2123 0 2 extended_df %&gt;% filter(origin == &quot;3&quot;) %&gt;% head() ## (Intercept) weight origin2 origin3 weight:origin2 weight:origin3 origin ## 1 1 2372 0 1 0 2372 3 ## 2 1 2130 0 1 0 2130 3 ## 3 1 2130 0 1 0 2130 3 ## 4 1 2228 0 1 0 2228 3 ## 5 1 1773 0 1 0 1773 3 ## 6 1 1613 0 1 0 1613 3 So what is the expected miles per gallon for a car with origin == 1 as a function of weight? \\[ \\mathtt{mpg} = \\beta_0 + \\beta_1 \\times \\mathtt{weight} \\] Now how about a car with origin == 2? \\[ \\mathtt{mpg} = \\beta_0 + \\beta_1 \\times \\mathtt{weight} + \\beta_2 + \\beta_4 \\times \\mathtt{weight} \\] Now think of the graphical representation of these lines. For origin == 1 the intercept of the regression line is \\(\\beta_0\\) and its slope is \\(\\beta_1\\). For origin == 2 the intercept of the regression line is \\(\\beta_0 + \\beta_2\\) and its slope is \\(\\beta_1+\\beta_4\\). ggplot does this when we map a factor variable to a aesthetic, say color, and use the geom_smooth method: Auto %&gt;% ggplot(aes(x=weight, y=mpg, color=origin)) + geom_point() + geom_smooth(method=lm) The intercept of the three lines seem to be different, but the slope of origin == 3 looks different (decreases faster) than the slopes of origin == 1 and origin == 2 that look very similar to each other. Let’s fit the model and see how much statistical confidence we can give to those observations: auto_fit &lt;- lm(mpg~weight*origin, data=Auto) auto_fit_stats &lt;- auto_fit %&gt;% tidy() auto_fit_stats %&gt;% knitr::kable() term estimate std.error statistic p.value (Intercept) 43.1484685 1.1861118 36.3780794 0.0000000 weight -0.0068540 0.0003423 -20.0204971 0.0000000 origin2 1.1247469 2.8780381 0.3908033 0.6961582 origin3 11.1116815 3.5743225 3.1087518 0.0020181 weight:origin2 0.0000036 0.0011106 0.0032191 0.9974332 weight:origin3 -0.0038651 0.0015411 -2.5079723 0.0125521 So we can say that for origin == 3 the relationship between mpg and weight is different but not for the other two values of origin. Now, there is still an issue here because this could be the result of a poor fit from a linear model, it seems none of these lines do a very good job of modeling the data we have. We can again check this for this model: auto_fit %&gt;% augment() %&gt;% ggplot(aes(x=.fitted, y=.resid)) + geom_point() The fact that residuals are not centered around zero suggests that a linear fit does not work well in this case. 26.6.1 Additional issues with linear regression We saw previously some issues with linear regression that we should take into account when using this method for modeling. Multiple linear regression introduces an additional issue that is extremely important to consider when interpreting the results of these analyses: collinearity. In this example, you have two predictors that are very closely related. In that case, the set of \\(\\beta\\)’s that minimize RSS may not be unique, and therefore our interpretation is invalid. You can identify this potential problem by regressing predictors onto each other. The usual solution is to fit models only including one of the colinear variables. "],
["linear-models-for-classification.html", "27 Linear models for classification 27.1 An example classification problem 27.2 Why not linear regression? 27.3 Classification as probability estimation problem 27.4 Logistic regression 27.5 Linear Discriminant Analysis 27.6 Classifier evaluation 27.7 Summary", " 27 Linear models for classification The general classification setting is: can we predict categorical response/output \\(Y\\), from set of predictors \\(X_1,X_2,\\ldots,X_p\\)? As in the regression case, we assume training data \\((\\mathbf{x}_1, y_1), \\ldots, (\\mathbf{x}_n, y_n)\\). In this case, however, responses \\(y_i\\) are categorical and take one of a fixed set of values. 27.1 An example classification problem An individual’s choice of transportation mode to commute to work. Predictors: income, cost and time required for each of the alternatives: driving/carpooling, biking, taking a bus, taking the train. Response: whether the individual makes their commute by car, bike, bus or train. From a classification model based on this data we could perform an inference task: how do people value price and time when considering their transportation choice. 27.2 Why not linear regression? In our previous unit we learned about linear regression. Why can’t we use linear regression in the classification setting. For categorical responses with more than two values, if order and scale (units) don’t make sense, then it’s not a regression problem \\[ Y = \\begin{cases} 1 &amp; \\textrm{if } \\mathtt{stroke} \\\\ 2 &amp; \\textrm{if } \\mathtt{drug overdose} \\\\ 3 &amp; \\textrm{if } \\mathtt{epileptic seizure} \\end{cases} \\] For binary responses, it’s a little better: \\[ Y = \\begin{cases} 0 &amp; \\textrm{if } \\mathtt{stroke} \\\\ 1 &amp; \\textrm{if } \\mathtt{drug overdose} \\\\ \\end{cases} \\] We could use linear regression in this setting and interpret response \\(Y\\) as a probability (e.g, if \\(\\hat{y} &gt; 0.5\\) predict \\(\\mathtt{drug overdose}\\)) 27.3 Classification as probability estimation problem This observation motivates how we will address the classification problem in general. Instead of modeling classes 0 or 1 directly, we will model the conditional class probability \\(p(Y=1|X=x)\\), and classify based on this probability. In general, classification approaches use discriminant (think of scoring) functions to do classification. Logistic regression is one way of estimating the class probability \\(p(Y=1|X=x)\\) (also denoted \\(p(x)\\)) 27.4 Logistic regression The basic idea behind logistic regression is to build a linear model related to \\(p(x)\\), since linear regression directly (i.e. \\(p(x) = \\beta_0 + \\beta_1 x\\)) doesn’t work. Why? Instead we build a linear model of log-odds: \\[ \\log \\frac{p(x)}{1-p(x)} = \\beta_0 + \\beta_1 x \\] Odds are equivalent to ratios of probabilities. For example, “two to one odds that Serena Williams wins the French Open” means “the probability that Serena Williams wins the French Open is double the probability he loses”. So, if odds = 2, \\(p(x)=2/3\\). If odds = 1/2, \\(p(x)=1/3\\). In general odds = \\(\\frac{p(x)}{1-p(x)}\\). 27.4.1 Exercises Suppose an individual has a 16% chance of defaulting on their credit card payment. What are the odds that she will default? On average, what fraction of people with an odds of 0.37 of defaulting on their credit card payment will in fact default? Here is how we compute a logistic regression model in R library(ISLR) library(dplyr) library(broom) library(ggplot2) data(Default) default_fit &lt;- glm(default ~ balance, data=Default, family=binomial) default_fit %&gt;% tidy() %&gt;% knitr::kable(digits=4) term estimate std.error statistic p.value (Intercept) -10.6513 0.3612 -29.4922 0 balance 0.0055 0.0002 24.9531 0 Interpretation of logistic regression models is slightly different than the linear regression model we looked at. In this case, the odds that a person defaults increase by \\(e^{0.05} \\approx 1.051\\) for every dollar in their account balance. As before, the accuracy of \\(\\hat{\\beta}_1\\) as an estimate of the population parameter is given its standard error. We can again construct a confidence interval for this estimate as we’ve done before. As before, we can do hypothesis testing of a relationship between account balance and the probability of default. In this case, we use a \\(Z\\)-statistic \\(\\frac{\\hat{\\beta}_1}{\\mathrm{SE}(\\hat{\\beta}_1)}\\) which plays the role of the t-statistic in linear regression: a scaled measure of our estimate (signal / noise). As before, the P-value is the probability of seeing a Z-value as large (e.g., 24.95) under the null hypothesis that there is no relationship between balance and the probability of defaulting, i.e., \\(\\beta_1=0\\) in the population. In accordance to the “inverse problem” view we’ve been developing in class, we require an algorithm required to estimate parameters \\(\\beta_0\\) and \\(\\beta_1\\) according to a data fit criterion. In logistic regression we use the Bernoulli probability model we saw previously (think of flipping a coin weighted by \\(p(x)\\)), and estimate parameters to maximize the likelihood of the observed training data under this coin flipping (binomial) model. I.e.: solve the following optimization problem \\[ \\max_{\\beta_0, \\beta_1} \\sum_{i:\\, y_i=1} log(p(x_i)) + \\sum_{i: y_i=0} log(1-p(x_i)) \\] This is a non-linear (but convex) optimization problem. You can learn algorithms to solve it in “Computational Methods” class (CMSC 460) 27.4.2 Making predictions We can use a learned logistic regression model to make predictions. E.g., “on average, the probability that a person with a balance of $1,000 defaults is”: \\[ \\hat{p}(1000) = \\frac{e^{\\hat{\\beta}_0 + \\hat{\\beta}_1 \\times 1000}}{1+e^{\\beta_0 + \\beta_1 \\times 1000}} \\approx \\frac{e^{-10.6514 + 0.0055 \\times 1000}}{1+e^{-10.6514 + 0.0055 \\times 1000}} \\\\ \\approx 0.00576 \\] 27.4.3 Multiple logistic regression This is a classification analog to linear regression: \\[ \\log \\frac{p(\\mathbf{x})}{1-p(\\mathbf{x})} = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p \\] fit &lt;- glm(default ~ balance + income + student, data=Default, family=&quot;binomial&quot;) fit %&gt;% tidy() %&gt;% knitr::kable(digits=4) term estimate std.error statistic p.value (Intercept) -10.8690 0.4923 -22.0801 0.0000 balance 0.0057 0.0002 24.7376 0.0000 income 0.0000 0.0000 0.3698 0.7115 studentYes -0.6468 0.2363 -2.7376 0.0062 As in multiple linear regression it is essential to avoid confounding!. Consider an example of single logistic regression of default vs. student status: fit1 &lt;- glm(default ~ student, data=Default, family=&quot;binomial&quot;) fit1 %&gt;% tidy() %&gt;% knitr::kable(digits=4) term estimate std.error statistic p.value (Intercept) -3.5041 0.0707 -49.5542 0e+00 studentYes 0.4049 0.1150 3.5202 4e-04 and a multiple logistic regression: fit2 &lt;- glm(default ~ balance + income + student, data=Default, family=&quot;binomial&quot;) fit2 %&gt;% tidy() %&gt;% knitr::kable(digits=4) term estimate std.error statistic p.value (Intercept) -10.8690 0.4923 -22.0801 0.0000 balance 0.0057 0.0002 24.7376 0.0000 income 0.0000 0.0000 0.3698 0.7115 studentYes -0.6468 0.2363 -2.7376 0.0062 27.4.4 Exercise Suppose we collect data for a group of students in a statistics class with variables X1 = hours studied, X2 = undergrad GPA, and Y = receive an A. We fit a logistic regression and produce estimated coefficients, \\(\\hat{\\beta}_0=-6, \\hat{\\beta}_1=0.05,\\hat{\\beta}_2=1\\). Estimate the probability that a student who studies for 40h and has an undergraduate GPA of 3.5 gets an A in the class. With estimated parameters from previous question, and GPA of 3.5 as before, how many hours would the student need to study to have a 50% chance of getting an A in the class? 27.5 Linear Discriminant Analysis Linear Discriminant Analysis (LDA) is a different linear method to estimate a probability model used for classification. Recall that we want to partition data based on class probability: e.g., find the \\(\\mathbf{X}\\) for which \\(P(\\mathrm{default=Yes}|X) &gt; P(\\mathrm{default=No}|X)\\). In logistic regression, we made no assumption about \\(\\mathbf{X}\\). In other cases, we can make assumptions about \\(\\mathbf{X}\\) that improve prediction performance (if assumptions hold, obviously) This suggests we can model balance for each of the classes with a normal distribution. WARNING, BIG ASSUMPTION: We will assume balance has the same variance for both classes (this is what makes LDA linear). So, we estimate average balance for people who do not default: \\[ \\hat{\\mu}_0 = \\frac{1}{n_0} \\sum_{i:\\, y_i=0} x_i \\] for people who do default: \\[ \\hat{\\mu}_1 = \\frac{1}{n_1} \\sum_{i:\\, y_i=1} x_i \\] and estimate variance for both classes as \\[ \\hat{\\sigma}^2 = \\frac{1}{n-2} \\sum_{k=1,2} \\sum_{i:\\, y_i=k} (x_i - \\hat{\\mu}_k)^2 \\] ## # A tibble: 2 x 2 ## default balance_mean ## &lt;fct&gt; &lt;dbl&gt; ## 1 No 804. ## 2 Yes 1748. ## # A tibble: 1 x 1 ## balance_sd ## &lt;dbl&gt; ## 1 453. We can “score” values of balance based on these estimates: \\[ f_k(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp \\left(-\\frac{1}{2\\sigma^2} (x-\\mu_k)^2 \\right) \\] Remember, what we want is posterior class probability \\(p(Y=k|X)\\), for that we need to include the probability that we observe class \\(k\\). This is called prior class probability, denoted \\(\\pi_k\\), means the proportion of times you expect people to default regardless of any other attribute. We can estimate from training data as the proportion of observations with label \\(k\\). Bayes’ Rule (or Theorem) gives us a way of computing \\(P(Y=k|X)\\) using score \\(f_k(x)\\) (from the class normal assumption) and prior \\(\\pi_k\\): \\[ P(Y=k|X) = \\frac{f_k(x) \\pi_k}{\\sum_l f_l(x) \\pi_l} \\] If data (conditioned by class) is distributed so that \\(f_k\\) is the right probability function to use, then predicting the class that maximizes \\(P(Y=k|X)\\) is the optimal thing to do. This is referred to the Bayes classifier (aka the Holy Grail of classification) 27.5.1 How to train LDA Compute class means and squared error based on class mean lda_stats &lt;- Default %&gt;% group_by(default) %&gt;% mutate(class_mean=mean(balance), squared_error=(balance-class_mean)^2) Compute class sizes and sum of squared errors lda_stats &lt;- lda_stats %&gt;% summarize(class_mean=first(class_mean), class_size=n(), sum_squares=sum(squared_error)) Compute class prior and variance (note same variance for both classes) lda_stats &lt;- lda_stats %&gt;% mutate(class_prior=class_size/sum(class_size), sigma2=sum(sum_squares) / (sum(class_size) - 2)) %&gt;% dplyr::select(default, class_mean, class_prior, sigma2) knitr::kable(lda_stats) default class_mean class_prior sigma2 No 803.9438 0.9667 205318.6 Yes 1747.8217 0.0333 205318.6 How do we predict with LDA? Predict Yes if \\(P(Y=1|X) &gt; P(Y=0|X)\\) Equivalently: \\[ \\log{\\frac{P(Y=1|X)}{P(Y=0|X)}} &gt; 0 \\Rightarrow \\\\ \\log f_1(x) + \\log \\pi_1 &gt; \\log f_0(x) + \\log \\pi_0 \\] This turns out to be a linear function of \\(x\\)! lda_log_ratio &lt;- function(balance, lda_stats) { n &lt;- length(balance) # subtract class mean centered_balance &lt;- rep(balance, 2) - rep(lda_stats$class_mean, each=n) # scale by standard deviation scaled_balance &lt;- centered_balance / sqrt(lda_stats$sigma2[1]) # compute log normal density and add log class prior lprobs &lt;- dnorm(scaled_balance, log=TRUE) + log(rep(lda_stats$class_prior, each=n)) # compute log ratio of class probabilities lprobs &lt;- matrix(lprobs, nc=2) colnames(lprobs) &lt;- lda_stats$default lprobs[,&quot;Yes&quot;] - lprobs[,&quot;No&quot;] } test_balance &lt;- seq(0, 3000, len=100) plot(test_balance, lda_log_ratio(test_balance, lda_stats), type=&quot;l&quot;, xlab=&quot;Balance&quot;, ylab=&quot;Log Probability Ratio&quot;, cex=1.4) 27.6 Classifier evaluation How do we determine how well classifiers are performing? One way is to compute the error rate of the classifier, the percent of mistakes it makes when predicting class library(MASS) lda_fit &lt;- lda(default ~ balance, data=Default) lda_pred &lt;- predict(lda_fit, data=Default) print(table(predicted=lda_pred$class, observed=Default$default)) ## observed ## predicted No Yes ## No 9643 257 ## Yes 24 76 # error rate mean(Default$default != lda_pred$class) * 100 ## [1] 2.81 # dummy error rate mean(Default$default != &quot;No&quot;) * 100 ## [1] 3.33 In this case, it would seem that LDA performs well. But in fact, we can get similar error rate by always predicting “no default”. We can see from this table that LDA errors are not symmetric. It’s most common error is that it misses true defaults. We need a more precise language to describe classification mistakes: True Class + True Class - Total Predicted Class + True Positive (TP) False Positive (FP) P* Predicted Class - False Negative (FN) True Negative (TN) N* Total P N Using these we can define statistics that describe classifier performance Name Definition Synonyms False Positive Rate (FPR) FP / N Type-I error, 1-Specificity True Positive Rate (TPR) TP / P 1 - Type-II error, power, sensitivity, recall Positive Predictive Value (PPV) TP / P* precision, 1-false discovery proportion Negative Predicitve Value (NPV) FN / N* In the credit default case we may want to increase TPR (recall, make sure we catch all defaults) at the expense of FPR (1-Specificity, clients we lose because we think they will default) This leads to a natural question: Can we adjust our classifiers TPR and FPR? Remember we are classifying Yes if \\[ \\log \\frac{P(Y=\\mathtt{Yes}|X)}{P(Y=\\mathtt{No}|X)} &gt; 0 \\Rightarrow \\\\ P(Y=\\mathtt{Yes}|X) &gt; 0.5 \\] What would happen if we use \\(P(Y=\\mathtt{Yes}|X) &gt; 0.2\\)? library(ROCR) pred &lt;- prediction(lda_pred$posterior[,&quot;Yes&quot;], Default$default) layout(cbind(1,2)) plot(performance(pred, &quot;tpr&quot;)) plot(performance(pred, &quot;fpr&quot;)) A way of describing the TPR and FPR tradeoff is by using the ROC curve (Receiver Operating Characteristic) and the AUROC (area under the ROC) auc &lt;- unlist(performance(pred, &quot;auc&quot;)@y.values) plot(performance(pred, &quot;tpr&quot;, &quot;fpr&quot;), main=paste(&quot;LDA AUROC=&quot;, round(auc, 2)), lwd=1.4, cex.lab=1.7, cex.main=1.5) Consider comparing an LDA model using all predictors in the dataset. full_lda &lt;- lda(default~., data=Default) full_lda_preds &lt;- predict(full_lda, Default) pred_list &lt;- list( balance_lda = lda_pred$posterior[,&quot;Yes&quot;], full_lda = full_lda_preds$posterior[,&quot;Yes&quot;], dummy = rep(0, nrow(Default))) pred_objs &lt;- lapply(pred_list, prediction, Default$default) aucs &lt;- sapply(pred_objs, function(x) unlist( performance(x, &quot;auc&quot;)@y.values)) roc_objs &lt;- lapply(pred_objs, performance, &quot;tpr&quot;, &quot;fpr&quot;) for (i in seq(along=roc_objs)) { plot(roc_objs[[i]], add = i != 1, col=i, lwd=3, cex.lab=1.5) } legend(&quot;bottomright&quot;, legend=paste(gsub(&quot;_&quot;, &quot; &quot;, names(pred_list)), &quot;AUROC=&quot;,round(aucs, 2)), col=1:3, lwd=3, cex=2) Another metric that is frequently used to understand classification errors and tradeoffs is the precision-recall curve: library(caTools) pr_objs &lt;- lapply(pred_objs, performance, &quot;prec&quot;, &quot;rec&quot;) for (i in seq(along=pr_objs)) { plot(pr_objs[[i]], add = i != 1, col=i, lwd=3, cex.lab=1.5) } legend(&quot;bottomleft&quot;, legend=paste(gsub(&quot;_&quot;, &quot; &quot;, names(pred_list))), col=1:3, lwd=3, cex=2) 27.7 Summary We approach classification as a class probability estimation problem. Logistic regression and LDA partition predictor space with linear functions. Logistic regression learns parameter using Maximum Likelihood (numerical optimization), while LDA learns parameter using means and variances (and assuming normal distribution) Error and accuracy statistics are not enough to understand classifier performance. Classifications can be done using probability cutoffs to trade, e.g., TPR-FPR (ROC curve), or precision-recall (PR curve). Area under ROC or PR curve summarize classifier performance across different cutoffs. "],
["solving-linear-ml-problems.html", "28 Solving linear ML problems 28.1 Case Study 28.2 Gradient Descent 28.3 Stochastic gradient descent 28.4 Parallelizing gradient descent", " 28 Solving linear ML problems In this unit we address the question: How to fit the type of analysis methods we’ve seen so far? We saw previously that learning methods based on group-by and summarize type of workflows, e.g., trees and LDA can be fit efficiently using a shared-nothing parallel architecture like Map-Reduce. This leaves other learning methods we have seen, like linear and logistic regression. In those cases, the key insight to answer this question is to recognize that these methods were presented as optimization problems and we can devise optimization algorithms that process data efficiently. We will use linear regression as a case study of how this insight would work. 28.1 Case Study Let’s use linear regression with one predictor, no intercept as a case study. Given: Training set \\(\\{(x_1, y_1), \\ldots, (x_n, y_n)\\}\\), with continuous response \\(y_i\\) and single predictor \\(x_i\\) for the \\(i\\)-th observation. Do: Estimate parameter \\(\\beta_1\\) in model \\(y=\\beta_1 x\\) to solve \\[ \\min_{\\beta_1} L(\\beta_1) = \\frac{1}{2} \\sum_{i=1}^n (y_i - \\beta_1 x_i)^2 \\] And suppose we want to fit this model to the following (simulated) data: set.seed(1234) true_beta &lt;- 5 x &lt;- runif(100, -10, 10) y &lt;- x * true_beta + rnorm(100, mean=0, sd=sqrt(10)) plot(x,y,pch=19,cex=1.4,main=&quot;Simulated Data&quot;, cex.lab=1.5, cex.main=2) abline(a=0, b=true_beta, col=&quot;red&quot;, lwd= 2) Our goal is then to find the value of \\(\\beta_1\\) that minimizes mean squared error. This corresponds to finding one of these many possible lines: Each of which has a specific error for this dataset: Insights: As we saw before in class, loss is minimized when the derivative of the loss function is 0 and, the derivative of the loss (with respect to \\(\\beta_1\\) ) at a given estimate \\(\\beta_1\\) suggests new values of \\(\\beta_1\\) with smaller loss! Let’s take a look at the derivative: \\[ \\frac{\\partial}{\\partial \\beta_{1}} L(\\beta_1) = \\frac{\\partial}{\\partial \\beta_{1}} \\frac{1}{2} \\sum_{i=1}^n (y_i - \\beta_1 x_i)^2 \\\\ {} = \\sum_{i=1}^n (y_i - \\beta_1 x_i) \\frac{\\partial}{\\partial \\beta_1} (y_i - \\beta_1 x_i) \\\\ {} = \\sum_{i=1}^n (y_i - \\beta_1 x_i) (-x_i) \\] and plot it for our case study data: 28.2 Gradient Descent This plot suggests an algorithm: Initialize \\(\\beta_1=0\\) Repeat until convergence Set \\(\\beta_1 = \\beta_1 + \\alpha \\sum_{i=1}^n (y_i - f(x_i)) x_i\\) This algorithm is called gradient descent in the general case. The basic idea is to move the current estimate of \\(\\beta_1\\) in the direction that minimizes loss the fastest. Another way of calling this algorithm is Steepest Descent. This is a full implementation of this algorithm in R: # Implementation of gradient descent for least squares regression # for a single predictor (x) # # There is some code here that is only used to generate illustrative plots and would not be part of real solver gradient_descent &lt;- function(x, y, tol=1e-6, maxit=50, plot=FALSE) { # initialize estimate beta_1 &lt;- 0; old_beta_1 &lt;- Inf; i &lt;- 0; beta_keep &lt;- NA # compute loss at first estimate loss &lt;- compute_loss(beta_1, x, y); loss_keep &lt;- NA # starting step size alpha &lt;- 1e-3 difference &lt;- Inf # check for convergence # (in practice, we do include a limit on the number of iterations) while ((difference &gt; tol) &amp;&amp; (i &lt; maxit)) { cat(&quot;it: &quot;, i, &quot; beta: &quot;, round(beta_1, 2), &quot;loss: &quot;, round(loss, 2), &quot; alpha: &quot;, round(alpha, 6), &quot;\\n&quot;) # this piece of code just adds steps to an existing plot if (plot &amp;&amp; !is.na(beta_keep) &amp;&amp; !is.na(loss_keep)) { suppressWarnings(arrows(beta_keep, loss_keep, beta_1, loss, lty=2, col=&quot;blue&quot;)) } # store the last estimate for plotting beta_keep &lt;- beta_1; loss_keep &lt;- loss; # store the last estimate to check convergence old_beta_1 &lt;- beta_1 # update estimate f &lt;- beta_1 * x resid &lt;- y - f beta_1 &lt;- beta_1 + alpha * sum(resid * x) # compute difference after taking step # to check convergence difference &lt;- (beta_1 - old_beta_1)^2 / (old_beta_1)^2 # compute loss and derivative for updated estimate loss &lt;- compute_loss(beta_1, x, y) i &lt;- i+1 # shorten the step size if ((i %% 3) == 0) alpha &lt;- alpha / 2 } if (plot) { suppressWarnings(arrows(beta_keep, loss_keep, beta_1, loss, lty=2, col=&quot;blue&quot;)) } beta_1 } Let’s run this algorithm and track what it does: ## it: 0 beta: 0 loss: 405.87 alpha: 0.001 ## it: 1 beta: 16.11 loss: 2004.46 alpha: 0.001 ## it: 2 beta: -19.85 loss: 9969.82 alpha: 0.001 ## it: 3 beta: 60.41 loss: 49659.42 alpha: 5e-04 ## it: 4 beta: -29.17 loss: 18852.85 alpha: 5e-04 ## it: 5 beta: 26.02 loss: 7159.08 alpha: 5e-04 ## it: 6 beta: -7.98 loss: 2720.28 alpha: 0.00025 ## it: 7 beta: 2.5 loss: 104.56 alpha: 0.00025 ## it: 8 beta: 4.51 loss: 8.19 alpha: 0.00025 ## it: 9 beta: 4.89 loss: 4.64 alpha: 0.000125 ## it: 10 beta: 4.93 loss: 4.55 alpha: 0.000125 ## it: 11 beta: 4.95 loss: 4.52 alpha: 0.000125 ## it: 12 beta: 4.96 loss: 4.51 alpha: 6.2e-05 This algorithm is referred to as “Batch” gradient descent, since we take a step (update \\(\\beta_1\\)) by calculating derivative with respect to all \\(n\\) observations in our dataset. For clarity, let’s write out the update equation again: \\[ \\beta_1 = \\beta_1 + \\alpha \\sum_{i=1}^n (y_i - f(x_i, \\beta_1)) x_i \\] where \\(f(x_i) = \\beta_1 x_i\\). For multiple predictors (e.g., adding an intercept), this generalizes to the gradient i.e., the vector of first derivatives of loss with respect to parameters. In this case, the model sets \\(f(\\mathbf{x}_i, \\mathbf{\\beta}) = \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_p x_{ip}\\) and the gradient given by partial derivatives for each parameter \\[ \\nabla_{\\mathbf{\\beta}}L(\\mathbf{\\beta}) = \\left[ \\begin{array}{c} \\frac{\\partial L(\\mathbf{\\beta})}{\\partial \\beta_0} \\\\ \\frac{\\partial L(\\mathbf{\\beta})}{\\partial \\beta_1} \\\\ \\vdots \\\\ \\frac{\\partial L(\\mathbf{\\beta})}{\\partial \\beta_p} \\\\ \\end{array} \\right] \\] The update equation is exactly the same for least squares regression \\[ \\mathbf{\\beta} = \\mathbf{\\beta} + \\alpha \\sum_{i=1}^n (y_i - f(\\mathbf{x}_i, \\beta)) \\mathbf{x}_i \\] where \\(f(\\mathbf{x}_i, \\mathbf{\\beta}) = \\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_p x_{ip}\\) and \\(\\mathbf{x}_i\\) is the vector of data values for entity \\(i\\) with an additional entry of 1 to account for the intercept parameter \\(\\beta_0\\): \\[ \\mathbf{x}_i = \\left[ \\begin{array}{c} 1 \\\\ x_{i1} \\\\ x_{i2} \\\\ \\vdots \\\\ x_{ip} \\end{array} \\right] \\] Gradiest descent falls within a family of optimization methods called first-order methods (first-order means they use derivatives only). These methods have properties amenable to use with very large datasets: Inexpensive updates “Stochastic” version can converge with few sweeps of the data “Stochastic” version easily extended to streams Easily parallelizable Drawback: Can take many steps before converging 28.2.1 Logistic Regression Gradient descent is also used to solve the logistic regression problem. The same procedure follows: (1) define a loss function; (2) derive the update equation; (3) run the iterative gradient descent algorithm. Let’s take a look at the first two steps in this case. For logistic regression, we turn to maximum likelihood to formulate a loss function. We mentioned this concept previously, but it follows directly from the “inverse problem” view of data analysis we have been using throughout this unit. For logistic regression, we will assume that the data is generated from a Bernoulli probability distribution. Once we make that assumption, we setup an inverse problem that looks for parameter values that maximizes the probability of the data we observe under this assumption. For the logistic regression problem we are given dataset \\(\\{\\langle \\mathbf{x}_1, y_1\\rangle, \\ldots, \\langle \\mathbf{x}_n, y_n \\rangle \\}\\), where outcomes \\(y_i \\in \\{0,1\\}\\) since we are learning a binary classification problem. The goal is to estimate parameters \\(\\mathbf{\\beta}\\) in model \\[ \\log{ \\frac{p(Y=1 | \\mathbf{X}=\\mathbf{x})}{1-p(Y=1 | \\mathbf{X}=\\mathbf{x})}} = \\beta_0 + \\beta_0 x_{1} + \\cdots + \\beta_p x_{p} \\] To establish a loss function we first assume a model for data generation. The assumption we make here is if an entity has attribute values \\(\\mathbf{x}\\), then the outcome \\(Y\\) is a \\(\\mathrm{Bernoulli}(p(\\mathbf{\\beta}; \\mathbf{x}))\\) random variable and derive this probability from the equation above as \\[ p(\\mathbf{\\beta}; \\mathbf{x}) = \\frac{e^{f(\\mathbf{\\beta}; \\mathbf{x})}}{1+e^{f(\\mathbf{\\beta}; \\mathbf{x})}} \\] Note that we use the same notation \\(f(\\mathbf{\\beta}; \\mathbf{x})\\) as we did in linear regression. Now, we can ask, what is the probability of the data we observe for entity \\(i\\) under this model? We can write this probability in this form: \\[ p(\\mathbf{\\beta}; \\mathbf{x}_i)^{y_i}(1-p(\\mathbf{\\beta};\\mathbf{x}_i))^{(1-y_i)} \\] Note that this expression equals \\(p(\\mathbf{\\beta};\\mathbf{x}_i)\\) if \\(y_i=1\\) and \\(1-p(\\mathbf{\\beta}; \\mathbf{x}_i)\\) othwerwise, which is the behavoir we desire for this probability model. Now, we can put these together for all observed entities since we assume that these are generated independently to get a likelihood function: \\[ \\mathcal{L}(\\mathbf{\\beta}) = \\prod_{i=1}^n p_i(\\mathbf{\\beta}; \\mathbf{x}_i)^{y_i}(1-p_i(\\mathbf{\\beta};\\mathbf{x}_i))^{(1-y_i)} \\] Now, we need to turn this into a loss function we can minimize. The likelihood function we wrote down is one we would maximize. Also, it is usually more convenient to work with the logarithm of likelihoods. So the loss function we use for gradient descent is the negative log likelihood \\[ L(\\mathbf{\\beta}) = \\sum_{i=1}^n -y_i f(\\mathbf{\\beta}; \\mathbf{x}_i) + \\log(1+e^{f(\\mathbf{\\beta};\\mathbf{x}_i)}) \\] We leave this derivation as an exercise, but note that \\[ \\log{p(\\mathbf{\\beta};\\mathbf{x}_i)} = f(\\mathbf{\\beta};\\mathbf{x}_i) - \\log{(1+e^{f(\\mathbf{\\beta};\\mathbf{x}_i)})} \\] and \\[ \\log{(1-p(\\mathbf{\\beta};\\mathbf{x}_i))} = - \\log{(1+e^{f(\\mathbf{\\beta};\\mathbf{x}_i)})} \\] So, now that we have a loss function, we need to derive it’s gradient to use the gradient descent algorithm. \\[ \\begin{aligned} \\nabla_{\\mathbf{\\beta}}L(\\mathbf{\\beta}) &amp; = &amp; \\nabla_{\\mathbf{\\beta}} \\sum_{i=1}^n -y_i f(\\mathbf{\\beta};\\mathbf{x}) + \\log{(1+e^{f(\\mathbf{\\beta}; \\mathbf{x}_i)})} \\\\ {} &amp; = &amp; \\sum_{i=1}^n -y_i \\nabla_{\\mathbf{\\beta}}f(\\mathbf{\\beta};\\mathbf{x}_i) + \\nabla_{\\mathbf{\\beta}} \\log{(1+e^{f(\\mathbf{\\beta};\\mathbf{x}_i)})}\\\\ {} &amp; = &amp; \\sum_{i=1}^n -y_i \\mathbf{x}_i + \\frac{1}{1+e^{f(\\mathbf{\\beta};\\mathbf{x}_i)}} \\nabla_{\\mathbf{\\beta}} (1+e^{f(\\mathbf{\\beta};\\mathbf{x}_i)}) \\\\ {} &amp; = &amp; \\sum_{i=1}^n -y_i \\mathbf{x}_i + \\frac{e^{f(\\mathbf{\\beta};\\mathbf{x}_i)}}{1+e^{f(\\mathbf{\\beta};\\mathbf{x}_i)}} \\nabla_{\\mathbf{\\beta}}f(\\mathbf{\\beta};\\mathbf{x}_i) \\\\ {} &amp; = &amp; \\sum_{i=1}^n -y_i \\mathbf{x}_i + p(\\mathbf{\\beta};\\mathbf{x}_i) \\mathbf{x}_i \\\\ {} &amp; = &amp; \\sum_{i=1}^n (p(\\mathbf{\\beta};\\mathbf{x}_i) - y_i) \\mathbf{x}_i \\end{aligned} \\] Note the nice similarity to the gradient for linear regression. It multiplies each data (expanded) data vector \\(\\mathbf{x}_i\\) by the difference between a prediction, in this case the probability that the outcome \\(y_i=1\\) and the observed outcome \\(y_i\\). You will see that a similar pattern holds for many other probability models. 28.3 Stochastic gradient descent Key Idea: Update parameters using update equation one observation at a time: Initialize \\(\\beta=\\mathbf{0}\\), \\(i=1\\) Repeat until convergence For \\(i=1\\) to \\(n\\) Set \\(\\beta = \\beta + \\alpha (y_i - f(\\mathbf{x}_i, \\beta)) \\mathbf{x}_i\\) This is a full implementation of stochastic gradient descent for our example dataset: # Implementation of stochastic gradient descent for least squares regression # for a single predictor (x) # # There is some code here that is only used to generate illustrative plots stochastic_gradient_descent &lt;- function(x, y, tol=1e-6, maxit=50, plot=FALSE) { n &lt;- length(y) # initialize estimate beta_1 &lt;- 0; i &lt;- 0; beta_keep &lt;- NA # compute loss at first estimate loss &lt;- compute_loss(beta_1, x, y); loss_keep &lt;- NA # initial step size alpha &lt;- 1e-3 difference &lt;- Inf # check for convergence # (in practice a max number of iterations is used) while ((difference &gt; tol) &amp;&amp; (i &lt; maxit)) { cat(&quot;it: &quot;, i, &quot; beta: &quot;, round(beta_1, 2), &quot;loss: &quot;, round(loss, 2), &quot; alpha: &quot;, round(alpha, 6), &quot;\\n&quot;) # store last estimate to check convergence old_beta_1 &lt;- beta_1 # iterate over observations for (j in seq(1,n)) { # add step to plot if (plot &amp;&amp; !is.na(beta_keep) &amp;&amp; !is.na(loss_keep)) { suppressWarnings(arrows(beta_keep, loss_keep, beta_1, loss, lty=2, col=&quot;blue&quot;)) } # store last estimate and loss for plotting beta_keep &lt;- beta_1; loss_keep &lt;- loss; # update estimate with j-th observation f &lt;- beta_1 * x[j] resid &lt;- y[j] - f beta_1 &lt;- beta_1 + alpha * resid * x[j] # compute loss with new estimate loss &lt;- compute_loss(beta_1, x, y) } # check difference between current and old estimate # to check convergence difference &lt;- (beta_1 - old_beta_1)^2 / old_beta_1^2 i &lt;- i+1 # update step size if ((i %% 5) == 0) alpha &lt;- alpha / 2 } if (plot) { suppressWarnings(arrows(beta_keep, loss_keep, beta_1, loss, lty=2, col=&quot;blue&quot;)) } beta_1 } Let’s run this and see what it does: ## it: 0 beta: 0 loss: 405.87 alpha: 0.001 ## it: 1 beta: 4.81 loss: 4.97 alpha: 0.001 ## it: 2 beta: 4.99 loss: 4.5 alpha: 0.001 ## it: 3 beta: 4.99 loss: 4.5 alpha: 0.001 The stochastic gradient descent algorithm can easily adapt to data streams where we receive observations one at a time and assume they are not stored. This setting falls in the general category of online learning. 28.4 Parallelizing gradient descent Gradient descent algorithms are easily parallelizable: Split observations across computing units For each step, compute partial sum for each partition (map), compute final update (reduce) \\[ \\beta = \\beta + \\alpha * \\sum_{\\mathrm{partition}\\; p} \\sum_{i \\in p} (y_i - f(\\mathbf{x_i}, \\beta)) \\mathbf{x}_i \\] This observation has resulted in their implementation if systems for large-scale learning: Vowpal Wabbit Implements general framework of (sparse) stochastic gradient descent for many optimization problems R interface: [http://cran.r-project.org/web/packages/RVowpalWabbit/index.html] Spark MLlib Implements many learning algorithms using Spark framework we saw previously Some access to the MLlib API via R, but built on primitives accessible through SparkR library we saw previously "],
["tree-based-methods.html", "29 Tree-Based Methods 29.1 Regression Trees 29.2 Classification (Decision) Trees 29.3 Specifics of the partitioning algorithm 29.4 Properties of Tree Method 29.5 Random Forests 29.6 Tree-based methods summary", " 29 Tree-Based Methods We saw in previous units the limitation of using linear methods for classification. In particular, the partition of predictor space into regions using a linear model like logistic regression is very limiting. In this unit, we look at a set of elegant and versatile methods that allow these regions to take more complex shapes, but still produce models that are interpretable. These are very popular, well-known and studied methods in Statistical Learning. We will concentrate on Regression and Decision Trees and their extension to Random Forests. 29.1 Regression Trees Consider a task where we are trying to predict a car’s fuel consumption in miles per gallon based on the car’s weight. A linear model in this case is not a good fit. Let’s take a look at what a regression tree estimates in this case. library(tree) tree &lt;- tree(mpg~weight, data=Auto) plot(tree) text(tree, pretty=0, cex=1.3) The decision trees partitions the weight predictor into regions based on its value. We can show this graphically as below. The idea behind the regression tree is that outcome \\(Y\\) (mpg in this case) is estimated (or predicted) to be it’s mean within each of the data partitions. Think of it as the conditional mean of \\(Y\\) where conditioning is given by this region partitioning. Regression and decision trees operate by prediction an outcome variable \\(Y\\) by partitioning feature (predictor) space. The regression tree model then: Partitions space into \\(J\\) non-overlapping regions, \\(R_1, R_2, \\ldots, R_J\\). For every observation that falls within region \\(R_j\\), predict response as mean of response for training observations in \\(R_j\\). The important observation is that Regression Trees create partition recursively For example, consider finding a good predictor \\(j\\) to partition space its axis. A recursive algorithm would look like this: Find predictor \\(j\\) and value \\(s\\) that minimize RSS: \\[ \\sum_{i:\\, x_i \\in R_1(j,s))} (y_i - \\hat{y}_{R_1})^2 + \\sum_{i:\\, x_i \\in R_2(j,s))} (y_i - \\hat{y}_{R_2})^2 \\] Where \\(R_1\\) and \\(R_2\\) are regions resulting from splitting observations on predictor \\(j\\) and value \\(s\\): \\[ R_1(j,s) = \\{X|X_j &lt; s\\} \\mathrm{ and } R_2(j,s) \\{X|X_j \\geq s\\} \\] This is then applied recursively to regions \\(R_1\\) and \\(R_2\\). Within each region a prediction is made using \\(\\hat{y}_{R_j}\\) which is the mean of the response \\(Y\\) of observations in \\(R_j\\). Consider building a model that used both horsepower and weight. In this plot the value of the response \\(Y\\) is indicated by the size of the point. This is what a decision tree would look like for these two predictors: tree &lt;- tree(mpg~horsepower+weight, data=Auto) plot(tree) text(tree, pretty=0) 29.2 Classification (Decision) Trees Classification, or decision trees, are used in classification problems, where the outcome is categorical. The same partitioning principle holds, but now, each region predicts the majority class for training observations within region. The recursive partitioning method requires a score function to choose predictors (and values) to partition with. In classification we could use a naive approach of looking for partitions that minimize training error. However, better performing approaches use more sophisticated metrics, which we will see shortly. Let’s look at how a classification tree performs on a credit card default dataset. default_tree &lt;- tree(default~student+balance+income, data=Default) plot(default_tree) text(default_tree, pretty=0) 29.3 Specifics of the partitioning algorithm 29.3.1 The predictor space Suppose we have \\(p\\) explanatory variables \\(X_1,\\ldots,X_p\\) and \\(N\\) observations. Each of the \\(X_i\\) can be a numeric variable: there are \\(n-1\\) possible splits an ordered factor (categorical variable): there are \\(k-1\\) possible splits an unordered factor: \\(2^{k-1}-1\\) possible splits. 29.3.2 Learning Strategy The general procedure for tree learning is the following: Grow an overly large tree using forward selection as follows: at each step, find the best split among all attributes. Grow until all terminal nodes either have \\(&lt; m\\) (perhaps \\(m=1\\)) data points are “pure” (all points in a node have [almost] the same outcome). Prune the tree back, creating a nested sequence of trees, decreasing in complexity 29.3.3 Tree Growing The recursive partitioning algorithm is as follows: INITIALIZE All cases in the root node REPEAT Find optimal allowed split Partition leaf according to split STOP Stop when pre-defined criterion is met A problem in tree construction is how to use the training data to determine the binary splits of dataset \\(\\mathcal{X}\\) into smaller and smaller pieces. The fundamental idea is to select each split of a subset so that the data in each of the descendent subsets are “purer” than the data in the parent subset. 29.3.4 Deviance as a measure of impurity A simple approach is to assume a multinomial model and then use deviance as a definition of impurity. Assume \\(Y \\in \\mathcal{G}=\\{1,2,\\ldots,k\\}\\). At each node \\(i\\) of a classification tree we have a probability distribution \\(p_{ik}\\) over the \\(k\\) classes. We observe a random sample \\(n_{ik}\\) from the multinomial distribution specified by the probabilities \\(p_{ik}\\). Given \\(X\\), the conditional likelihood is then proportional to \\(\\prod_{(\\text{leaves } i)} \\prod_{(\\text{classes } k)} p_{ik}^{n_{ik}}\\). Define a deviance \\(D=\\sum D_i\\), where \\(D_i=-2\\sum_k n_{ik} \\log(p_{ik})\\). Estimate \\(p_{ik}\\) by \\(\\hat{p}_{ik}=\\frac{n_{ik}}{n_i}\\). 29.3.5 Other measures of impurity Other commonly used measures of impurity at a node \\(i\\) of a classification tree are missclasification rate: \\(\\frac{1}{n_i} \\sum_{j\\in A_i} I(y_j \\neq k_i)=1-\\hat{p}_{ik_i}\\) entropy: \\(\\sum p_{ik} \\log(p_{ik})\\) GINI index: \\(\\sum_{j\\neq k} p_{ij}p_{ik} = 1-\\sum_k p_{ik}^2\\) where \\(k_i\\) is the most frequent class in node \\(i\\). For regression trees we use the residual sum of squares: \\[ D = \\sum_{\\text{cases } j} (y_j-\\mu_{[j]})^2 \\] where \\(\\mu_{[j]}\\) is the mean values in the node that case \\(j\\) belongs to. 29.3.6 Tree Pruning Grow a big tree \\(T\\) Consider snipping off terminal subtrees (resulting in so-called rooted subtrees) Let \\(R_i\\) be a measure of impurity at leaf \\(i\\) in a tree. Define \\(R=\\sum_i R_i\\) Define size as the number leaves in a tree Let \\(R_{\\alpha} = R + \\alpha \\times \\mathrm{size}\\) The set of rooted subtrees of \\(T\\) that minimize \\(R_{\\alpha}\\) is nested. 29.4 Properties of Tree Method Good properties of Regression and Classification trees include: Decision trees are very “natural” constructs, in particular when the explanatory variables are catgorical (and even better when they are binary) Trees are easy to explain to non-data analysts The models are invariant under transformations in the predictor space Multi-factor responses are easily dealt with The treatment of missing values is more satisfactory than for most other models The models go after interactions immediately, rather than as an afterthought Tree growth is much more efficient than described here However, they do have important issues to address Tree space is huge, so we may need lots of data We might not be able to find the best model at all as it is a greedy algorithm It can be hard to assess uncertainty in inference about trees Results can be quite variable (tree selection is not very stable) Simple trees usually don’t have a lot of predictive power 29.5 Random Forests Random Forests are a very popular approach that addresses these shortcomings via resampling of the training data. Their goal is to improve prediction performance and reduce instability by averaging multiple decision trees (a forest constructed with randomness). It uses two ideas to accomplish this. The first idea is Bagging (bootstrap aggregation) General scheme: 1. Build many decision trees \\(T_1, T_2, \\ldots, T_B\\) from training set 2. Given a new observation, let each \\(T_j\\) predict \\(\\hat{y}_j\\) 3. For regression: predict average \\(\\frac{1}{B} \\sum_{j=1}^B \\hat{y}_j\\), for classification: predict with majority vote (most frequent class) How do we get many decision trees from a single training set? For this we use the bootstrap resampling technique. To create \\(T_j, \\, j=1,\\ldots,B\\) from training set of size \\(n\\): create a bootstrap training set by sampling \\(n\\) observations from training set with replacement build a decision tree from bootstrap training set The second idea used in Random Forests is to use a random selection of features to split when deciding partitions. Specifically, when building each tree \\(T_j\\), at each recursive partition only consider a randomly selected subset of predictors to check for best split. This reduces correlation between trees in forest, improving prediction accuracy. Let’s look at the same car dataset again set.seed(1234) train_indices &lt;- sample(nrow(Auto), nrow(Auto)/2) train_set &lt;- Auto[train_indices,] test_set &lt;- Auto[-train_indices,] library(randomForest) auto_rf &lt;- randomForest(mpg~cylinders+displacement+horsepower+weight+acceleration+year+origin, importance=TRUE, mtry=3, data=train_set) Let’s plot the predicted miles per gallon given by a random forest compared to the observed miiles per gallon in the training dataset. Now let’s look at the same plot on a testing dataset. A disadvantage of random forests is that we lose interpretability. However, we can use the fact that a bootstrap sample was used to construct trees to measure variable importance from the random forest. Here is a table of variable importance for the random forest we just constructed. variable_importance &lt;- importance(auto_rf) knitr::kable(head(round(variable_importance, digits=2))) %I ncMSE In cNodePurity cylinders 15.11 2328.05 displacement 20.00 2480.60 horsepower 21.39 2779.68 weight 19.88 2325.81 acceleration 8.01 377.69 year 43.20 1341.62 And a barplot of the same data. 29.6 Tree-based methods summary Tree-based methods are very interpretable prediction models. For which some inferential tasks are possible (e.g., variable importance in random forests), but are much more limited than the linear models we saw previously. These methods are very commonly used across many application domains and Random Forests often perform at state-of-the-art for many tasks. "],
["model-selection.html", "30 Model Selection 30.1 Cross Validation 30.2 Validation Set 30.3 Resampled validation set 30.4 Leave-one-out Cross-Validation 30.5 k-fold Cross-Validation 30.6 Cross-Validation in Classification 30.7 Comparing models using cross-validation 30.8 Summary", " 30 Model Selection Our discussion on regression and classification has been centered on fitting models by minizing error or maximizing likelihood given a dataset (also referred to as training data). This is usually fine when we want to use our model for explanatory or inferential tasks. Or when we use relatively inflexible models, like linear regression or logistic regression. However, as our interests shift to prediction and more complex models, like non-linear regression, Tree-based methods or Support Vector Machines, this is usually not sufficient. In these cases, our goal is to avoid building models that are too specific for the dataset we have on hand. Complex models can easily overfit our training data, in which case we don’t learn much about the population from which we obtain the training data and instead learn only about the training data itself. We say that we want to learn, or train models that generalize beyond the training data to other, unseen, data from the same population. This leads to a bit of an issue. How do we measure our models ability to predict unseen data, when we only have access to training data? 30.1 Cross Validation The most common method to evaluate model generalization performance is cross-validation. It is used in two essential data analysis phases: Model Selection and Model Assessment. In Model Selection, we decide how complex of a model we should fit. Consider a linear regression example: I will fit a linear regression model, what predictors should be included?, interactions?, data transformations? Another example is what classification tree depth to use. In Model Assessment, we determine how well does our selected model performs as a general model. Example: I’ve built a linear regression models, with specific predictors. How well will it perform on unseen data? The same question can be asked of a classification tree (of specific depth). Cross-validation is a resampling method to obtain estimates of test error rate (or any other performance measure on unseen data). In some instances, you will have a large predefined test dataset that you should never use when training. In the absence of access to this kind of dataset, cross validation can be used. 30.2 Validation Set The simplest option to use cross-validation is to create a validation set, where our dataset is randomly divided into training and validation sets. Then the validation is set aside, and not used at until until we are ready to compute test error rate (once, don’t go back and check if you can improve it). Let’s look at our running example using automobile data, where we want to build a regression model capable of predicting miles per gallon given other auto attributes. We saw in previous lectures that a linear regression model was not appropriate for this dataset. So instead we will use polynomial regression as an illustrative example. library(ggplot2) library(ISLR) data(Auto) ggplot(Auto, aes(x=horsepower, y=mpg)) + geom_point() + geom_smooth() In this case our regression model (for a single predictor \\(x\\)) is given as a \\(d\\) degree polynomial. \\[ \\mathbb{E}[Y|X=x] = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\cdots + \\beta_d x^d \\] In the Model Selection case, we want to decide what degree \\(d\\) we should use to model this data. Using the validation set method, we split our data into a training set, fit the regression model with different polynomial degrees \\(d\\) on the training set, and measure test error on the validation set. set.seed(1234) in_validation &lt;- sample(nrow(Auto), nrow(Auto)/2) validation_set &lt;- Auto[in_validation,] training_set &lt;- Auto[-in_validation,] library(broom) library(dplyr) degrees &lt;- seq(1, 10) error_rates &lt;- sapply(degrees, function(deg) { fit &lt;- lm(mpg~poly(horsepower, degree=deg), data=training_set) predicted &lt;- predict(fit, newdata=validation_set) mean((validation_set$mpg - predicted)^2) }) plot(degrees, error_rates, type=&quot;b&quot;, xlab=&quot;Polynomial Degree&quot;, ylab=&quot;Mean Squared Error&quot;, pch=19, lwd=1.4, cex=1.4) 30.3 Resampled validation set This approach can be prone to sampling issues. It can be highly variable as error rate is a random quantity and depends on observations in training and validation sets. We can improve our estimate of test error by averaging multiple measurements of it (remember the law of large numbers). We can do so by replicating our validation resampling 10 times (with different validation and training sets) and averaging the resulting test errors. 30.4 Leave-one-out Cross-Validation This approach still has some issues. Each of the training sets in our validation approach only uses 50% of data to train, which leads to models that may not perform as well as models trained with the full dataset and thus we can overestimate error. To alleviate this situation, we can extend our approach to the extreme. Make each single training point it’s own validation set. Procedure: For each observation \\(i\\) in data set: a. Train model on all but \\(i\\)-th observation b. Predict response for \\(i\\)-th observation c. Calculate prediction error This gives us the following cross-validation estimate of error. \\[ CV_{(n)} = \\frac{1}{n} \\sum_i (y_i - \\hat{y}_i)^2 \\] The advantages of this approach is that now we use \\(n-1\\) observations to train each model and there is no randomness introduced since error is estimated on each sample. However, it has disadvantages as well. Depending on the models we are trying to fit, it can be very costly to train \\(n-1\\) models. Also, the error estimate for each model is highly variable (since it comes from a single datapoint). For linear models (and some non-linear models) there is a nice trick that allows one to compute (exactly or approximately) LOOCV from the full data model fit which we will not get into here. 30.5 k-fold Cross-Validation This discussion leads us to the most commonly used cross-validation approach k-fold Cross-Validation. Procedure: Partition observations randomly into \\(k\\) groups (folds). For each of the \\(k\\) groups of observations: - Train model on observations in the other \\(k-1\\) folds - Estimate test-set error (e.g., Mean Squared Error) Compute average error across \\(k\\) folds \\[ CV_{(k)} = \\frac{1}{k} \\sum_i MSE_i \\] where \\(MSE_i\\) is mean squared error estimated on the \\(i\\)-th fold In this case, we have fewer models to fit (only \\(k\\) of them), and there is less variance in each of the computed test error estimates in each fold. It can be shown that there is a slight bias (over estimating usually) in error estimate obtained from this procedure. 30.6 Cross-Validation in Classification Each of these procedures can be used for classification as well. In this case we would substitute MSE with performance metric of choice. E.g., error rate, accuracy, TPR, FPR, AUROC. Note however that not all of these work with LOOCV (e.g. AUROC since it can’t be defined over single data points). 30.7 Comparing models using cross-validation Suppose you want to compare two classification models (logistic regression vs. a decision tree) on the Default dataset. We can use Cross-Validation to determine if one model is better than the other, using a paired \\(t\\)-test for example. library(ISLR) library(cvTools) library(tree) data(Default) fold_indices &lt;- cvFolds(n=nrow(Default), K=10) error_rates &lt;- sapply(1:10, function(fold_index) { test_indices &lt;- which(fold_indices$which == fold_index) test_set &lt;- Default[test_indices,] train_set &lt;- Default[-test_indices,] logis_fit &lt;- glm(default~., data=train_set, family=&quot;binomial&quot;) logis_pred &lt;- ifelse(predict(logis_fit, newdata=test_set, type=&quot;response&quot;) &gt; 0.5, &quot;Yes&quot;, &quot;No&quot;) logis_error &lt;- mean(test_set$default != logis_pred) tree_fit &lt;- tree(default~., data=train_set) pruned_tree &lt;- prune.tree(tree_fit, best=3) tree_pred &lt;- predict(pruned_tree, newdata=test_set, type=&quot;class&quot;) tree_error &lt;- mean(test_set$default != tree_pred) c(logis_error, tree_error) }) rownames(error_rates) &lt;- c(&quot;logis&quot;, &quot;tree&quot;) error_rates &lt;- as.data.frame(t(error_rates)) library(tidyr) library(dplyr) error_rates &lt;- error_rates %&gt;% mutate(fold=1:n()) %&gt;% gather(method,error,-fold) error_rates %&gt;% head() %&gt;% knitr::kable(&quot;html&quot;) fold method error 1 logis 0.028 2 logis 0.037 3 logis 0.021 4 logis 0.030 5 logis 0.029 6 logis 0.018 dotplot(error~method, data=error_rates, ylab=&quot;Mean Prediction Error&quot;) lm(error~method, data=error_rates) %&gt;% tidy() %&gt;% knitr::kable() term estimate std.error statistic p.value (Intercept) 0.0267 0.0020306 13.148828 0.0000000 methodtree 0.0030 0.0028717 1.044677 0.3099998 In this case, we do not observe any significant difference between these methods. 30.8 Summary Model selection and assessment are critical steps of data analysis. Resampling methods are general tools used for this purpose. "],
["unsupervised-learning-clustering.html", "31 Unsupervised Learning: Clustering 31.1 Motivating Example 31.2 Some Preliminaries 31.3 Cluster Analysis 31.4 Dissimilarity-based Clustering 31.5 K-means Clustering 31.6 Choosing the number of clusters 31.7 Summary", " 31 Unsupervised Learning: Clustering So far we have seen “Supervised Methods” where our goal is to analyze a response (or outcome) based on various predictors. In many cases, especially for Exploratory Data Analysis, we want methods to extract patterns on variables without analyzing a specific response. Methods for the latter case are called “Unsupervised Methods”. Examples are Principal Component Analysis and Clustering. Interpretation of these methods is much more subjective than in Supervised Learning. For example: if we want to know if a given predictor is related to response, we can perform statistical inference using hypothesis testing. In another example, if we want to know which predictors are useful for prediction: use cross-validation to do model selection. Finally, if we want to see how well we can predict a specific response, we can use cross-validation to report on test error. In unsupervised methods, there is no similar clean evaluation methodology. Nonetheless, they can be very useful methods to understand data at hand. 31.1 Motivating Example Throughout this unit we will use a time series dataset of mortgage affordability as calculated and distributed by Zillow: https://www.zillow.com/research/data/. library(tidyverse) library(readr) library(lubridate) datadir &lt;- &quot;data&quot; url &lt;- &quot;http://files.zillowstatic.com/research/public/Affordability_Wide_2017Q4_Public.csv&quot; filename &lt;- basename(url) datafile &lt;- file.path(datadir, filename) if (!file.exists(datafile)) { download.file(url, file.path(datadir, filename)) } afford_data &lt;- read_csv(datafile) tidy_afford &lt;- afford_data %&gt;% filter(Index == &quot;Mortgage Affordability&quot;) %&gt;% drop_na() %&gt;% filter(RegionID != 0) %&gt;% dplyr::select(RegionID, matches(&quot;^[1|2]&quot;)) %&gt;% gather(time, affordability, matches(&quot;^[1|2]&quot;)) %&gt;% type_convert(col_types=cols(time=col_date(format=&quot;%Y-%m&quot;))) wide_afford_df &lt;- tidy_afford %&gt;% dplyr::select(RegionID, time, affordability) %&gt;% spread(time, affordability) value_mat &lt;- wide_afford_df %&gt;% dplyr::select(-RegionID) %&gt;% as.matrix() The dataset contains affordability measurements for 81 counties with data from 1979 to 2017. Here we plot the time series of affordability for all counties. tidy_afford %&gt;% ggplot(aes(x=time,y=affordability,group=factor(RegionID))) + geom_line(color=&quot;GRAY&quot;, alpha=3/4, size=1/2) + labs(title=&quot;County-Level Mortgage Affordability over Time&quot;, x=&quot;Date&quot;, y=&quot;Mortgage Affordability&quot;) A natural question to ask about this data is, “can we partition counties into groups of counties with similar value trends across time” 31.2 Some Preliminaries Mathematically, the previous sections on “Supervised Learning” we were concerned with estimates that minimize some error function relative to the outcome of interest \\(Y\\): \\[ \\mu(x) = \\arg \\min_{\\theta} E_{Y|X} L(Y, \\beta) \\] In order to do this, explicitly or not, the methods we were using would be concerned with properties of the conditional probability distribution \\(p(Y|X)\\), and do so without concerning itself with probability distribution \\(p(X)\\) of the covariates themselves. In unsupervised learning, we are interested in properties of \\(p(X)\\). In our example, what can we say about the distribution of home value time series? Since the dimensionality of \\(p(X)\\) can be large, unsupervised learning methods seek to find structured representations of \\(p(X)\\) that would be possible to estimate. In clustering we assume that predictor space is partitioned and that \\(p(X)\\) is defined over those partitions. In dimensionality reduction we assume that \\(p(X)\\) is really defined over a space of smaller dimension. We will start studying clustering first. 31.3 Cluster Analysis The high-level goal of cluster analysis is to organize objects (observations) that are similar to each other into groups. We want objects within a group to be more similar to each other than objects in different groups. Central to this high-level goal is how to measure the degree of similarity between objects. A clustering method then uses the similarity measure provided to it to group objects into clusters. library(broom) library(stringr) augmented_data %&gt;% ggplot(aes(x=time, y=affordability)) + geom_line(aes(group=RegionID), color=&quot;GRAY&quot;, alpha=1/2, size=1/2) + facet_wrap(~cluster) + geom_line(data=kmeans_centers, color=&quot;BLACK&quot;, alpha=1/2, size=1/2) + labs(main=&quot;Kmeans Clustering (k=9)&quot;, xlab=&quot;Date&quot;, ylab=&quot;affordability&quot;) + theme(axis.text.x=element_text(angle=45, hjust=1)) This plot shows the result of the k-means algorithm partitioning the data into 16 clusters. We observe that series within each cluster have similar trends. The darker series within each cluster shows the average time series within the cluster. We also see that some clusters correspond to very similar series suggesting that partitioning the data into fewer clusters would be better. 31.4 Dissimilarity-based Clustering For certain algorithms, instead of similarity we work with dissimilarity, often represented as distances. When we have observations defined over attributes, or predictors, we define dissimilarity based on these attributes. Given measurements \\(x_{ij}\\) for \\(i=1,\\ldots,N\\) observations over \\(j=1,\\ldots,p\\) predictors. Suppose we define a dissimilarity \\(d_j(x_{ij}, x_{i&#39;j})\\), we can then define a dissimilarity between objects as \\[ d(x_i, x_{i&#39;}) = \\sum_{j=1}^p d_j(x_{ij},x_{i&#39;j}) \\] In the k-means algorithm, and many other algorithms, the most common usage is squared distance \\[ d_j(x_{ij},x_{i&#39;j}) = (x_{ij}-x_{i&#39;j})^2 \\] We can use different dissimilarities, for example \\[ d_j(x_{ij}, x_{i&#39;j}) = |x_{ij}-x_{i&#39;j}| \\] which may affect our choice of clustering algorithm later on. For categorical variables, we could set \\[ d_j(x_{ij},x_{i&#39;j}) = \\begin{cases} 0 \\; \\textrm{if } x_{ij} = x_{i&#39;j} \\\\ 1 \\; \\textrm{o.w.} \\end{cases} \\] If the values the categorical variable have an intrinsic similarity we could generalize using symmetric matrix \\(L\\) with elements \\(L_{rr&#39;} = L_{r&#39;r}\\), \\(L_{rr}=0\\) and \\(L_{rr&#39;} \\geq 0\\) otherwise. This may of course lead to a dissimilarity that is not a proper distance. 31.5 K-means Clustering A commonly used algorithm to perform clustering is the K-means algorithm. It is appropriate when using squared Euclidean distance as the measure of object dissimilarity. \\[ \\begin{aligned} d(x_{i},x_{i&#39;}) &amp; = \\sum_{j=1}^p (x_{ij}-x_{i&#39;j})^2 \\\\ {} &amp; = \\|x_i - x_{i&#39;}\\|^2 \\end{aligned} \\] K-means partitions observations into \\(K\\) clusters, with \\(K\\) provided as a parameter. Given some clustering, or partition, \\(C\\), the cluster assignment of observation \\(x_i\\) to cluster \\(k \\in \\{1,\\ldots,K\\}\\) is denoted as \\(C(i)=k\\). K-means seeks to minimize a clustering criterion measuring the dissimilarity of observations assigned to the same cluster. \\[ W(C) = \\frac{1}{2} \\sum_{k=1}^K \\sum_{i: \\, C(i)=k} \\sum_{i&#39;:\\, C(i&#39;)=k} \\|x_i - x_{i&#39;}\\|^2 \\] Note however, that this is equivalent to minimizing \\[ W(C) = \\frac{1}{2}\\sum_{k=1}^K N_k \\sum_{i:\\,C(i)=k} \\|x_i - \\bar{x}_k\\|^2 \\] where \\(\\bar{x}_k=(\\bar{x}_{k1},\\ldots,\\bar{x}_{kp})\\) and \\(\\bar{x}_{kj}\\) is the average of predictor \\(j\\) over the observations assigned to cluster \\(k\\), i.e., \\(C(i)=k\\), and \\(N_k\\) is the number of observations assigned to cluster \\(k\\). Thus the criteria to minimize is the total distance given by each observation to the mean (centroid) of the cluster to which the observation is assigned. An iterative algorithm is used to minimize this criterion Initialize by choosing \\(K\\) observations as centroids \\(m_1,m_2,\\ldots,m_k\\) Assign each observation \\(i\\) to the cluster with the nearest centroid, i.e., set \\(C(i)=\\arg\\min_{1 \\leq k \\leq K} \\|x_i - m_k\\|^2\\) Update centroids \\(m_k=\\bar{x}_k\\) Iterate steps 1 and 2 until convergence Here we illustrate the k-means algorithm over four iterations on our example data with \\(K=4\\). Each row in this plot is an iteration of the algorithm, and each column corresponds to a cluster. We observe, that in this case, there is little update of the cluster assignments and centroids beyond the first iteration. Criterion \\(W(C)\\) is reduced in each iteration so the algorithm is assured to converge. However, as this is not a convex criterion, the clustering we obtain may not be globally optimal. To address this in practice, the algorithm is run with multiple initializations (step 0) and the best clustering achieved is used. Also, selection of observations as centroids can be improved using the K-means++ algorithm: Choose an observation as centroid \\(m_1\\) uniformly at random To choose centroid \\(m_k\\), compute for each observation \\(i\\) not chosen as a centroid the distance to the nearest centroid \\(d_i = \\min_{1\\leq l &lt; k} \\|x_i - m_l\\|^2\\) Set centroid \\(m_k\\) to an observation randomly chosen with probability \\(\\frac{e^d_i}{\\sum_{i&#39;} e^d_{i&#39;}}\\) Iterate steps 1 and 2 until \\(K\\) centroids are chosen This ensures that initial centroids are well distributed in predictor space and yield a better clustering. 31.6 Choosing the number of clusters The number of parameters must be determined before running the K-means algorithm. As opposed to the supervised learning setting where model selection is performed by minimizing expected prediction error through, for example, cross-validation, there is no clean direct method for choosing the number of clusters to use in the K-means algorithm. Furthermore, looking at criterion \\(W(C)\\) alone is not sufficient as the criterion will become smaller as the value of \\(K\\) is reduced. Here we show the behavior of \\(W_K(C)\\) in our example dataset. However, there are properties of this statistic that may be used to heuristically choose a proper value of \\(K\\). The intuition behind the method is that, supposing there is a true underlying number \\(K^*\\) of clusters in the data, improvement in the \\(W_K(C)\\) statistic will be fast for values of \\(K \\leq K^*\\), and slower for values of \\(K &gt; K^*\\). In the first case, there will be a cluster which will contain observations belonging to two of the true underlying clusters, and therefore will have poor within cluster similarity. As \\(K\\) is increased, observations may then be separated into separate clusters, providing a sharp improvement in the \\(W_K(C)\\) statistic. On the other hand, for values of \\(K &gt; K^*\\) observations belonging to a single true cluster are split into multiple cluster, all with generally high within-cluster similarity, therefore splitting these clusters further will not improve the \\(W_K(C)\\) statistic very sharply. The curve will therefore have an inflection point around \\(K^*\\). We see this behavior in our example plot, where improvement in \\(W_K(C)\\) is slower after \\(K=3\\). The gap statistic is used to identify the inflection point in the curve. It compares the behavior of the \\(W_K(C)\\) statistic based on the data with the behavior of the \\(W_K(C)\\) statistic for data generated uniformly at random over the range of the data. Below we plot the gap statistic for our example data. For this dataset, the gap statistic suggests that there is no clear cluster structure and therefore \\(K=1\\) is an optimal choice. 31.7 Summary Clustering methods are intuitive methods useful to understand structure within unlabeled observations. The K-means algorithm is a frequently used, easy to implement and understand algorithm for clustering based on Euclidean distance between data entities. "],
["unsupervised-learning-dimensionality-reduction.html", "32 Unsupervised Learning: Dimensionality Reduction 32.1 Principal Component Analysis 32.2 Multidimensional Scaling 32.3 Summary", " 32 Unsupervised Learning: Dimensionality Reduction Recall that in unsupervised data we are interested in characterizing patterns in predictor space where observation measurements are represented. Mathematically, we stated as an interest in characterizing \\(p(X)\\) over \\(p\\)-dimensional predictor space. Clustering methods assume that this space \\(p(X)\\) can be partitioned into subspaces containing “similar” observations. In dimensionality reduction, we assume that observations can be represented in a space with dimension much lower than \\(p\\). We will see two general strategies for dimensionality reduction: data transformations into spaces of smaller dimension that capture global properties of a data set \\(X\\), and data embeddings into lower dimensional spaces that retain local properties of a data set \\(X\\). 32.1 Principal Component Analysis Principal Component Analysis (PCA) is a dimensionality reduction method. The goal is to embed data in high dimensional space (e.g., observations with a large number of variables), onto a small number of dimensions. Note that its most frequent use is in EDA and visualization, but it can also be helpful in regression (linear or logistic) where we can transform input variables into a smaller number of predictors for modeling. Mathematically, the PCA problem is: Given: - Data set \\(\\{\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_n\\}\\), where \\(\\mathbf{x}_i\\) is the vector of \\(p\\) variable values for the \\(i\\)-th observation. Return: - Matrix \\(\\left[ \\phi_1, \\phi_2, \\ldots, \\phi_p \\right]\\) of linear transformations that retain maximal variance. You can think of the first vector \\(\\phi_1\\) as a linear transformation that embeds observations into 1 dimension: \\[ Z_1 = \\phi_{11}X_1 + \\phi_{21} X_2 + \\cdots + \\phi_{p1} X_p \\] where \\(\\phi_1\\) is selected so that the resulting dataset \\(\\{ z_1, \\ldots, z_n\\}\\) has maximum variance. In order for this to make sense mathematically data has to be centered, i.e., each \\(X_j\\) has mean equal to zero and transformation vector \\(\\phi_1\\) has to be normalized, i.e., \\(\\sum_{j=1}^p \\phi_{j1}^2=1\\). We can find \\(\\phi_1\\) by solving an optimization problem: \\[ \\max_{\\phi{11},\\phi_{21},\\ldots,\\phi_{p1}} \\frac{1}{n} \\sum_{i=1}^n \\left( \\sum_{j=1}^p \\phi_{j1} x_{ij} \\right)^2 \\\\ \\mathrm{s.t.} \\sum_{j=1}^p \\phi_{j1}^2 = 1 \\] Conceptually this optimization problem says maximize variance but subject to normalization constraint. The second transformation \\(\\phi_2\\) is obtained next solving a similar problem with the added constraint that \\(\\phi_2\\) is orthogonal to \\(\\phi_1\\). Taken together \\(\\left[ \\phi_1, \\phi_2 \\right]\\) define a pair of linear transformations of the data into 2 dimensional space. \\[ Z_{n\\times 2} = X_{n \\times p} \\left[ \\phi_1, \\phi_2 \\right]_{p \\times 2} \\] Each of the columns of the \\(Z\\) matrix are called Principal Components. The units of the PCs are meaningless. In particular, comparing numbers across PCs doesn’t make mathematical sense. In practice, one may also use a scaling transformation on the variables \\(X_j\\) to have unit variance. In general, if variables \\(X_j\\) are measured in different units (e.g, miles vs. liters vs. dollars), variables should be scaled to have unit variance. Conversely, if they are all measured in the same units, they should be scaled. library(tidyverse) library(readr) library(lubridate) datadir &lt;- &quot;data&quot; url &lt;- &quot;http://files.zillowstatic.com/research/public/Affordability_Wide_2017Q4_Public.csv&quot; filename &lt;- basename(url) datafile &lt;- file.path(datadir, filename) if (!file.exists(datafile)) { download.file(url, file.path(datadir, filename)) } afford_data &lt;- read_csv(datafile) tidy_afford &lt;- afford_data %&gt;% filter(Index == &quot;Mortgage Affordability&quot;) %&gt;% drop_na() %&gt;% filter(RegionID != 0) %&gt;% dplyr::select(RegionID, matches(&quot;^[1|2]&quot;)) %&gt;% gather(time, affordability, matches(&quot;^[1|2]&quot;)) %&gt;% type_convert(col_types=cols(time=col_date(format=&quot;%Y-%m&quot;))) wide_afford_df &lt;- tidy_afford %&gt;% dplyr::select(RegionID, time, affordability) %&gt;% spread(time, affordability) value_mat &lt;- wide_afford_df %&gt;% dplyr::select(-RegionID) %&gt;% as.matrix() pca_res &lt;- prcomp(value_mat, scale=FALSE) pca_au &lt;- broom::augment(pca_res, wide_afford_df) pca_d &lt;- broom::tidy(pca_res, matrix=&quot;d&quot;) pc_loading &lt;- broom::tidy(pca_res, matrix=&quot;variables&quot;) %&gt;% type_convert(col_types=cols(column=col_date(&quot;%Y-%m-%d&quot;))) %&gt;% mutate(PC=as.character(PC)) pc_mean &lt;- pca_res$center pc_mean &lt;- data_frame(column=names(pc_mean), PC=&quot;mean&quot;, value=pc_mean) %&gt;% type_convert(col_types=cols(column=col_date(&quot;%Y-%m-%d&quot;))) ## Warning: `data_frame()` is deprecated, use `tibble()`. ## This warning is displayed once per session. pc_loading &lt;- pc_mean %&gt;% bind_rows(pc_loading) Here we plot the mortgage affordability data embedded into the first two principal components. There are some time in these two components that may be treated as outliers. Also, a clustering analysis in this reduced space seems like a reasonable next step. ggplot(pca_au, aes(.fittedPC1, .fittedPC2)) + geom_point(size=2) + labs(x=&quot;PC1&quot;, y=&quot;PC2&quot;) A natural question that arises: How many PCs should we consider in post-hoc analysis? One result of PCA is a measure of the variance corresponding to each PC relative to the total variance of the dataset. From that we can calculate the percentage of variance explained for the \\(m\\)-th PC: \\[ PVE_m=\\frac{\\sum_{i=1}^n z_{im}^2}{\\sum_{j=1}^p \\sum_{i=1}^n x_{ij}^2} \\] We can use this measure to choose number of PCs in an ad-hoc manner. In our case, using more than 10 or so PCs does not add information. pca_d &lt;- broom::tidy(pca_res, matrix=&quot;d&quot;) pca_d %&gt;% filter(PC &lt;= 30) %&gt;% ggplot(aes(PC, 100 * cumulative)) + geom_line(size=1.32) + labs(x=&quot;PC&quot;, y=&quot;Pct. Variance Explained&quot;) A useful rule of thumb: - If no apparent patterns in first couple of PCs, stop! - Otherwise, look at other PCs using PVE as guide. There are bootstrap based methods to perform a statistically guided selection of the number of PCs. However, there is no commonly agreed upon method for choosing number of PCs used in practice, and methods are somewhat ad-hoc. 32.1.1 Solving the PCA Algorithmically, the Principle Components solutions \\(\\phi\\) are obtained from the singular value decomposition of observation matrix \\(X_{n\\times p}=UDV^T\\), where matrices \\(U\\) and \\(V\\) are orthogonal matrices (\\(U^TU=I\\) and \\(V^TV=I\\)) called the left and right singular vectors respectively. \\(D\\) is a diagonal matrix with \\(d_1 \\geq d_2 \\geq \\ldots d_p \\geq 0\\). These are referred to as the singular values. Using our previous notation \\(V\\) is the transformation matrix \\(V=\\left[\\phi_1,\\phi_2,\\cdots,\\phi_p \\right]\\). Principal components \\(Z\\) are given by the columns of \\(UD\\). Since \\(U\\) is orthogonal, \\(d_j^2\\) equals the variance of the \\(j\\)th PC. From this observation we also see that we can write original observations \\(x_i\\) in terms of PCs \\(z\\) and transformations \\(\\phi\\). Specifically \\(x_i = z_{i1}\\phi_1 + z_{i2}\\phi_2 + \\cdots + z_{ip} \\phi_p\\). We can think of the \\(\\phi_j\\) vectors as a basis over which we can represent original observations \\(i\\). For this reason, another useful post-hoc analysis is to plot the transformation vectors \\(\\phi_1, \\phi_2, \\ldots\\). Here we plot the mean time series (since we center observations \\(X\\) before performing the embedding) along with the first three \\(\\phi_j\\) vectors. pc_loading %&gt;% mutate(PC=forcats::fct_shift(factor(PC),-1)) %&gt;% filter(PC %in% c(&quot;mean&quot;,1:3)) %&gt;% ggplot(aes(column, value)) + geom_line() + facet_wrap(~PC) 32.2 Multidimensional Scaling Multidimensional scaling is a similar approach to PCA but looks at the task in a little different manner. Given observations \\(x_1,\\ldots,x_N\\) in \\(p\\) dimensions, let \\(d_{ij}\\) be the distance between observations \\(i\\) and \\(j\\). We may also use this algorithm given distances initially instead of \\(p\\) dimensional observations. Multidimensional Scaling (MDS) seeks to find embeddings \\(z_1, \\ldots, z_N\\) of \\(k\\) dimensions for which Euclidean distance (in \\(k\\) dimensional space) is close to the input distances \\(d_{ij}\\). In least squares MDS, we can do this by minimizing \\[ S_M(z_1,\\ldots,z_N) = \\sum_{i\\neq j} (d_{ij}- \\|z_i - z_j\\|)^2 \\] A gradient descent algorithm is used to minimize this function. A related method that tends to better capture small distances is given by the Sammon mapping: \\[ S_{S_m}(z_1,\\ldots,z_N) = \\sum_{i\\neq j} \\frac{(d_{ij}- \\|z_i - z_j\\|)^2}{d_{ij}} \\] Since MDS can use distances as input, it is suitable to use when distances between observations are all we have. 32.3 Summary Principal Component Analysis is a conceptually simple but powerful EDA tool. It is very useful at many stages of analyses. PCA interpretation can be very ad-hoc, however. It is part of large set of unsupervised methods based on matrix decompositions, including Kernel PCA, Non-negative Matrix Factorization and others. Embedding methods seek to capture local properties of observations. A popular recent method is the t-SNE method. "]
]
