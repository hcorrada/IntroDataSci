<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Statistical Principles</title>
    <meta charset="utf-8" />
    <meta name="author" content="Héctor Corrada Bravo" />
    <meta name="date" content="2020-03-30" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/default-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="custom.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">


class: title-slide, center, middle
count: false

.banner[![](img/epiviz.png)]

.title[Introduction to Data Science: Statistical Principles]

.author[Héctor Corrada Bravo]

.other-info[
University of Maryland, College Park, USA  
CMSC320: 2020-03-30
]

.logo[![](img/logo.png)]

---



# Why Stats?

In this class we learn _Statistical and Machine Learning_ techniques for data analysis.

By the time we are done, you should 

- be able to read **critically** papers or reports that use these methods.
- be able to use these methods for daata analysis

---

# Why Stats?

In either case, you will need to ask yourself if findings are **statistically significant**. 

---
class: split-50

# Why Stats?

.column[
- Use a classification algorithm to distinguish images
- Accurate 70 out of 100 cases.
- Could this happen by chance alone? 

]

.column[
.image-50[![](img/images.png)]
]

---

# Why Stats?

To be able to answer these question, we need to understand some basic probabilistic and statistical principles. 

In this course unit we will review some of these principles.

---
layout: true

# Variation, randomness and stochasticity

---
class: split-30

So far, we have not spoken about _randomness_ and _stochasticity_. 
We have, however, spoken about _variation_. 

.column[_spread_ in a dataset refers to the fact that in a population of entities
there is naturally occuring variation in measurements]

.column[
![](stat_intro_files/figure-html/unnamed-chunk-1-1.png)&lt;!-- --&gt;
]

---
class: split-50

.column[
Another example: in sets of tweets there is natural variation in the frequency of word usage.
]

.column[
.image-50[![](img/trump_words.svg)]
]

---

In summary, we can discuss the notion of _variation_ without referring to any randomness, stochasticity or noise. 

---
layout: true

# Why Probability?

---


Because, we **do** want to distinguish, when possible:

  - natural occuring variation, vs.
  - randomness or stochasticity

---

Example: want to learn something about education loan debt for 19-30 year olds in Maryland. 

- Find loan debt for **all** 19-30 year old Maryland residents, and calculate mean and standard deviation. 

--

- That's difficult to do for all residents. 

--

- Instead we sample (say by randomly sending Twitter surveys), and _estimate_ the average and standard deviation of debt in this population from the sample. 

---

Now, this presents an issue since we could do the same from a different random sample and get a different set of estimates. Why? 

--

Because there is naturally-occuring variation in this population.  

---

So, a simple question to ask is: 

&gt; How good are our _estimates_ of debt mean and standard deviation from sample of 19-30 year old Marylanders? 

---

Another example: suppose we build a predictive model of loan debt for 19-30 year old Marylanders based on other variables (e.g., sex, income, education, wages, etc.) from our sample. 

--

&gt; How good will this model perform when predicting debt in general?

---

We use probability and statistics to answer these questions. 

--

- Probability captures stochasticity in the sampling process, while

--

- we _model_ naturally occuring variation in measurements in a population of interest.

---
layout: false

# One final word

The term _population_ means 

&gt; **the entire** collection of entities we want to model 

This could include people, but also images, text, chess positions, etc.

---
layout: true

## Random variables

---

The basic concept in our discussion of probability is the _random variable_. 

Task: is a given tweet was generated by a bot? 

Action: Sample a tweet **at random** from the set of all tweets ever written and have a human expert decide if it was generated by a bot or not. 

Principle: Denote this as a _binary_ random variable `\(X \in \{0,1\}\)`, with value `\(1\)` if the tweet is bot-gerneated and 0 otherwise. 

--

Why is this a random value? Because it depends on the tweet that was _randomly_ sampled.

---
layout: true

## (Discrete) Probability distributions

---

A _probability distribution_ `\(P:\mathcal{D} \to [0,1]\)` over set `\(\mathcal{D}\)` of all values random variable `\(X\)` can take to the interval `\([0,1]\)`. 

--

We start with a _probability mass function_ `\(p\)`:

a. `\(p(X=x) \geq 0\)` for all values `\(x \in \mathcal{D}\)`, and  
b. `\(\sum_{x\in \mathcal{D}} p(X=x) = 1\)`

---

How to interpret quantity `\(p(X=1)\)`?

--

a. `\(p(X=1)\)` is the _probability_ that a uniformly random sampled tweet is bot-generated, which implies  

--

b. the proportion of bot-generated tweets in the set of "all" tweets is `\(p(X=1)\)`.

---

### Example The oracle of TWEET

Suppose we have a magical oracle and know for a _fact_ that 70% of "all" tweets are bot-generated. 

--

In that case `\(p(X=1) = .7\)` and `\(p(X=0)=1-.7=.3\)`. 

---

_cumulative probability distribution_ `\(P\)` describes the sum of probability up to a given value: 

$$
P(x) = \sum_{x' \mathcal D \textrm{ s.t. } x' \leq x} p(X=x')
$$

---

## Expectation

What if I randomly sampled `\(n=100\)` tweets? 

How many of those do I _expect_ to be bot-generated?

--

_Expectation_ is a formal concept in probability:

$$
\mathbb{E} [X] = \sum_{x\in \mathcal{D}} x p(X=x)
$$

---

What is the expectation of `\(X\)` (a single sample) in our tweet example? 

--

$$
\mathbb{E}[X] = 0 \times p(X=0) + 1 \times p(X=1) = \\
0 \times .3 + 1 \times .7 = .7
$$

---

What is the expected number of bot-generated tweets in a sample of `\(n=100\)` tweets. 

Define `\(Y=X_1 + X_2 + \cdots + X_{100}\)`. 

Then we need `\(\mathbb{E}[Y]\)`

---

We have `\(X_i=\{0,1\}\)` for each of the `\(n=100\)` tweets

Each obtained by uniformly and _independently_ sampling from the set of all tweets. 

Then, random variable `\(Y\)` is _the number of bot-generated tweets in my sample of `\(n=100\)` tweets_. 

---

$$
`\begin{aligned}
\mathbb{E} [Y] &amp; = \mathbb{E} [X_1 + X_2 + \cdots + X_{100}] \\
{} &amp; = \mathbb{E} [X_1] + \mathbb{E} [X_2] + \cdots + \mathbb{E} [X_{100}] \\
{} &amp; = .7 + .7 + \cdots + .7 \\
{} &amp; = 100 \times .7 \\
{} &amp; = 70
\end{aligned}`
$$

---

This uses some facts about expectation you can show in general.

(1) For any pair of random variables `\(X_1\)` and `\(X_2\)`, `\(\mathbb{E} [X_1 + X_2] = \mathbb{E} [X_1] + \mathbb{E} [X_2]\)`.  

(2) For any random variable `\(X\)` and _constant_ a, `\(\mathbb{E} [aX] = a \mathbb{E} [X]\)`.

---
layout: true

## Estimation

---

So far we assume we have access to an oracle that told us `\(p(X=1)=.7\)`.

In reality, we _don't_. 

--

For our tweet analysis task, we need to _estimate_ the proportion of "all" tweets that are bot-generated. 

--

This is where our probability model and the expectation we derive from it comes in.

---

Given _data_ `\(x_1, x_2, x_3, \ldots, x_{100}\)`, 

With 67 of those tweets labeled as bot-generated (i.e., `\(x_i=1\)` for 67 of them) 

--

We can say `\(y=\sum_i x_i=67\)`. 

--

We _expect_ `\(y=np\)` with `\(p=p(X=1)\)`

--

Use that observation to _estimate_ `\(p\)`!

---

$$
`\begin{aligned}
np = 67 &amp; \Rightarrow \\
100p = 67 &amp; \Rightarrow \\
\hat{p} = \frac{67}{100} &amp; \Rightarrow \\
\hat{p} = .67
\end{aligned}`
$$
---

Our estimate ($\hat{p}=.67$) is wrong, but close.

Can we ever get it right? 

Can I say how wrong I should expect my estimates to be? 
---

Notice that our estimate of `\(\hat{p}\)` is the sample _mean_ of `\(x_1,x_2,\ldots,x_n\)`. 

Let's go back to our oracle of tweet to do a thought experiment and replicate how we derived our estimate from 100 tweets a few thousand times. 

---



&lt;img src="stat_intro_files/figure-html/unnamed-chunk-3-1.png" style="display: block; margin: auto;" /&gt;

---

What does this say about our estimates of the proportion of bot-generated tweets if we use 100 tweets in our sample?

Now what if instead of sampling `\(n=100\)` tweets we used other sample sizes?

---

&lt;img src="stat_intro_files/figure-html/unnamed-chunk-4-1.png" style="display: block; margin: auto;" /&gt;

---

We can make a couple of observations:

1. The distribution of estimate `\(\hat{p}\)` is _centered_ at `\(p=.7\)`, our unknown _population_ proportion, and   

2. The _spread_ of the distribution **decreases** as the number of samples `\(n\)` **increases**.

---

This was a simulation, we faked the **data generating procedure**. 

In reality, we can't.

--

What to do we do then?

(1) Math, or   
(2) Resample

---
layout: true

## Solve with Math

---

Our simulation is an illustration of two central tenets of statistics:

(a) The law of large numbers (LLN)   
(b) The central limit theorem (CLT)

---


### Law of large numbers (LLN)

Given _independently_ sampled random variables `\(X_1,X_2,\cdots,X_n\)` with `\(\mathbb{E} [X_i]=\mu\)` for all `\(i\)`,

$$
\frac{1}{n} \sum_i X_i \to \mu, \textrm{ as } n\to\infty
$$

I.E. `\(\overline{x}\)` _tends_ to the expected value `\(\mu\)` (under some assumptions beyond the scope of this class) regardless of the distribution `\(X_i\)`.

---

Implication: the sample mean was a good procedure to use to estimate parameters by matching their expected value! 

---

### Central Limit Theorem (CLT)

The LLN says that estimates built using the sample mean will tend to the correct answer 

The CLT describes how these estimates are _spread_ around the correct answer.

---

Here we will use the concept of _variance_ which is expected _spread_, measured in squared distance, from the _expected value_ of a random variable:

$$
\mathrm{var[X]} = \mathbb{E} [(X - \mathbb{E} [X])^2]
$$

---

Example: consider the variance of our random tweet example:

$$
`\begin{aligned}
\mathrm{var[X]} &amp; = \sum_{\mathcal{D}} (x-\mathbb{E} [X])^2 p(X=x) \\
{} &amp; = (0 - p)^2 \times (1-p) + (1 - p)^2 \times p \\
{} &amp; = p^2(1-p) + (1-p)^2p \\
{} &amp; = p(1-p) (p + (1-p)) \\
{} &amp; = p(1-p) (p - p + 1) \\
{} &amp; = p(1-p)
\end{aligned}`
$$

---

CLT: Given _independently_ sampled random variables `\(X_1,X_2,\cdots,X_n\)` **from the same probability distribution** `\(P(X)\)`, with `\(\mathbb{E} [X_i]=\mu\)` and `\(\mathrm{var}[X_i]=\sigma^2\)` for all `\(i\)`,


$$
P\left( \frac{1}{n} \sum_{i=1} X_i \right) \to N \left( \mu,\frac{\sigma}{n} \right), \textrm{ as } n\to \infty 
$$

---

This says, that as sample size `\(n\)` increases, the distribution of sample means is _well_ approximated by a **normal distribution**. 

This means we can approximate the _expected error_ of our estimates well.

---
layout: true

## (Continuous) Random Variables

---

### The normal distribution

Random variable `\(Y=\sum_{i=1}^n X_i\)` is _continuous_.

The normal distribution describes the distribution of _continuous_ random variables over the range `\((-\infty,\infty)\)` using two parameters: 

**mean** `\(\mu\)` and **standard deviation** `\(\sigma\)`.

--

We write " `\(Y\)` is normally distributed with mean `\(\mu\)` and standard deviation `\(\sigma\)`" as `\(Y\sim N(\mu,\sigma)\)`. 

---

Continuous random variables are described by a _probability density function_. For normally distributed random variables:

`$$p(Y=y) = \frac{1}{\sqrt{2\pi}\sigma} \mathrm{exp} \left\{ -\frac{1}{2} \left( \frac{y-\mu}{\sigma} \right)^2 \right\}$$`

---

Three examples of normal probability density functions with mean `\(\mu=60,50,60\)` and standard deviation `\(\sigma=2,2,6\)`:

&lt;img src="stat_intro_files/figure-html/unnamed-chunk-5-1.png" style="display: block; margin: auto;" /&gt;

---

Like the discrete case, probability density functions for continuous random variables need to satisfy certain conditions:

a. `\(p(Y=y) \geq 0\)` for all values `\(Y \in (-\infty,\infty)\)`, and  
b. `\(\int_{-\infty}^{\infty} p(Y=y) dy = 1\)`

---

One way of interpreting the density function of the normal distribution is that probability decays exponentially with rate `\(\sigma\)` based on squared distance to the mean `\(\mu\)`. (Here is squared distance again!)

$$
p(Y=y) \propto \exp \left\{ -{\frac{1}{2\sigma^2} (y-\mu)^2} \right \}
$$

---

Also, notice the term inside the square?

$$
z = \left( \frac{y - \mu}{\sigma} \right)
$$

this is the _standardization_ transformation we saw before. 

---

The name _standardization_ comes from the _standard normal distribution_ `\(N(0,1)\)` (mean 0 and standard deviation 1), 

Which is very convenient to work with because it's density function is much simpler:

`$$p(Z=z) = \frac{1}{\sqrt{2\pi}} \mathrm{exp} \left\{ -\frac{1}{2} z^2 \right\}$$`

--

In fact, if random variable `\(Y \sim N(\mu,\sigma)\)` then random variable `\(Z=\frac{Y-\mu}{\sigma} \sim N(0,1)\)`.

---

One more technicality:

The cumulative probability function for continuous random variables is given by

$$
P(Y\leq y) = \int_{\mathcal D} p(Y=y) dy
$$

where `\(\mathcal{D}\)` is the range of values random variable `\(Y\)` can take (e.g., for normal distribution `\(\mathcal{D}=(-\infty,\infty)\)`)

---
layout: true

## Solve with Math

---

### CLT continued

We need one last bit of terminology to finish the statement of the CLT. 

Consider data `\(X_1,X_2,\cdots,X_n\)` with `\(\mathbb{E}[X_i]= \mu\)` for all `\(i\)`, **and** `\(\mathrm{var}(X_i)=\sigma^2\)` for all `\(i\)`, 

and sample mean `\(Y=\frac{1}{n} \sum_i X_i\)`. 

The standard deviation of `\(Y\)` is called the _standard error_:

$$
\mathrm{se}(Y) = \frac{\sigma}{\sqrt{n}}
$$

---

Now we can restate the CLT statement precisely: 

the distribution of `\(Y\)` tends _towards_ `\(N\left( \mu,\frac{\sigma}{\sqrt{n}} \right)\)` as `\(n \rightarrow \infty\)`. 

This says, that as sample size increases the distribution of sample means is well approximated by a normal distribution, 

and that the spread of the distribution goes to zero at the rate `\(\sqrt{n}\)`.

---

_Disclaimer_ There a few mathematical subtleties. Two important ones are that

a. `\(X_1,\ldots,X_n\)` are iid (independent, identically distributed) random variables, and  
b. `\(\mathrm{var}[X] &lt; \infty\)`

---

Let's redo our simulated replications of our tweet samples to illustrate the CLT at work:

&lt;img src="stat_intro_files/figure-html/unnamed-chunk-6-1.png" style="display: block; margin: auto;" /&gt;

---

Here we see the three main points of the LLN and CLT: 

(1) the normal density is centered around `\(\mu=.7\)`,  
(2) the normal approximation gets better as `\(n\)` increases, and  
(3) the standard error goes to 0 as `\(n\)` increases.

---
layout: true

## Solve with computation

### The Bootstrap Procedure

---

What if the conditions that we used for the CLT don't hold? 

For instance, samples `\(X_i\)` may not be independent. What can we do then, how can we say something about the precision of sample mean estimate `\(Y\)`?

---

A useful procedure to use in this case is the **bootstrap**. 

It is based on using _randomization_ to simulate the stochasticity resulting from the population sampling procedure we are trying to capture in our analysis.

---

The main idea is the following: given observations `\(x_1,\ldots,x_n\)` 

and the estimate `\(y=\frac{1}{n}\sum_{i=1}^n x_i\)`, 

what can we say about the standard error of `\(y\)`?

---

There are two challenges here: 

1) our estimation procedure is deterministic, that is, if I compute the sample mean of a specific dataset, I will always get the same answer; and 

2) we should retain whatever properties of estimate `\(y\)` result from obtaining it from `\(n\)` samples.

---

The bootstrap is a randomization procedure that measures the variance of estimate `\(y\)`, 

using randomization to address challenge (1), 

but doing so with randomized samples of size `\(n\)`, addressing challenge (2).

---

The procedure goes as follows:

1. Generate `\(B\)` random datasets by sampling _with replacement_ from dataset `\(x_1,\ldots,x_n\)`. Denote randomized dataset `\(b\)` as `\(x_{1b},\ldots,x_{nb}\)`.

2. Construct estimates from _each_ dataset, `\(y_b = \frac{1}{n}\sum_i x_{ib}\)`

3. Compute center (mean) and spread (variance) of estimates `\(y_b\)`

---

Let's see how this works on tweet oracle example

&lt;img src="stat_intro_files/figure-html/unnamed-chunk-7-1.png" style="display: block; margin: auto;" /&gt;

---

Not great, math works better when conditions are met.

---

Let's look at a case where we don't expect the normal approximation to not work so well by making  samples not identically distributed. 


Let's make a new ORACLE of tweet where the probability of a tweet being bot-generated depends on the previous tweet

---



&lt;img src="stat_intro_files/figure-html/unnamed-chunk-9-1.png" style="display: block; margin: auto;" /&gt;

---

Here, an analysis based on the classical CLT is not  appropriate ( `\(X_i\)` s are not independent) 

But the bootstrap analysis gives some information about the variability of our estimates. 

---
layout: true

## Summary

How do we learn information about a **population**? Estimation.

What can we say about estimates? Say something about sampling error.

How? Math (LLN and CL), or Computation (Bootstrap)
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>
<script>
remark.macros['scale'] = function (percentage) {
  var url = this;
  return '<img src="' + url + '" style=width: ' + percentage + '"/>';
};
</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
