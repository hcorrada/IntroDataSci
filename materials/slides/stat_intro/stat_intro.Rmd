---
title: "Statistical Principles"
author: "Héctor Corrada Bravo"
company: "University of Maryland"
date: "`r Sys.Date()`"
css: ["custom.css"]
output:
  xaringan::moon_reader:
    lib_dir: libs
    seal: false
    includes: 
      after_body: "custom.html"
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
---

class: title-slide, center, middle
count: false

.banner[![](img/epiviz.png)]

.title[Introduction to Data Science: Statistical Principles]

.author[Héctor Corrada Bravo]

.other-info[
University of Maryland, College Park, USA  
CMSC320: `r Sys.Date()`
]

.logo[![](img/logo.png)]

---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(cache=TRUE)
```

# Why Stats?

In this class we learn _Statistical and Machine Learning_ techniques for data analysis.

By the time we are done, you should 

- be able to read **critically** papers or reports that use these methods.
- be able to use these methods for daata analysis

---

# Why Stats?

In either case, you will need to ask yourself if findings are **statistically significant**. 

---
class: split-50

# Why Stats?

.column[
- Use a classification algorithm to distinguish images
- Accurate 70 out of 100 cases.
- Could this happen by chance alone? 

]

.column[
.image-50[![](img/images.png)]
]

---

# Why Stats?

To be able to answer these question, we need to understand some basic probabilistic and statistical principles. 

In this course unit we will review some of these principles.

---
layout: true

# Variation, randomness and stochasticity

---
class: split-30

So far, we have not spoken about _randomness_ and _stochasticity_. 
We have, however, spoken about _variation_. 

.column[_spread_ in a dataset refers to the fact that in a population of entities
there is naturally occuring variation in measurements]

.column[
```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.height=5}
library(tidyverse)
theme_set(theme_bw())

library(nycflights13)
flights %>%
  mutate(log_dep_delay = sign(dep_delay) * log(abs(dep_delay + 1))) %>%
  ggplot(aes(x=log_dep_delay)) +
    geom_histogram(bins=20)
```
]

---
class: split-50

.column[
Another example: in sets of tweets there is natural variation in the frequency of word usage.
]

.column[
.image-50[![](img/trump_words.svg)]
]

---

In summary, we can discuss the notion of _variation_ without referring to any randomness, stochasticity or noise. 

---
layout: true

# Why Probability?

---


Because, we **do** want to distinguish, when possible:

  - natural occuring variation, vs.
  - randomness or stochasticity

---

Example: want to learn something about education loan debt for 19-30 year olds in Maryland. 

- Find loan debt for **all** 19-30 year old Maryland residents, and calculate mean and standard deviation. 

--

- That's difficult to do for all residents. 

--

- Instead we sample (say by randomly sending Twitter surveys), and _estimate_ the average and standard deviation of debt in this population from the sample. 

---

Now, this presents an issue since we could do the same from a different random sample and get a different set of estimates. Why? 

--

Because there is naturally-occuring variation in this population.  

---

So, a simple question to ask is: 

> How good are our _estimates_ of debt mean and standard deviation from sample of 19-30 year old Marylanders? 

---

Another example: suppose we build a predictive model of loan debt for 19-30 year old Marylanders based on other variables (e.g., sex, income, education, wages, etc.) from our sample. 

--

> How good will this model perform when predicting debt in general?

---

We use probability and statistics to answer these questions. 

--

- Probability captures stochasticity in the sampling process, while

--

- we _model_ naturally occuring variation in measurements in a population of interest.

---
layout: false

# One final word

The term _population_ means 

> **the entire** collection of entities we want to model 

This could include people, but also images, text, chess positions, etc.

---
layout: true

## Random variables

---

The basic concept in our discussion of probability is the _random variable_. 

Task: is a given tweet was generated by a bot? 

Action: Sample a tweet **at random** from the set of all tweets ever written and have a human expert decide if it was generated by a bot or not. 

Principle: Denote this as a _binary_ random variable $X \in \{0,1\}$, with value $1$ if the tweet is bot-gerneated and 0 otherwise. 

--

Why is this a random value? Because it depends on the tweet that was _randomly_ sampled.

---
layout: true

## (Discrete) Probability distributions

---

A _probability distribution_ $P:\mathcal{D} \to [0,1]$ over set $\mathcal{D}$ of all values random variable $X$ can take to the interval $[0,1]$. 

--

We start with a _probability mass function_ $p$:

a. $p(X=x) \geq 0$ for all values $x \in \mathcal{D}$, and  
b. $\sum_{x\in \mathcal{D}} p(X=x) = 1$

---

How to interpret quantity $p(X=1)$?

--

a. $p(X=1)$ is the _probability_ that a uniformly random sampled tweet is bot-generated, which implies  

--

b. the proportion of bot-generated tweets in the set of "all" tweets is $p(X=1)$.

---

### Example The oracle of TWEET

Suppose we have a magical oracle and know for a _fact_ that 70% of "all" tweets are bot-generated. 

--

In that case $p(X=1) = .7$ and $p(X=0)=1-.7=.3$. 

---

_cumulative probability distribution_ $P$ describes the sum of probability up to a given value: 

$$
P(x) = \sum_{x' \mathcal D \textrm{ s.t. } x' \leq x} p(X=x')
$$

---

## Expectation

What if I randomly sampled $n=100$ tweets? 

How many of those do I _expect_ to be bot-generated?

--

_Expectation_ is a formal concept in probability:

$$
\mathbb{E} [X] = \sum_{x\in \mathcal{D}} x p(X=x)
$$

---

What is the expectation of $X$ (a single sample) in our tweet example? 

--

$$
\mathbb{E}[X] = 0 \times p(X=0) + 1 \times p(X=1) = \\
0 \times .3 + 1 \times .7 = .7
$$

---

What is the expected number of bot-generated tweets in a sample of $n=100$ tweets. 

Define $Y=X_1 + X_2 + \cdots + X_{100}$. 

Then we need $\mathbb{E}[Y]$

---

We have $X_i=\{0,1\}$ for each of the $n=100$ tweets

Each obtained by uniformly and _independently_ sampling from the set of all tweets. 

Then, random variable $Y$ is _the number of bot-generated tweets in my sample of $n=100$ tweets_. 

---

$$
\begin{aligned}
\mathbb{E} [Y] & = \mathbb{E} [X_1 + X_2 + \cdots + X_{100}] \\
{} & = \mathbb{E} [X_1] + \mathbb{E} [X_2] + \cdots + \mathbb{E} [X_{100}] \\
{} & = .7 + .7 + \cdots + .7 \\
{} & = 100 \times .7 \\
{} & = 70
\end{aligned}
$$

---

This uses some facts about expectation you can show in general.

(1) For any pair of random variables $X_1$ and $X_2$, $\mathbb{E} [X_1 + X_2] = \mathbb{E} [X_1] + \mathbb{E} [X_2]$.  

(2) For any random variable $X$ and _constant_ a, $\mathbb{E} [aX] = a \mathbb{E} [X]$.

---
layout: true

## Estimation

---

So far we assume we have access to an oracle that told us $p(X=1)=.7$.

In reality, we _don't_. 

--

For our tweet analysis task, we need to _estimate_ the proportion of "all" tweets that are bot-generated. 

--

This is where our probability model and the expectation we derive from it comes in.

---

Given _data_ $x_1, x_2, x_3, \ldots, x_{100}$, 

With 67 of those tweets labeled as bot-generated (i.e., $x_i=1$ for 67 of them) 

--

We can say $y=\sum_i x_i=67$. 

--

We _expect_ $y=np$ with $p=p(X=1)$

--

Use that observation to _estimate_ $p$!

---

$$
\begin{aligned}
np = 67 & \Rightarrow \\
100p = 67 & \Rightarrow \\
\hat{p} = \frac{67}{100} & \Rightarrow \\
\hat{p} = .67
\end{aligned}
$$
---

Our estimate ($\hat{p}=.67$) is wrong, but close.

Can we ever get it right? 

Can I say how wrong I should expect my estimates to be? 
---

Notice that our estimate of $\hat{p}$ is the sample _mean_ of $x_1,x_2,\ldots,x_n$. 

Let's go back to our oracle of tweet to do a thought experiment and replicate how we derived our estimate from 100 tweets a few thousand times. 

---

```{r, echo=FALSE, message=FALSE}
library(tidyverse)
theme_set(theme_bw())
```

```{r, echo=FALSE, fig.align="center", fig.width=9}
# proportion of bot-tweets in the the tweet population
# as given by the oracle of TWEET
p <- 0.7

# let's sample 100 tweets
# this function chooses between values in a vector (0 and 1)
# with probability given by vector prob
# we need 100 samples from this vector with replacement
# since there are fewer items in the vector than the size
# of the sample we are making
x <- sample(c(0,1), size=100, replace=TRUE, prob=c(1-p,p))

# compute the estimated proportion that are bot-generated (using the sample mean)
phat <- mean(x)

# if we had an oracle that let's us do this cheaply,
# we could replicate our experiment 1000 times
# (you don't in real life)

# first let's write a function that gets an estimate
# of proportion from a random sample
get_estimate <- function(n, p=0.7) mean(sample(c(0,1), size=n, replace=TRUE, prob=c(1-p,p)))

# let's make a vector with 1000 _estimates_
phats_100 <- replicate(1000, get_estimate(100))

# now let's plot a histogram of the 
hist(phats_100, xlab=expression(hat(p)), xlim=c(0.5,1), main="Distribution of p estimates from 100 tweets")
```

---

What does this say about our estimates of the proportion of bot-generated tweets if we use 100 tweets in our sample?

Now what if instead of sampling $n=100$ tweets we used other sample sizes?

---

```{r, echo=FALSE, fig.align="center", fig.width=9}
par(mfrow=c(2,3))
# what if we sample 10 tweets
phats_10 <- replicate(1000, get_estimate(10))
hist(phats_10, main="10 tweets", xlab="p hat", xlim=c(.5,1), probability=TRUE)

# what if we sample 100 tweets
phats_100 <- replicate(1000, get_estimate(100))
hist(phats_100, main="100 tweets", xlab="p hat", xlim=c(.5,1), probability=TRUE)

# what if we sample 500 tweets
phats_500 <- replicate(1000, get_estimate(500))
hist(phats_500, main="500 tweets", xlab="p hat", xlim=c(.5,1), probability=TRUE)

# what about 1000 tweets
phats_1000 <- replicate(1000, get_estimate(1000))
hist(phats_1000, main="1000 tweets", xlab="p hat", xlim=c(.5,1), probability=TRUE)

# what about 5000 tweets
phats_5000 <- replicate(1000, get_estimate(5000))
hist(phats_5000, main="5000 tweets", xlab="p hat", xlim=c(.5,1), probability=TRUE)

# what about 10000 tweets
phats_10000 <- replicate(1000, get_estimate(10000))
hist(phats_10000, main="10000 tweets", xlab="p hat", xlim=c(.5,1), probability=TRUE)
```

---

We can make a couple of observations:

1. The distribution of estimate $\hat{p}$ is _centered_ at $p=.7$, our unknown _population_ proportion, and   

2. The _spread_ of the distribution **decreases** as the number of samples $n$ **increases**.

---

This was a simulation, we faked the **data generating procedure**. 

In reality, we can't.

--

What to do we do then?

(1) Math, or   
(2) Resample

---
layout: true

## Solve with Math

---

Our simulation is an illustration of two central tenets of statistics:

(a) The law of large numbers (LLN)   
(b) The central limit theorem (CLT)

---


### Law of large numbers (LLN)

Given _independently_ sampled random variables $X_1,X_2,\cdots,X_n$ with $\mathbb{E} [X_i]=\mu$ for all $i$,

$$
\frac{1}{n} \sum_i X_i \to \mu, \textrm{ as } n\to\infty
$$

I.E. $\overline{x}$ _tends_ to the expected value $\mu$ (under some assumptions beyond the scope of this class) regardless of the distribution $X_i$.

---

Implication: the sample mean was a good procedure to use to estimate parameters by matching their expected value! 

---

### Central Limit Theorem (CLT)

The LLN says that estimates built using the sample mean will tend to the correct answer 

The CLT describes how these estimates are _spread_ around the correct answer.

---

Here we will use the concept of _variance_ which is expected _spread_, measured in squared distance, from the _expected value_ of a random variable:

$$
\mathrm{var[X]} = \mathbb{E} [(X - \mathbb{E} [X])^2]
$$

---

Example: consider the variance of our random tweet example:

$$
\begin{aligned}
\mathrm{var[X]} & = \sum_{\mathcal{D}} (x-\mathbb{E} [X])^2 p(X=x) \\
{} & = (0 - p)^2 \times (1-p) + (1 - p)^2 \times p \\
{} & = p^2(1-p) + (1-p)^2p \\
{} & = p(1-p) (p + (1-p)) \\
{} & = p(1-p) (p - p + 1) \\
{} & = p(1-p)
\end{aligned}
$$

---

CLT: Given _independently_ sampled random variables $X_1,X_2,\cdots,X_n$ **from the same probability distribution** $P(X)$, with $\mathbb{E} [X_i]=\mu$ and $\mathrm{var}[X_i]=\sigma^2$ for all $i$,


$$
P\left( \frac{1}{n} \sum_{i=1} X_i \right) \to N \left( \mu,\frac{\sigma}{n} \right), \textrm{ as } n\to \infty 
$$

---

This says, that as sample size $n$ increases, the distribution of sample means is _well_ approximated by a **normal distribution**. 

This means we can approximate the _expected error_ of our estimates well.

---
layout: true

## (Continuous) Random Variables

---

### The normal distribution

Random variable $Y=\sum_{i=1}^n X_i$ is _continuous_.

The normal distribution describes the distribution of _continuous_ random variables over the range $(-\infty,\infty)$ using two parameters: 

**mean** $\mu$ and **standard deviation** $\sigma$.

--

We write " $Y$ is normally distributed with mean $\mu$ and standard deviation $\sigma$" as $Y\sim N(\mu,\sigma)$. 

---

Continuous random variables are described by a _probability density function_. For normally distributed random variables:

$$p(Y=y) = \frac{1}{\sqrt{2\pi}\sigma} \mathrm{exp} \left\{ -\frac{1}{2} \left( \frac{y-\mu}{\sigma} \right)^2 \right\}$$

---

Three examples of normal probability density functions with mean $\mu=60,50,60$ and standard deviation $\sigma=2,2,6$:

```{r, echo=FALSE, fig.align="center", fig.height=5.5}
# 100 equally spaced values between 40 and 80
yrange <- seq(40, 80, len=100)

# values of the normal density function
density_values_1 <- dnorm(yrange, mean=60, sd=2)
density_values_2 <- dnorm(yrange, mean=50, sd=2)
density_values_3 <- dnorm(yrange, mean=60, sd=6)

# now plot the function
plot(yrange, density_values_1, type="l", col="red", lwd=2, xlab="y", ylab="density")
lines(yrange, density_values_2, col="blue", lwd=2)
lines(yrange, density_values_3, col="orange", lwd=2)
legend("topright", legend=c("mean 60, sd 2", "mean 50, sd 2", "mean 60, sd 6"), col=c("red","blue","orange"), lwd=2)
```

---

Like the discrete case, probability density functions for continuous random variables need to satisfy certain conditions:

a. $p(Y=y) \geq 0$ for all values $Y \in (-\infty,\infty)$, and  
b. $\int_{-\infty}^{\infty} p(Y=y) dy = 1$

---

One way of interpreting the density function of the normal distribution is that probability decays exponentially with rate $\sigma$ based on squared distance to the mean $\mu$. (Here is squared distance again!)

$$
p(Y=y) \propto \exp \left\{ -{\frac{1}{2\sigma^2} (y-\mu)^2} \right \}
$$

---

Also, notice the term inside the square?

$$
z = \left( \frac{y - \mu}{\sigma} \right)
$$

this is the _standardization_ transformation we saw before. 

---

The name _standardization_ comes from the _standard normal distribution_ $N(0,1)$ (mean 0 and standard deviation 1), 

Which is very convenient to work with because it's density function is much simpler:

$$p(Z=z) = \frac{1}{\sqrt{2\pi}} \mathrm{exp} \left\{ -\frac{1}{2} z^2 \right\}$$

--

In fact, if random variable $Y \sim N(\mu,\sigma)$ then random variable $Z=\frac{Y-\mu}{\sigma} \sim N(0,1)$.

---

One more technicality:

The cumulative probability function for continuous random variables is given by

$$
P(Y\leq y) = \int_{\mathcal D} p(Y=y) dy
$$

where $\mathcal{D}$ is the range of values random variable $Y$ can take (e.g., for normal distribution $\mathcal{D}=(-\infty,\infty)$)

---
layout: true

## Solve with Math

---

### CLT continued

We need one last bit of terminology to finish the statement of the CLT. 

Consider data $X_1,X_2,\cdots,X_n$ with $\mathbb{E}[X_i]= \mu$ for all $i$, **and** $\mathrm{var}(X_i)=\sigma^2$ for all $i$, 

and sample mean $Y=\frac{1}{n} \sum_i X_i$. 

The standard deviation of $Y$ is called the _standard error_:

$$
\mathrm{se}(Y) = \frac{\sigma}{\sqrt{n}}
$$

---

Now we can restate the CLT statement precisely: 

the distribution of $Y$ tends _towards_ $N\left( \mu,\frac{\sigma}{\sqrt{n}} \right)$ as $n \rightarrow \infty$. 

This says, that as sample size increases the distribution of sample means is well approximated by a normal distribution, 

and that the spread of the distribution goes to zero at the rate $\sqrt{n}$.

---

_Disclaimer_ There a few mathematical subtleties. Two important ones are that

a. $X_1,\ldots,X_n$ are iid (independent, identically distributed) random variables, and  
b. $\mathrm{var}[X] < \infty$

---

Let's redo our simulated replications of our tweet samples to illustrate the CLT at work:

```{r, echo=FALSE, fig.align="center", fig.height=5.5, fig.width=9}
# we can calculate standard error for each of the
# settings we saw previously and compare these replications
# to the normal distribution given by the CLT

# let's write a function that adds a normal density
# plot for a given sample size
draw_normal_density <- function(n,p=.7) {
  se <- sqrt(p*(1-p))/sqrt(n)
  f <- dnorm(seq(0.5,1,len=1000), mean=p, sd=se)
  lines(seq(0.5,1,len=1000), f, col="red", lwd=1.6)
}

par(mfrow=c(2,3))
# what if we sample 10 tweets
phats_10 <- replicate(1000, get_estimate(10))
hist(phats_10, main="10 tweets", xlab="p hat", xlim=c(.5,1), probability=TRUE)
draw_normal_density(10)

# what if we sample 100 tweets
phats_100 <- replicate(1000, get_estimate(100))
hist(phats_100, main="100 tweets", xlab="p hat", xlim=c(.5,1), probability=TRUE)
draw_normal_density(100)

# what if we sample 500 tweets
phats_500 <- replicate(1000, get_estimate(500))
hist(phats_500, main="500 tweets", xlab="p hat", xlim=c(.5,1), probability=TRUE)
draw_normal_density(500)

# what about 1000 tweets
phats_1000 <- replicate(1000, get_estimate(1000))
hist(phats_1000, main="1000 tweets", xlab="p hat", xlim=c(.5,1), probability=TRUE)
draw_normal_density(1000)

# what about 5000 tweets
phats_5000 <- replicate(1000, get_estimate(5000))
hist(phats_5000, main="5000 tweets", xlab="p hat", xlim=c(.5,1), probability=TRUE)
draw_normal_density(5000)

# what about 10000 tweets
phats_10000 <- replicate(1000, get_estimate(10000))
hist(phats_10000, main="10000 tweets", xlab="p hat", xlim=c(.5,1), probability=TRUE)
draw_normal_density(10000)

```

---

Here we see the three main points of the LLN and CLT: 

(1) the normal density is centered around $\mu=.7$,  
(2) the normal approximation gets better as $n$ increases, and  
(3) the standard error goes to 0 as $n$ increases.

---
layout: true

## Solve with computation

### The Bootstrap Procedure

---

What if the conditions that we used for the CLT don't hold? 

For instance, samples $X_i$ may not be independent. What can we do then, how can we say something about the precision of sample mean estimate $Y$?

---

A useful procedure to use in this case is the **bootstrap**. 

It is based on using _randomization_ to simulate the stochasticity resulting from the population sampling procedure we are trying to capture in our analysis.

---

The main idea is the following: given observations $x_1,\ldots,x_n$ 

and the estimate $y=\frac{1}{n}\sum_{i=1}^n x_i$, 

what can we say about the standard error of $y$?

---

There are two challenges here: 

1) our estimation procedure is deterministic, that is, if I compute the sample mean of a specific dataset, I will always get the same answer; and 

2) we should retain whatever properties of estimate $y$ result from obtaining it from $n$ samples.

---

The bootstrap is a randomization procedure that measures the variance of estimate $y$, 

using randomization to address challenge (1), 

but doing so with randomized samples of size $n$, addressing challenge (2).

---

The procedure goes as follows:

1. Generate $B$ random datasets by sampling _with replacement_ from dataset $x_1,\ldots,x_n$. Denote randomized dataset $b$ as $x_{1b},\ldots,x_{nb}$.

2. Construct estimates from _each_ dataset, $y_b = \frac{1}{n}\sum_i x_{ib}$

3. Compute center (mean) and spread (variance) of estimates $y_b$

---

Let's see how this works on tweet oracle example

```{r, echo=FALSE, fig.height=5.5, fig.width=8, fig.align="center"}
# remember our dataset is in variable x

# this is how we get one bootstrap replicate
# sample n observations from dataset x _with replacement_
xb <- sample(x, length(x), replace=TRUE)

# let's do B=100 bootstrap randomizations using the 
# replicate function (it just replicates the given expression
# however many times it is directed to do so)
B <- 200
xb <- replicate(B, sample(x,length(x), replace=TRUE))

# xb is a matrix with 100 rows (the original length of dataset) and
# 200 columns (the number of replicates)

# now let's compute the bootstrap estimates y
yb <- colMeans(xb)

# and make a histogram of the bootstrap estimates
hist(yb, probability=TRUE, main="Histogram of bootstrap estimates",
     xlab="Bootsrap Estimates",xlim=c(0.5,1))
abline(v=p, col="blue")
draw_normal_density(100)
```

---

Not great, math works better when conditions are met.

---

Let's look at a case where we don't expect the normal approximation to not work so well by making  samples not identically distributed. 


Let's make a new ORACLE of tweet where the probability of a tweet being bot-generated depends on the previous tweet

---

```{r, echo=FALSE}
create_chain_dataset <- function(n=100, first_p=.7, change_p=.6) {
  res <- vector("numeric", n)
  res[1] <- sample(c(0,1), 1, prob=c(1-first_p, first_p))
  for (i in seq(2,n)) {
    res[i] <- ifelse(runif(1) <= change_p, 1-res[i-1], res[i-1])
  }
  res
}

chain_x <- create_chain_dataset(100)
```

```{r, echo=FALSE, cache=FALSE, fig.align="center", fig.width=8, fig.height=5.5}
# Now let's do the same bootstrap procedure in this case
xb <- replicate(B, sample(chain_x, length(chain_x), replace=TRUE))

# xb is a matrix with 100 rows (the original length of dataset) and
# 200 columns (the number of replicates)

# now let's compute the bootstrap estimates y
yb <- colMeans(xb)

# and make a histogram of the bootstrap estimates
hist(yb, probability=TRUE, main="Histogram of bootstrap estimates",
     xlab="Bootsrap Estimates", xlim=c(0,1))

draw_normal_density <- function(n,p=.7) {
  se <- sqrt(p*(1-p))/sqrt(n)
  f <- dnorm(seq(0,1,len=1000), mean=p, sd=se)
  lines(seq(0,1,len=1000), f, col="red", lwd=1.6)
}
  
draw_normal_density(100, mean(chain_x))
```

---

Here, an analysis based on the classical CLT is not  appropriate ( $X_i$ s are not independent) 

But the bootstrap analysis gives some information about the variability of our estimates. 

---
layout: true

## Summary

How do we learn information about a **population**? Estimation.

What can we say about estimates? Say something about sampling error.

How? Math (LLN and CL), or Computation (Bootstrap)

