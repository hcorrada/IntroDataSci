<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Tree-based Methods</title>
    <meta charset="utf-8" />
    <meta name="author" content="Héctor Corrada Bravo" />
    <meta name="date" content="2020-04-19" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="custom.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">




class: title-slide, center, middle
count: false

.banner[![](img/epiviz.png)]

.title[Introduction to Data Science: Tree-based Methods]

.author[Héctor Corrada Bravo]

.other-info[
University of Maryland, College Park, USA  
2020-04-19
]

.logo[![](img/logo.png)]

---

## Tree-based Methods

We saw in previous units the limitation of using linear methods for regression classification. 

--

In this unit, we look at tree-based methods.

--

These are elegant and versatile methods that allow modeling of predictor space with regions that take complex, non-linear, shapes

--

But still produce models that are interpretable. 

--

We will concentrate on Regression and Decision Trees and their extension to Random Forests.

---
class: split-30

## Regression Trees

.column[
Consider a task where we are trying to predict a car's fuel consumption in miles per gallon based on the car's weight. A linear model in this case is not a good fit.
]

.column[


&lt;img src="tree-methods_files/figure-html/trees_auto_plot-1.png" style="display: block; margin: auto;" /&gt;
]

---
class:split-50

## Regression Trees

Let's take a look at what a regression tree estimates in this case.

.column[
![](tree-methods_files/figure-html/unnamed-chunk-2-1.png)&lt;!-- --&gt;
]

.column[
![](tree-methods_files/figure-html/unnamed-chunk-3-1.png)&lt;!-- --&gt;
]

---
class:split-50

## Regression trees

The decision trees partitions the `weight` predictor into regions based on its value. 

.column[
![](tree-methods_files/figure-html/unnamed-chunk-4-1.png)&lt;!-- --&gt;
]



.column[
![](tree-methods_files/figure-html/unnamed-chunk-6-1.png)&lt;!-- --&gt;

```
## [1] 9
```
]

---
class: split-50

## Regression Trees

Outcome `\(Y\)` (`mpg` in this case) is predicted to be the mean _within each of the data partitions_. 

.column[
![](tree-methods_files/figure-html/unnamed-chunk-7-1.png)&lt;!-- --&gt;
]

.column[
![](tree-methods_files/figure-html/unnamed-chunk-8-1.png)&lt;!-- --&gt;

```
## [1] 9
```
]

---
class: split-50

## Regression Trees

Thus provides an empirical estimate of `\(E[Y|X=x]\)` where conditioning is given by this region partitioning.

.column[
![](tree-methods_files/figure-html/unnamed-chunk-9-1.png)&lt;!-- --&gt;
]

.column[
![](tree-methods_files/figure-html/unnamed-chunk-10-1.png)&lt;!-- --&gt;

```
## [1] 9
```
]

---

## Tree models

Regression and decision trees operate by prediction an outcome variable `\(Y\)` by partitioning feature (predictor) space.

--

In general, the regression tree model:

1. Partitions space into `\(J\)` non-overlapping regions, `\(R_1, R_2, \ldots, R_J\)`.
2. For every observation that falls within region `\(R_j\)`, predict response as mean of response for training observations in `\(R_j\)`.

--

The important observation is that **Regression Trees create partitions recursively**


---

## Tree Models

For example, consider finding a good predictor `\(j\)` to partition space along its axis. A recursive algorithm would look like this:

- Find predictor `\(j\)` and value `\(s\)` that minimize RSS:

`$$\sum_{i:\, x_i \in R_1(j,s))} (y_i - \hat{y}_{R_1})^2 +
\sum_{i:\, x_i \in R_2(j,s))} (y_i - \hat{y}_{R_2})^2$$`

Where `\(R_1\)` and `\(R_2\)` are regions resulting from splitting observations on predictor `\(j\)` and value `\(s\)`:

$$
R_1(j,s) = \{X|X_j &lt; s\} \, \mathrm{ and } \, R_2(j,s) \{X|X_j \geq s\}
$$

---

## Tree Models

For example, consider finding a good predictor `\(j\)` to partition space along its axis. A recursive algorithm would look like this:

- Find predictor `\(j\)` and value `\(s\)` that minimize RSS:

`$$\sum_{i:\, x_i \in R_1(j,s))} (y_i - \hat{y}_{R_1})^2 +
\sum_{i:\, x_i \in R_2(j,s))} (y_i - \hat{y}_{R_2})^2$$`

- Apply recursively to regions `\(R_1\)` and `\(R_2\)`. 

---

## Tree Models

Within each region a prediction `\(\hat{y}_{R_j}\)` is made as the mean of the response `\(Y\)` of observations in `\(R_j\)`.

![](img/8.3.png)

---
class: split-50

## Regression Trees

Consider building a model that used both `horsepower` and `weight`. 

.column[
Here, value of the response `\(Y\)` is indicated by the size of the point.
]

.column[
![](tree-methods_files/figure-html/unnamed-chunk-11-1.png)&lt;!-- --&gt;
]

---
class: split-50

## Regression Trees

This is what a decision tree would look like for these two predictors:

.column[
![](tree-methods_files/figure-html/unnamed-chunk-12-1.png)&lt;!-- --&gt;
]



.column[
![](tree-methods_files/figure-html/unnamed-chunk-14-1.png)&lt;!-- --&gt;
]

---
class: split-50

## Regression Trees

.column[
![](tree-methods_files/figure-html/unnamed-chunk-15-1.png)&lt;!-- --&gt;
]

.column[
_Quiz_ What would this tree predict as `mpg` for an instance with variable values

- `horsepower=85`
- `weight=2800`
]

---

## Classification (Decision) Trees

Classification, or decision trees, are used in classification problems, where the outcome is categorical. 

--

The same partitioning principle holds, but now, each region predicts the majority class for training observations within region. 

--

The recursive partitioning method requires a score function to choose predictors (and values) to partition with. 

--

A naive approach would looking for partitions that minimize training error. 

--

Better performing approaches use more sophisticated metrics.

---
class: split-50

## Decision Trees

Let's look at how a classification tree performs on a credit card default dataset.


.column[
![](tree-methods_files/figure-html/unnamed-chunk-17-1.png)&lt;!-- --&gt;
]



.column[
![](tree-methods_files/figure-html/unnamed-chunk-19-1.png)&lt;!-- --&gt;
]

---

## Specifics of the partitioning algorithm

### The predictor space

Suppose we have `\(p\)` predictor attributes `\(X_1,\ldots,X_p\)` and `\(n\)` observations. 

--

Each of the `\(X_i\)` can be 

a)   a numeric variable: there are `\(n-1\)` possible splits  
b)   an ordered factor (categorical variable): there are `\(k-1\)` possible splits  
c)   an unordered factor: `\(2^{k-1}-1\)` possible splits.

---

## Specifics of the partitioning algorithm

### Learning Strategy

The general procedure for tree learning is the following:

**Grow**: an overly large tree using forward selection as follows: at each step, find the *best* split among all attributes. Grow until all terminal nodes either

(a) have `\(&lt; m\)` (perhaps `\(m=1\)`) data points  
(b) are "pure" (all points in a node have [almost] the same outcome).

---

## Specifics of the partitioning algorithm

### Learning Strategy

The general procedure for tree learning is the following:

**Grow**: an overly large tree using forward selection 

**Prune**: the tree back, creating a nested sequence of trees, decreasing in *complexity*

---

## Specifics of the partitioning algorithm

### Tree Growing

The recursive partitioning algorithm is as follows:

INITIALIZE    All cases in the root node  
REPEAT	      Find optimal allowed split; Partition leaf according to split  
STOP	        Stop when pre-defined criterion is met

---

## Specifics of the partitioning algorithm

### Tree Growing

An important issue in tree construction is how to use the training data to determine the binary splits of dataset `\(\mathbf{X}\)` 

--

The fundamental idea is to select each split of a subset so that the data in each of the descendent subsets are "purer" than the data in the parent subset.

---
exclude:true

## Specifics of the partitioning algorithm

### Deviance as a measure of impurity

A simple approach is to assume a multinomial model and then use deviance as a definition of impurity.

---
exclude: true

## Specifics of the partitioning algorithm

### Deviance as a measure of impurity

Assume `\(Y \in \mathcal{G}=\{1,2,\ldots,k\}\)`.

*      At each node `\(i\)` of a classification tree we have a probability
       distribution `\(p_{ik}\)` over the `\(k\)` classes.

*      We observe a random sample `\(n_{ik}\)` from the multinomial
       distribution specified by the probabilities `\(p_{ik}\)`.

---
exclude: true

## Specifics of the partitioning algorithm

### Deviance as a measure of impurity

Assume `\(Y \in \mathcal{G}=\{1,2,\ldots,k\}\)`.

*      Given `\(X\)`, the conditional likelihood is then proportional to `\(\prod_{(\text{leaves } i)} \prod_{(\text{classes } k)} p_{ik}^{n_{ik}}\)`.

*      Estimate `\(p_{ik}\)` by `\(\hat{p}_{ik}=\frac{n_{ik}}{n_i}\)`.

*      Define deviance `\(D=\sum D_i\)`, where `\(D_i=-2\sum_k n_{ik} \log(p_{ik})\)`.

--
exclude: true

Select splits that improve deviance `\(D\)`

---
exclude: true

## Specifics of the partitioning algorithm

### Deviance as a measure of impurity

_Quiz_ Compute deviance for the following cases

a) `\(n_{i1}=6, \, n_{i2}=1, \, n_{i3}=1\)`  
b) `\(n_{i1}=9, \, n_{i2}=1, \, n_{i3}=0\)`  
c) `\(n_{i1}=90, \, n_{i2}=10, \, n_{i3}=0\)`  

---

## Specifics of the partitioning algorithm

### Measures of impurity

Commonly used measures of impurity at a node `\(i\)` of a
classification tree are

**missclasification rate**: `\(\frac{1}{n_i} \sum_{j\in A_i} I(y_j \neq k_i)=1-\hat{p}_{ik_i}\)`  
**entropy**: `\(\sum p_{ik} \log(p_{ik})\)`  
**GINI index**: `\(\sum_{j\neq k} p_{ij}p_{ik} = 1-\sum_k p_{ik}^2\)`  

where `\(k_i\)` is the most frequent class in node `\(i\)`.

--

In practice, the GINI index is preferred

---

## Specifics of the partitioning algorithm

For regression trees we use the residual sum of squares:

`$$D = \sum_{\text{cases } j} (y_j-\mu_{[j]})^2$$`

where `\(\mu_{[j]}\)` is the mean values in the node that case `\(j\)` belongs
to.

---
exclude: true

## Specifics of the partitioning algorithm

### Tree Pruning

*	Grow a big tree `\(T\)`
*	Consider snipping off terminal subtrees (resulting in
	so-called rooted subtrees)
*	Let `\(D_i\)` be a measure of impurity at leaf `\(i\)` in a
	tree. Define `\(D=\sum_i D_i\)`
*	Define size as the number leaves in a tree
*	Let `\(D_{\alpha} = D + \alpha \times \mathrm{size}\)`

--
exclude: true

The set of rooted subtrees of `\(T\)` that minimize `\(D_{\alpha}\)` is
nested.

---
exclude: true

## Specifics of the partitioning algorithm

### Tree Pruning

We can prune the tree sequentially

--
exclude: true

Given tree `\(T\)`, 
- for every node `\(R_j\)` in tree, compute `\(D_{\alpha}\)` after removing subtree rooted at `\(R_j\)`  
- select node `\(R_j\)` that minimizes `\(D_{\alpha}\)`
- Remote subtree rooted at `\(R_j\)` from `\(T\)`
- Continue until `\(D_{\alpha}\)` increases


---

## Properties of Tree Method

Good properties of Regression and Classification trees include:

* Decision trees are very "natural" constructs, in particular when the explanatory variables are catgorical (and even better when they are binary)


* Trees are easy to explain to non-data analysts

* The models are invariant under transformations in the predictor space

---

## Properties of Tree Method

Good properties of Regression and Classification trees include:

* Multi-factor responses are easily dealt with

* The treatment of missing values is more satisfactory than for most other models

* The models go after interactions immediately, rather than as an afterthought

* Tree growth is much more efficient than described here


---

## Properties of Tree Method

However, they do have important issues to address

* Tree space is huge, so we may need lots of data

* We might not be able to find the *best* model at all as it is a greedy algorithm

* It can be hard to assess uncertainty in inference about trees

---

## Properties of Tree Method

However, they do have important issues to address

* Results can be quite variable (tree selection is not very stable)

* Simple trees usually don't have a lot of predictive power

---

## Random Forests

Random Forests are a **very popular** approach that addresses these shortcomings via resampling of the training data. 

--

Their goal is to improve prediction performance and reduce instability by _averaging_ multiple decision trees (a forest constructed with randomness). 

---

## Random Forests

It uses two ideas to accomplish this. The first idea is *Bagging* (bootstrap aggregation)

General scheme:
  1. Build many decision trees `\(T_1, T_2, \ldots, T_B\)` from training set  
  2. Given a new observation, let each `\(T_j\)` predict `\(\hat{y}_j\)`  
  3. For regression: predict average `\(\frac{1}{B} \sum_{j=1}^B \hat{y}_j\)`,
     for classification: predict with majority vote (most frequent class)
     
---
class: split-50

## Random Forests

How do we get many decision trees from a single training set?

.column[
Use the _bootstrap_ resampling technique. 
]

.column[
.image-50[![](img/bootstrap.png)]
]

---
class: split-50

## Random Forests

How do we get many decision trees from a single training set?

.column[
To create `\(T_j, \, j=1,\ldots,B\)` from training set of size `\(n\)`:

a) create a bootstrap training set by sampling `\(n\)` observations from training set **with replacement**  
]

.column[
.image-50[![](img/bootstrap.png)]
]

---
class: split-50

## Random Forests

How do we get many decision trees from a single training set?

.column[
To create `\(T_j, \, j=1,\ldots,B\)` from training set of size `\(n\)`:

b) build a decision tree from bootstrap training set
]

.column[
.image-50[![](img/bootstrap.png)]
]

---

## Random Forests

The second idea used in Random Forests is to use a random selection of features to split when deciding partitions. 

--

Specifically, when building each tree `\(T_j\)`, at each recursive partition:

--

_only consider a randomly selected subset of predictors to find best split._ 

--

This reduces correlation between trees in forest, improving prediction accuracy.

---

## Random Forests





Let's look at the same car dataset again and plot predicted vs. true miles per gallon given by a random forest and a regression tree.



&lt;img src="tree-methods_files/figure-html/unnamed-chunk-23-1.png" style="display: block; margin: auto;" /&gt;

---

## Random Forests

Now let's look at the same plot on a _testing_ dataset.



&lt;img src="tree-methods_files/figure-html/unnamed-chunk-25-1.png" style="display: block; margin: auto;" /&gt;



---

## Random Forests

A disadvantage of random forests is that we lose interpretability. 

--

However, we can use the fact that a bootstrap sample was used to construct trees to measure _variable importance_ from the random forest.

--

Since we used bootstrap samples we can get out-of-bag (OOB) samples for each tree in the random forest.

---

## Random Forests

When the `\(b\)`th tree is constructed, use the OOB samples as follows

1. Compute error rate for the OOB samples
2. For each predictor `\(j\)`: 

  a. permute its values in the OOB samples and recompute error rate  
  b. calculate increase in error rate 
  
Report increase in error rate over all bootstrap samples

---

## Random Forests

Here is a table of _variable importance_ for the random forest we just constructed.

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt;   &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; %IncMSE &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; IncNodePurity &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; cylinders &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 13.22 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2238.51 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; displacement &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 17.79 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2161.24 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; horsepower &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 17.16 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1711.24 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; weight &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 20.54 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 3087.59 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; acceleration &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 13.64 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 442.72 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; year &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 42.08 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1813.08 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---

## Random Forests

And a plot of variable importance

&lt;img src="tree-methods_files/figure-html/unnamed-chunk-28-1.png" style="display: block; margin: auto;" /&gt;



---

## Tree-based methods summary

Tree-based methods are highly interpretable _prediction_ models.

--

Some inferential tasks are possible (e.g., variable importance in random forests), but are much more limited than linear models.

--

These methods are very commonly used across many application domains

--

Random Forests often perform at state-of-the-art for many tasks.
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();</script>
<script>
remark.macros['scale'] = function (percentage) {
  var url = this;
  return '<img src="' + url + '" style=width: ' + percentage + '"/>';
};
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
