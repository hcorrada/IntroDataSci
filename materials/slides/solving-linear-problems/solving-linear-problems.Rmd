---
title: "Solving Linear Problems"
author: "Héctor Corrada Bravo"
company: "University of Maryland"
date: "`r Sys.Date()`"
css: ["custom.css"]
output:
  xaringan::moon_reader:
    lib_dir: libs
    seal: false
    includes: 
      after_body: "custom.html"
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
---

class: title-slide, center, middle
count: false

.banner[![](img/epiviz.png)]

.title[Introduction to Data Science: Solving Linear Problems]

.author[Héctor Corrada Bravo]

.other-info[
University of Maryland, College Park, USA  
`r Sys.Date()`
]

.logo[![](img/logo.png)]

```{r setup1, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(cache=TRUE)
```

---
layout: true

## Solving linear problems

---

How to fit the type of analysis methods we've seen so far?

We will use linear regression as a case study of how this insight would work.

---

### Case Study

**Given**: Training set $\{(x_1, y_1), \ldots, (x_n, y_n)\}$, with continuous response $y_i$ and single predictor $x_i$ for the $i$-th observation.

**Do**: Estimate parameter $\beta_1$ in model $y=\beta_1 x$ to solve

$$\min_{\beta_1} L(\beta_1) = \frac{1}{2} \sum_{i=1}^n (y_i - \beta_1 x_i)^2$$

---

And suppose we want to fit this model to the following (simulated) data:

```{r,echo=FALSE, fig.align="center", fig.height=6}
set.seed(1234)
true_beta <- 5
x <- runif(100, -10, 10)
y <- x * true_beta + rnorm(100, mean=0, sd=sqrt(10))
plot(x,y,pch=19,cex=1.4,main="Simulated Data", cex.lab=1.5, cex.main=2)
abline(a=0, b=true_beta, col="red", lwd= 2)
```

---

Our goal is then to find the value of $\beta_1$ that minimizes mean squared error. This corresponds to finding one of these many possible lines:

```{r, echo=FALSE, fig.align="center", fig.height=6}
plot(x,y,pch=19,cex=1.4,main="Simulated Data", cex.lab=1.5, cex.main=2)
abline(a=0, b=true_beta, col="red", lwd= 2)
for (b in seq(-6,6, len=5)) {
  abline(a=0,b=b,col="blue", lwd=2, lty=2)
}
legend("bottom", legend=paste("beta=", seq(-6,6,len=5)), lwd=2, lty=2, cex=1.5)
```

---

Each of which has a specific error for this dataset:

```{r, echo=FALSE, fig.align="center", fig.height=6}
n <- length(y)
compute_loss <- function(beta, x, y) {
  0.5 * mean((y-x*beta)^2)
}
beta <- seq(-20, 20, len=100)
plot(beta, sapply(beta, compute_loss, x=x, y=y), type="l", lwd=2, ylab=expression(L(beta[1])),cex.lab=1.5,xlab=expression(beta[1]))
abline(v=true_beta, col="red", lwd=2)
abline(v=seq(-6,6,len=5), col="blue", lwd=2, lty=2)
```

---

Insights:

1) As we saw before in class, loss is minimized when the derivative of the loss function is 0

2) and, the derivative of the loss (with respect to $\beta_1$ ) at a given estimate $\beta_1$ suggests new values of $\beta_1$ with smaller loss!

---

Let's take a look at the derivative:

$$\frac{\partial}{\partial \beta_{1}} L(\beta_1) = \frac{\partial}{\partial \beta_{1}} \frac{1}{2} \sum_{i=1}^n (y_i - \beta_1 x_i)^2 \\
{} = \sum_{i=1}^n (y_i - \beta_1 x_i) \frac{\partial}{\partial \beta_1} (y_i - \beta_1 x_i) \\
{} = \sum_{i=1}^n (y_i - \beta_1 x_i) (-x_i)$$


---

and plot it for our case study data:

```{r, echo=FALSE, cache=FALSE, fig.align="center", fig.height=6}
loss_derivative <- function(beta, x, y) {
  f <- beta * x
  resid <- y - f
  sum(resid * (-x))
}

plot(beta, sapply(beta, loss_derivative, x=x, y=y), type="l", lwd=1.5, xlab=expression(beta[1]), ylab=expression(partialdiff * L(beta[1]) / partialdiff * beta[1]),cex.lab=1.7)

abline(v=true_beta, col="red", lwd=2)
abline(v=seq(-6,6,len=5), col="blue", lwd=2, lty=2)
abline(h=0, col="black", lwd=2, lty=2)
```

---

### Gradient Descent

This plot suggests an algorithm:

1. Initialize $\beta_1^0=0$
2. Repeat for $k=1,2,\ldots$ until convergence
  - Set $\beta_1^k = \beta_1^{k-1} + \alpha \sum_{i=1}^n (y_i - f(x_i;\beta_1^{k-1})) x_i$

Note: $f(x_i;\beta_1) = \beta_1 x_i$
---

This algorithm is called **gradient descent** in the general case.

The basic idea is to move the current estimate of $\beta_1$ in the direction that minimizes loss the *fastest*. 

Another way of calling this algorithm is **Steepest Descent**.

---


```{r,echo=FALSE, fig.align="center", fig.height=6}
# Implementation of gradient descent for least squares regression
# for a single predictor (x)
#
# There is some code here that is only used to generate illustrative plots and would not be part of real solver
gradient_descent <- function(x, y, tol=1e-6, maxit=50, plot=FALSE) {
  # initialize estimate
  beta_1 <- 0; old_beta_1 <- Inf; i <- 0; beta_keep <- NA
  
  # compute loss at first estimate
  loss <- compute_loss(beta_1, x, y); loss_keep <- NA
  
  # starting step size
  alpha <- 1e-3
  difference <- Inf
  
  # check for convergence
  # (in practice, we do include a limit on the number of iterations)
  while ((difference > tol) && (i < maxit)) {
    cat("it: ", i, " beta: ", round(beta_1, 2), "loss: ", round(loss, 2), " alpha: ", round(alpha, 6), "\n")
    
    # this piece of code just adds steps to an existing plot
    if (plot && !is.na(beta_keep) && !is.na(loss_keep)) {
      suppressWarnings(arrows(beta_keep, loss_keep, beta_1, loss, lty=2, col="blue"))
    }
    
    # store the last estimate for plotting
    beta_keep <- beta_1; loss_keep <- loss;
    
    # store the last estimate to check convergence
    old_beta_1 <- beta_1
    
    # update estimate
    f <- beta_1 * x
    resid <- y - f    
    beta_1 <- beta_1 + alpha * sum(resid * x)
    
    # compute difference after taking step
    # to check convergence
    difference <- (beta_1 - old_beta_1)^2 / (old_beta_1)^2
  
    # compute loss and derivative for updated estimate
    loss <- compute_loss(beta_1, x, y)

    i <- i+1
    
    # shorten the step size
    if ((i %% 3) == 0) alpha <- alpha / 2
  }
  if (plot) {
    suppressWarnings(arrows(beta_keep, loss_keep, beta_1, loss, lty=2, col="blue"))
  }
  beta_1
}
```


Let's run this algorithm and track what it does:

```{r, echo=FALSE, cache=FALSE, fig.align="center", fig.height=6}
plot(beta, sapply(beta, compute_loss, x=x, y=y), type="l", lwd=2, ylab=expression(L(beta[1])),cex.lab=1.5,xlab=expression(beta[1]), xlim=c(-20,20), main="Gradient Descent")

estimate <- gradient_descent(x, y, plot=TRUE)
```

---

This algorithm is referred to as "Batch" gradient descent, 

we take a step (update $\beta_1$) by calculating derivative with respect to _all_ $n$ observations in our dataset. 

---

For clarity, let's write out the update equation again:

$$\beta_1^k = \beta_1^{k-1} + \alpha \sum_{i=1}^n (y_i - f(x_i; \beta_1^{k-1})) x_i$$

where $f(x_i; \beta_1) = \beta_1 x_i$.

---

For multiple predictors (e.g., adding an intercept), this generalizes to the _gradient_ i.e., the vector of first derivatives of _loss_ with respect to parameters.

In this case, the model sets 

$$\begin{array}{l} 
f(\mathbf{x}_i; \mathbf{\beta}) & = & \beta_0 + \beta_1 x_{i1} + \cdots + \beta_p x_{ip} \\
{} & = & \sum_{j=0}^p \beta_j x_{ij} \\
{} & = & \beta'x
\end{array}$$

Note: we take $x_{i0} = 1$

---
exclude: false

The gradient given by partial derivatives for each parameter

$$\nabla_{\mathbf{\beta}}L(\mathbf{\beta}) = \begin{bmatrix}
\frac{\partial L(\beta)}{\partial \beta_0} \\
\frac{\partial L(\beta)}{\partial \beta_1} \\
\vdots \\
\frac{\partial L(\beta)}{\partial \beta_p}
\end{bmatrix}$$

---
exclude: false

The update equation is exactly the same for least squares regression

$$\mathbf{\beta}^k = \mathbf{\beta}^{k-1} + \alpha \sum_{i=1}^n (y_i - f(\mathbf{x}_i; \beta^{k-1})) \mathbf{x}_i$$



where $f(\mathbf{x}_i; \mathbf{\beta}) = \beta' \mathbf{x}_i$

Note: $x_{i0}=1$

---

Gradient descent falls within a family of optimization methods called _first-order methods_ 

These methods have properties amenable to use with very large datasets:

1. Inexpensive updates    
2. "Stochastic" version can converge with few sweeps of the data  
3. "Stochastic" version easily extended to streams  
4. Easily parallelizable  

Drawback: Can take many steps before converging

---

### Logistic Regression

Gradient descent is also used to solve the logistic regression problem. The same procedure follows 

(1) define a loss function;  
(2) derive the update equation;  
(3) run the iterative gradient descent algorithm.  

---

Let's take a look at the first two steps in this case.

For logistic regression, we turn to _maximum likelihood_ to formulate a loss function. 

For the logistic regression problem we are given dataset $\{\langle \mathbf{x}_1, y_1\rangle, \ldots, \langle \mathbf{x}_n, y_n \rangle \}$, where outcomes $y_i \in \{0,1\}$ since we are learning a binary classification problem. 

---

The goal is to estimate parameters $\mathbf{\beta}$ in model

$$\begin{array}{l}
\log{ \frac{p(Y=1 | \mathbf{X}=\mathbf{x})}{1-p(Y=1 | \mathbf{X}=\mathbf{x})}} & = &
\beta_0 + \beta_0 x_{1} + \cdots + \beta_p x_{p} \\
{} & = & \beta'\mathbf{x}
\end{array}$$

Note: $x_{i0}=1$

---

To establish a loss function we first assume a model for data generation. The assumption we make here is if an entity has attribute values $\mathbf{x}$, then the outcome $Y=1$ with probability given by 

$$p(\mathbf{x}; \mathbf{\beta}) = \frac{e^{f(\mathbf{x}; \mathbf{\beta})}}{1+e^{f(\mathbf{x}; \mathbf{\beta})}}$$


Note that we use the same notation $f(\mathbf{x}; \beta)=\beta'\mathbf{x}$ as we did in linear regression.

---

Now, we can ask, what is the probability of the data we observe for entity $i$ under this model?
We can write this probability in this form:

$$
p(\mathbf{x}_i; \mathbf{\beta})^{y_i}(1-p(\mathbf{x}_i; \mathbf{\beta}))^{(1-y_i)}
$$

---

Now, we can put these together for all observed entities since we assume that these are generated independently to get a _likelihood_ function:

$$\mathcal{L}(\mathbf{\beta}) = \prod_{i=1}^n p_i(\mathbf{x}_i;\mathbf{\beta})^{y_i}(1-p_i(\mathbf{x}_i;\mathbf{\beta}))^{(1-y_i)}$$

---

Now, we need to turn this into a loss function we can _minimize_. 

The likelihood function we wrote down is one we would _maximize_. 

Also, it is usually more convenient to work with the logarithm of likelihoods. 

---

The loss function we use for gradient descent is the _negative log likelihood_

$$L(\mathbf{\beta}) = \sum_{i=1}^n -y_i f(\mathbf{x}_i;\mathbf{\beta}) + \log(1+e^{f(\mathbf{x}_i;\mathbf{\beta})})$$

---

So, now that we have a loss function, we need to derive it's gradient to use the gradient descent algorithm. Check the lecture notes.

$$\nabla_{\mathbf{\beta}}
L(\mathbf{\beta}) =  
\sum_{i=1}^n (p(\mathbf{x}_i; \mathbf{\beta}) - y_i) \mathbf{x}_i$$

---

Note the nice similarity to the gradient for linear regression. 

It multiplies each data (expanded) data vector $\mathbf{x}_i$ by the difference between a prediction, in this case the probability that the outcome $y_i=1$ and the observed outcome $y_i$. 

---
layout: true

## Stochastic gradient descent

---

**Key Idea**: Update parameters using update equation _one observation at a time_:

1. Initialize $\beta=\mathbf{0}$, $i=1$
2. Repeat until convergence
  - For $i=1$ to $n$
    - Set $\beta = \beta + \alpha (y_i - f(\mathbf{x}_i, \beta)) \mathbf{x}_i$

---

```{r, echo=FALSE}
# Implementation of stochastic gradient descent for least squares regression
# for a single predictor (x)
#
# There is some code here that is only used to generate illustrative plots
stochastic_gradient_descent <- function(x, y, tol=1e-6, maxit=50, plot=FALSE) {
  n <- length(y)
  
  # initialize estimate
  beta_1 <- 0; i <- 0; beta_keep <- NA
  
  # compute loss at first estimate
  loss <- compute_loss(beta_1, x, y); loss_keep <- NA
  
  # initial step size
  alpha <- 1e-3
  difference <- Inf
  
  # check for convergence
  # (in practice a max number of iterations is used)
  while ((difference > tol) && (i < maxit)) {
    cat("it: ", i, " beta: ", round(beta_1, 2), "loss: ", round(loss, 2), " alpha: ", round(alpha, 6), "\n")
    
    # store last estimate to check convergence
    old_beta_1 <- beta_1
    
    # iterate over observations
    for (j in seq(1,n)) {
      
      # add step to plot
      if (plot && !is.na(beta_keep) && !is.na(loss_keep)) {
        suppressWarnings(arrows(beta_keep, loss_keep, beta_1, loss, lty=2, col="blue"))
      }
      
      # store last estimate and loss for plotting
      beta_keep <- beta_1; loss_keep <- loss;
      
      # update estimate with j-th observation
      f <- beta_1 * x[j]
      resid <- y[j] - f      
      beta_1 <- beta_1 + alpha * resid * x[j]
      
      # compute loss with new estimate
      loss <- compute_loss(beta_1, x, y)
    }
    
    # check difference between current and old estimate
    # to check convergence
    difference <- (beta_1 - old_beta_1)^2 / old_beta_1^2
    i <- i+1
    
    # update step size
    if ((i %% 5) == 0) alpha <- alpha / 2
  }
  
  if (plot) {
    suppressWarnings(arrows(beta_keep, loss_keep, beta_1, loss, lty=2, col="blue"))
  }
  
  beta_1
}
```

Let's run this and see what it does:

```{r, echo=FALSE, cache=FALSE, fig.align="center", fig.height=6}
plot(beta, sapply(beta, compute_loss, x=x, y=y), type="l", lwd=2, ylab=expression(L(beta[1])),cex.lab=1.5,xlab=expression(beta[1]), xlim=c(-20,20), main="Stochastic Gradient Descent")
estimate <- stochastic_gradient_descent(x, y, plot=TRUE)
```

---

The stochastic gradient descent algorithm can easily adapt to _data streams_ where we receive observations one at a time and _assume_ they are not stored. 

This setting falls in the general category of _online_ learning.

---

## Parallelizing gradient descent

Gradient descent algorithms are easily parallelizable:

- Split observations across computing units  
- For each step, compute partial sum for each partition (map), compute final update (reduce)  

---

$$\beta^k = \beta^{k-1} + \alpha * \sum_{\mathrm{partition}\; P} \sum_{i \in P} (y_i - f(\mathbf{x_i}; \beta^{k-1})) \mathbf{x}_i$$

---

This observation has resulted in their implementation if systems for large-scale learning:

1. [Vowpal Wabbit](https://github.com/JohnLangford/vowpal_wabbit/wiki)
  - Implements general framework of (sparse) stochastic gradient descent for many optimization problems
  - R interface: [http://cran.r-project.org/web/packages/RVowpalWabbit/index.html]
  
---

This observation has resulted in their implementation if systems for large-scale learning:

2. [Spark MLlib](https://spark.apache.org/docs/1.2.1/mllib-guide.html)
  - Implements many learning algorithms using Spark framework we saw previously
  - Some access to the MLlib API via R, but built on primitives accessible through `SparkR` library we saw previously
  
