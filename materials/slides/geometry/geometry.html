<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Data Analysis with Geometry</title>
    <meta charset="utf-8" />
    <meta name="author" content="Héctor Corrada Bravo" />
    <meta name="date" content="2020-04-05" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/default-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="custom.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">


class: title-slide, center, middle
count: false

.banner[![](img/epiviz.png)]

.title[Introduction to Data Science: Data Analysis with Geometry]

.author[Héctor Corrada Bravo]

.other-info[
University of Maryland, College Park, USA  
2020-04-05
]

.logo[![](img/logo.png)]



---
layout: true

## Data Analysis with Geometry

---

A common situation: 

- an outcome attribute (variable) `\(Y\)`, and   
- one or more independent covariate or predictor attributes `\(X_1,\ldots,X_p\)`. 

One usually observes these variables for multiple "instances" (or entities).  

---

One may be interested in various things: 

- What effects do the covariates `\(X_i\)` have on the outcome `\(Y\)`? 
- How well can we quantify these effects? 
- Can we predict outcome `\(Y\)` using covariates `\(X_i\)`?, etc...

---

## Motivating Example: Credit Analysis

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; default &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; student &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; balance &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; income &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; No &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; No &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 729.5265 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 44361.625 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; No &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Yes &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 817.1804 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 12106.135 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; No &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; No &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1073.5492 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 31767.139 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; No &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; No &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 529.2506 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 35704.494 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; No &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; No &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 785.6559 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 38463.496 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; No &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Yes &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 919.5885 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 7491.559 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---

**Task** predict account default

What is the outcome `\(Y\)`?  
What are the predictors `\(X_j\)`?

---
exclude: true

We will sometimes call attributes `\(Y\)` and `\(X\)` the
outcome/predictors, sometimes observed/covariates, and even
input/output. We may call each entity an observation or example.
We will denote predictors with `\(X\)` and outcomes with `\(Y\)`
(quantitative) and `\(G\)` (qualitative). Notice `\(G\)` are not numbers, so
we cannot add or multiply them. We will use `\(G\)` to denote the set of possible
values. For gender it would be `\(G=\{Male,Female\}\)`. 

---
layout: true

## From data to feature vectors

---

The vast majority of ML algorithms we see in class treat instances as "feature vectors". 

We can represent each instance as a _vector_ in Euclidean space `\(\langle x_1,\ldots,x_p,y \rangle\)`. 

--

- every measurement is represented as a continuous value
- in particular, categorical variables become numeric (e.g., one-hot encoding)

---

Here is the same credit data represented as a matrix of feature vectors

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:right;"&gt; default &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; student &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; balance &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; income &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1717.0716 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 38408.89 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1983.2345 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 25687.93 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; -1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 883.1573 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 18213.08 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1975.6530 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 38221.84 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; -1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.0000 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 32809.33 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; -1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 528.0893 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 46389.34 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---
layout: true

## Technical notation

---

- Observed values will be denoted in lower case. So `\(x_i\)`
means the `\(i\)`th observation of the random variable `\(X\)`. 

- Matrices are
represented with bold face upper case. For example `\(\mathbf{X}\)` will
represent all observed predictors. 
- `\(N\)` (or `\(n\)`) will usually mean the number of
observations, or length of `\(Y\)`. `\(i\)` will be used to denote which
observation and `\(j\)` to denote which covariate or predictor. 

---

- Vectors
will not be bold, for example `\(x_i\)` may mean all predictors for
subject `\(i\)`, unless it is the vector of a particular predictor
`\(\mathbf{x}_j\)`. 
- All vectors are assumed to be column vectors, so the
`\(i\)`-th row of `\(\mathbf{X}\)` will be `\(x_i'\)`, i.e., the transpose of
`\(x_i\)`.

---
layout: true

## Geometry and Distances

---

Now that we think of instances as vectors we can do some interesting operations.

Let's try a first one: define a distance between two instances using Euclidean distance

`$$d(x_1,x_2) = \sqrt{\sum_{j=1}^p(x_{1j}-x_{2j})^2}$$`

---

### K-nearest neighbor classification

Now that we have a distance between instances we can create a classifier. Suppose we want to predict the class for an instance `\(x\)`. 

K-nearest neighbors uses the closest points in predictor space predict `\(Y\)`. 


$$
\hat{Y} = \frac{1}{k} \sum_{x_k \in N_k(x)} y_k.
$$

`\(N_k(x)\)` represents the `\(k\)`-nearest points to `\(x\)`. How would you use `\(\hat{Y}\)` to make a prediction?


---

![](img/knnalgo.png)

---

### Inductive bias

The assumptions we make about our data that allow us to make predictions. 

In KNN, our _inductive bias_ is that points that are **nearby** will be of the same class. 

---

Parameter `\(K\)` is a _hyper-parameter_, it's value may affect prediction accuracy significantly.

Question: which situation may lead to _overfitting_, high or low values of `\(K\)`? Why?

---
layout: true

## The importance of transformations

---
class: split-40

Feature scaling is an important issue in distance-based methods.

.column[
Which of these two features will affect distance the most?]

.column[
&lt;img src="geometry_files/figure-html/unnamed-chunk-3-1.png" style="display: block; margin: auto;" /&gt;
]

---
layout: true

## Quick vector algebra review

---

- A (real-valued) vector is just an array of real values, for instance `\(x = \langle 1, 2.5, −6 \rangle\)` is a three-dimensional
vector. 

- Vector sums are computed pointwise, and are only defined when dimensions match, so 

`$$\langle 1, 2.5, −6 \rangle + \langle 2, −2.5, 3 \rangle =  \langle 3, 0, −3 \rangle$$`.

In general, if `\(c = a + b\)` then `\(cd = ad + bd\)`
for all vectors `\(d\)`. 

---

Vector addition can
be viewed geometrically as taking a vector `\(a\)`, then tacking on `\(b\)` to the end of it; the new end point is
exactly `\(c\)`.

&lt;img src="img/vector_sum.png" width="20%" style="display: block; margin: auto;" /&gt;

---

_Scalar Multiplication_: vectors can be scaled by real values; 

`$$2\langle 1, 2.5, −6 \rangle = \langle 2, 5, −12\rangle$$`

In general, `\(ax = \langle ax_1, ax_2, \ldots, ax_p\rangle\)`

---
The norm of a vector `\(x\)`, written `\(\|x\|\)` is its length. 

Unless otherwise specified, this is its Euclidean length,
namely: 

`$$\|x\| = \sqrt{\sum_{j=1}^p x_j^2}$$`

---

### Quiz 

Write Euclidean distance of vectors `\(u\)` and `\(v\)` as a 
vector norm

---

The _dot product_, or _inner product_ of two vectors `\(u\)` and `\(v\)` is defined as

`$$u'v = \sum_{j=1}^p u_i v_i$$`

A useful geometric interpretation of the inner product `\(v'u\)` is that it gives the projection of `\(v\)` onto `\(u\)` (when `\(\|u\|=1\)`).

&lt;img src="img/innerprod.png" width="20%" style="display: block; margin: auto;" /&gt;

---
layout: false

## The curse of dimensionality

Distance-based methods like KNN can be problematic in high-dimensional problems

Consider the case where we have many covariates. We want to use `\(k\)`-nearest neighbor methods.  

Basically, we need to define distance and  look for small
multi-dimensional "balls"
around the target points. 

With many covariates this becomes
difficult. 

---
layout: false

## Summary

- We will represent many ML algorithms geometrically as vectors
- Vector math review
- K-nearest neighbors 
- The curse of dimensionality
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>
<script>
remark.macros['scale'] = function (percentage) {
  var url = this;
  return '<img src="' + url + '" style=width: ' + percentage + '"/>';
};
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
